{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ede80ff4-96c7-4c69-8695-db4e19bf987b",
   "metadata": {},
   "source": [
    "# AD_with_Plaques:1\n",
    "# NCI_with_No_Plaques: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "304db74f-daa9-4450-adf8-b335dbd2cc45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/12tb_dsk2/danish/Pytorch_Biologically_Informed_Neural_Network/all_subtypes_jupyter_notebook/Excitatory_Neurons\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17daee1e-8cea-4fa2-ae8c-d63b1396bf0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-30 13:15:49.146445: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-30 13:15:49.291649: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-30 13:15:49.358912: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-30 13:15:49.359454: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-30 13:15:49.465079: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-30 13:15:50.086635: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('./../../')\n",
    "\n",
    "import tensorflow as tf\n",
    "from gene_expression import *\n",
    "from pathway_hierarchy import *\n",
    "from utils import *\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fb0cea0-3807-442e-8374-f4c7a4b49355",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': {'train': '/12tb_dsk2/danish/Pytorch_Biologically_Informed_Neural_Network/Preprocessed_data/excitory_neurons/train.csv', 'test': '/12tb_dsk2/danish/Pytorch_Biologically_Informed_Neural_Network/Preprocessed_data/excitory_neurons/test.csv', 'val': '/12tb_dsk2/danish/Pytorch_Biologically_Informed_Neural_Network/Preprocessed_data/excitory_neurons/val.csv', 'y_train': '/12tb_dsk2/danish/Pytorch_Biologically_Informed_Neural_Network/Preprocessed_data/excitory_neurons/y_train.csv', 'y_test': '/12tb_dsk2/danish/Pytorch_Biologically_Informed_Neural_Network/Preprocessed_data/excitory_neurons/y_test.csv', 'y_val': '/12tb_dsk2/danish/Pytorch_Biologically_Informed_Neural_Network/Preprocessed_data/excitory_neurons/y_val.csv'}, 'model_output': {'model_save_dir': '/12tb_dsk2/danish/Pytorch_Biologically_Informed_Neural_Network/model_save/excitory_neurons/Exc_Neu/'}, 'train': {'epochs': 100, 'learning_rate': 0.01, 'weight_decay': 0.001, 'batch_size': 200000}, 'gene_expression': {'highly_expressed_threshold': 0, 'lowly_expressed_threshold': 0, 'normalization': True, 'marker': False, 'print_information': True}, 'pathways_network': {'species': 'human', 'n_hidden_layer': 4, 'pathway_relation': '/12tb_dsk2/danish/Pytorch_Biologically_Informed_Neural_Network/reactome/ReactomePathwaysRelation.txt', 'pathway_names': '/12tb_dsk2/danish/Pytorch_Biologically_Informed_Neural_Network/reactome/ReactomePathways.txt', 'ensemble_pathway_relation': '/12tb_dsk2/danish/Pytorch_Biologically_Informed_Neural_Network/reactome/Ensembl2Reactome_All_Levels.txt', 'datatype': 'diagnosis', 'h_thresh': 0.9, 'l_thresh': 0.0, 'pathway_relation_updated': '/12tb_dsk2/danish/Pytorch_Biologically_Informed_Neural_Network/reactome/Subset_reactomePathwaysRelation_v2.txt', 'ensemble_pathway_relation_updated': '/12tb_dsk2/danish/Pytorch_Biologically_Informed_Neural_Network/reactome/Subset_Ensembl2Reactome_All_Levels_v2.txt'}}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 439\u001b[0m\n\u001b[1;32m    432\u001b[0m         masking_df[i]\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;28mstr\u001b[39m(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_output\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_save_dir\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m+\u001b[39m date_string\u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmasking_df_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_dict_sparse, val_dataloader, test_dataloader, train_dataloader, train_x, train_y, val_x, y_val, test_x, y_test, config\n\u001b[0;32m--> 439\u001b[0m model_dict_sparse, val_dataloader, test_dataloader, train_dataloader,train_x, train_y, val_x, y_val, test_x, y_test, config \u001b[38;5;241m=\u001b[39m \u001b[43mmain_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpath_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j,i \u001b[38;5;129;01min\u001b[39;00m model_dict_sparse\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    441\u001b[0m \n\u001b[1;32m    442\u001b[0m \u001b[38;5;66;03m# Assuming 'model' is your neural network\u001b[39;00m\n\u001b[1;32m    443\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(i\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_output\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_save_dir\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate_string\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/model_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_state_dict_jupyter_notebook.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 297\u001b[0m, in \u001b[0;36mmain_file\u001b[0;34m(path_config)\u001b[0m\n\u001b[1;32m    295\u001b[0m config \u001b[38;5;241m=\u001b[39m load_config(path_config)\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28mprint\u001b[39m(config)\n\u001b[0;32m--> 297\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdataset\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m],index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    299\u001b[0m val \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m],index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m<frozen codecs>:331\u001b[0m, in \u001b[0;36mgetstate\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import argparse\n",
    "from utils import *\n",
    "from gene_expression import *\n",
    "from pathway_hierarchy import *\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from custom_neural_network import *\n",
    "from custom_fc_network import *\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import copy\n",
    "import pickle\n",
    "import random\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "path_config = '/12tb_dsk2/danish/Pytorch_Biologically_Informed_Neural_Network/all_subtypes_jupyter_notebook/Excitatory_Neurons/config-2.yml'\n",
    "model_dct = dict()\n",
    "\n",
    "# Hook function\n",
    "def hook_fn(module, input, output, layer_name):\n",
    "    global model_dct\n",
    "    input_list = [i.detach().cpu().numpy().tolist() for i in input]\n",
    "    output_list = output.detach().cpu().numpy().tolist()\n",
    "    \n",
    "    # If the layer name is not in the dictionary, create a new list for it\n",
    "    if layer_name not in model_dct:\n",
    "        model_dct[layer_name] = []\n",
    "\n",
    "    # Append the activations to the corresponding layer list\n",
    "    model_dct[layer_name].append({\n",
    "        'input': input_list,\n",
    "        'output': output_list\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "# Define the file path for the CSV file\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, count_matrix, label):\n",
    "        # Read the CSV file\n",
    "        self.data = count_matrix\n",
    "        # Separate features and target\n",
    "        self.features = self.data.values\n",
    "        self.target = label.values\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get features and target for a given index\n",
    "        features = torch.tensor(self.features[idx], dtype=torch.float32)\n",
    "        target = torch.tensor(self.target[idx], dtype=torch.float32)\n",
    "        return features, target\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predicted_list = []\n",
    "    probability_list = []\n",
    "    labels_list = []\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    loss = 0\n",
    "    with torch.no_grad():  # No need to compute gradients during evaluation\n",
    "        for features, labels in dataloader:\n",
    "            outputs = model(features)\n",
    "            #print(outputs)\n",
    "            probability = torch.sigmoid(outputs.data)\n",
    "            predicted = torch.round(torch.sigmoid(outputs.data))\n",
    "            #print(outputs)\n",
    "            #print(predicted)\n",
    "            loss += criterion(outputs, labels)\n",
    "            #_, predicted = torch.sigmoid(outputs.data)\n",
    "            predicted_list.extend(predicted)\n",
    "            labels_list.extend(labels)\n",
    "            probability_list.extend(probability)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    #print(total)\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy, loss, predicted_list, labels_list, probability_list\n",
    "\n",
    "def save_model(model_nn,model_path, model_state_dict_path):\n",
    "    \n",
    "    model_nn.eval()\n",
    "    torch.save(model_nn, model_path)\n",
    "    torch.save(model_nn.state_dict(), model_state_dict_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def model_fc(train_dataloader , val_dataloader, test_dataloader, test_cell_id, layers_node, masking, output_layer,model_save_dir, date_string, learning_rate=0.001, num_epochs=50, weight_decay = 0):\n",
    "\n",
    "    model_nn = CustomfcNetwork(layers_node, output_layer, masking)\n",
    "    optimizer = optim.AdamW(model_nn.parameters(), lr=learning_rate,weight_decay = weight_decay )  # Using SGD with momentum\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=5, verbose=True)\n",
    "    patience = 20\n",
    "    best_val_accuracy = 0.0\n",
    "    epochs_no_improve = 0\n",
    "    early_stop = False\n",
    "    csv_file_path = f'{model_save_dir}{date_string}/fc_training_log_{output_layer}.csv'\n",
    "\n",
    "    try:\n",
    "        os.makedirs(f'{model_save_dir}{date_string}')\n",
    "    except:\n",
    "        print(('...'))\n",
    "\n",
    "    with open(csv_file_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Epoch', 'Train_Loss', 'Train_accuracy','Validation_Loss','Val_accuracy'])\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        if early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        epoch_cost = 0.\n",
    "        \n",
    "        total_loss = 0\n",
    "        for batch_features,batch_targets in train_dataloader:\n",
    "            outputs = model_nn(batch_features)\n",
    "            #print(outputs)\n",
    "            #print(batch_targets)\n",
    "            #print(outputs)\n",
    "            loss = criterion(outputs, batch_targets)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "        \n",
    "        train_accuracy, train_loss, predicted_list_train, labels_list_train, train_probability_list = evaluate(model_nn, train_dataloader)\n",
    "        val_accuracy, val_loss, predicted_list_val, labels_list_val, val_probability_list = evaluate(model_nn, val_dataloader)\n",
    "        #scheduler.step(val_accuracy)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss.item():.4f}, Train_accuracy: {train_accuracy}, Val Loss: {val_loss.item():.4f}, Val_accuracy: {val_accuracy}')\n",
    "        with open(csv_file_path, mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([epoch + 1, loss.item(), train_accuracy, val_loss.item(), val_accuracy])\n",
    "        \n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            epochs_no_improve = 0\n",
    "        # Save the best model\n",
    "            model_path = f'{model_save_dir}{date_string}/fc_best_model_{output_layer}.pth'\n",
    "            model_state_dict_path = f'{model_save_dir}{date_string}/fc_best_model_{output_layer}_state_dict.pth'\n",
    "            save_model(model_nn, model_path, model_state_dict_path)\n",
    "            best_model_nn = copy.deepcopy(model_nn)\n",
    "            #torch.save(model_nn, f'{model_save_dir}{date_string}/fc_best_model_{output_layer}.pth')\n",
    "            #torch.save(model_nn.state_dict(), f'{model_save_dir}{date_string}/fc_best_model_{output_layer}_state_dict.pth')\n",
    "            print('Model saved.')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "    \n",
    "        # Early stopping\n",
    "        '''if epochs_no_improve >= patience:\n",
    "            early_stop = True\n",
    "            print(\"Early stopping triggered\")'''\n",
    "        \n",
    "    \n",
    "    train_accuracy, train_loss, predicted_list_train, labels_list_train, train_probability_list = evaluate(best_model_nn, train_dataloader)\n",
    "    val_accuracy, val_loss, predicted_list_val, labels_list_val, val_probability_list = evaluate(best_model_nn, val_dataloader)\n",
    "    test_accuracy, test_loss, predicted_list_test, labels_list_test, test_probability_list = evaluate(best_model_nn, test_dataloader)\n",
    "    print('Test Accucary', test_accuracy)\n",
    "    output_train = (predicted_list_train, labels_list_train)\n",
    "    output_val = (predicted_list_val, labels_list_val)\n",
    "\n",
    "    labels_list_test = [m.item() for m in labels_list_test]\n",
    "    predicted_list_test = [m.item() for m in predicted_list_test]\n",
    "    test_probability_list = [m.item() for m in test_probability_list]\n",
    "\n",
    "\n",
    "    test_df = pd.DataFrame({'cell_id': test_cell_id, 'true_y': labels_list_test, 'pred_y': predicted_list_test, 'probabilty': test_probability_list})\n",
    "    csv_file_path = f'{model_save_dir}{date_string}/fc_test_log_{output_layer}.csv'\n",
    "    test_df.to_csv(csv_file_path)\n",
    "    #torch.save(model_nn, f'{model_save_dir}{date_string}/fc_last_epoch_model_{output_layer}.pth')\n",
    "    return output_train, output_val,best_model_nn\n",
    "\n",
    "\n",
    "\n",
    "def model(train_dataloader , val_dataloader, test_dataloader, test_cell_id, layers_node, masking, output_layer,model_save_dir, date_string, learning_rate=0.001, num_epochs=50, weight_decay = 0):\n",
    "\n",
    "    model_nn = CustomNetwork(layers_node, output_layer, masking)\n",
    "    print(model_nn)\n",
    "    optimizer = optim.AdamW(model_nn.parameters(), lr=learning_rate,weight_decay = weight_decay )  # Using SGD with momentum\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=5, verbose=True)\n",
    "    patience = 20\n",
    "    best_val_accuracy = 0.0\n",
    "    epochs_no_improve = 0\n",
    "    early_stop = False\n",
    "    csv_file_path = f'{model_save_dir}{date_string}/training_log_{output_layer}.csv'\n",
    "\n",
    "    try:\n",
    "        os.makedirs(f'{model_save_dir}{date_string}')\n",
    "    except:\n",
    "        print(('...'))\n",
    "\n",
    "    with open(csv_file_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Epoch', 'Train_Loss', 'Train_accuracy','Validation_Loss','Val_accuracy'])\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        if early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        epoch_cost = 0.\n",
    "        \n",
    "        total_loss = 0\n",
    "        for batch_features,batch_targets in train_dataloader:\n",
    "            \n",
    "            #print(outputs)\n",
    "            #print(batch_targets)\n",
    "            #print(outputs)\n",
    "            \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model_nn(batch_features)\n",
    "            loss = criterion(outputs, batch_targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "        \n",
    "        train_accuracy, train_loss, predicted_list_train, labels_list_train, train_probability_list = evaluate(model_nn, train_dataloader)\n",
    "        val_accuracy, val_loss, predicted_list_val, labels_list_val, val_probability_list = evaluate(model_nn, val_dataloader)\n",
    "        #scheduler.step(val_accuracy)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss.item():.4f}, Train_accuracy: {train_accuracy}, Val Loss: {val_loss.item():.4f}, Val_accuracy: {val_accuracy}')\n",
    "        with open(csv_file_path, mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([epoch + 1, loss.item(), train_accuracy, val_loss.item(), val_accuracy])\n",
    "        \n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            epochs_no_improve = 0\n",
    "        # Save the best model\n",
    "            model_path = f'{model_save_dir}{date_string}/best_model_{output_layer}.pth'\n",
    "            model_state_dict_path = f'{model_save_dir}{date_string}/best_model_{output_layer}_state_dict.pth'\n",
    "            save_model(model_nn, model_path, model_state_dict_path)\n",
    "            best_model_nn = copy.deepcopy(model_nn)\n",
    "            #torch.save(model_nn, f'{model_save_dir}{date_string}/best_model_{output_layer}.pth')\n",
    "            #torch.save(model_nn.state_dict(), f'{model_save_dir}{date_string}/best_model_{output_layer}_state_dict.pth')\n",
    "            print('Model saved.')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "    \n",
    "        # Early stopping\n",
    "        '''if epochs_no_improve >= patience:\n",
    "            early_stop = True\n",
    "            print(\"Early stopping triggered\")'''\n",
    "        \n",
    "    \n",
    "    train_accuracy, train_loss, predicted_list_train, labels_list_train, train_probability_list = evaluate(best_model_nn, train_dataloader)\n",
    "    val_accuracy, val_loss, predicted_list_val, labels_list_val, val_probability_list = evaluate(best_model_nn, val_dataloader)\n",
    "    test_accuracy, test_loss, predicted_list_test, labels_list_test, test_probability_list = evaluate(best_model_nn, test_dataloader)\n",
    "    print('Test Accucary', test_accuracy)\n",
    "    output_train = (predicted_list_train, labels_list_train)\n",
    "    output_val = (predicted_list_val, labels_list_val)\n",
    "\n",
    "    labels_list_test = [m.item() for m in labels_list_test]\n",
    "    predicted_list_test = [m.item() for m in predicted_list_test]\n",
    "    test_probability_list = [m.item() for m in test_probability_list]\n",
    "\n",
    "\n",
    "    test_df = pd.DataFrame({'cell_id': test_cell_id, 'true_y': labels_list_test, 'pred_y': predicted_list_test, 'probabilty': test_probability_list})\n",
    "    csv_file_path = f'{model_save_dir}{date_string}/test_log_{output_layer}.csv'\n",
    "    test_df.to_csv(csv_file_path)\n",
    "    #torch.save(model_nn, f'{model_save_dir}{date_string}/last_epoch_model_{output_layer}.pth')\n",
    "    return output_train, output_val,best_model_nn\n",
    "\n",
    "\n",
    "def load_config(config_file):\n",
    "    with open(config_file, 'r') as file:\n",
    "        return yaml.safe_load(file)\n",
    "\n",
    "\n",
    "def main_file(path_config):\n",
    "\n",
    "    '''parser = argparse.ArgumentParser(description='Sample application with config and argparse')\n",
    "    parser.add_argument('--config', type=str, default='config.yml', help='Path to the configuration file')\n",
    "    args = parser.parse_args()'''\n",
    "\n",
    "    config = load_config(path_config)\n",
    "    print(config)\n",
    "    train = pd.read_csv(config['dataset']['train'],index_col=0)\n",
    "    test = pd.read_csv(config['dataset']['test'],index_col=0)\n",
    "    val = pd.read_csv(config['dataset']['val'],index_col=0)\n",
    "\n",
    "    y_train = pd.read_csv(config['dataset']['y_train'])\n",
    "    y_test = pd.read_csv(config['dataset']['y_test'])\n",
    "    y_val = pd.read_csv(config['dataset']['y_val'])\n",
    "  \n",
    "\n",
    "\n",
    "    r_data_tmp = train.T\n",
    "    q_data_tmp = test.T\n",
    "    v_data_tmp = val.T\n",
    "    r_label_tmp = y_train\n",
    "\n",
    "\n",
    "\n",
    "    pathway_relation, ensemble_pathway_relation = return_threshold_pathways(config['pathways_network']['pathway_relation'], \n",
    "                                  config['pathways_network']['ensemble_pathway_relation'],  config['pathways_network']['h_thresh'],\\\n",
    "                                      config['pathways_network']['l_thresh'])\n",
    "\n",
    "    if os.path.exists(config['pathways_network']['pathway_relation_updated']):\n",
    "        os.remove(config['pathways_network']['pathway_relation_updated'])\n",
    "\n",
    "    if os.path.exists(config['pathways_network']['ensemble_pathway_relation_updated']):\n",
    "        os.remove(config['pathways_network']['ensemble_pathway_relation_updated'])\n",
    "\n",
    "    pathway_relation.to_csv(config['pathways_network']['pathway_relation_updated'], sep = '\\t', index = False, header= False)\n",
    "    ensemble_pathway_relation.to_csv(config['pathways_network']['ensemble_pathway_relation_updated'], sep = '\\t', index = False, header= False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print('Getting Marker Genes.......')\n",
    "    train_x, test_x, val_x, train_y = get_expression(r_data_tmp,\n",
    "                                                q_data_tmp,\n",
    "                                                v_data_tmp,\n",
    "                                                r_label_tmp,\n",
    "                                                thrh=config['gene_expression']['highly_expressed_threshold'],\n",
    "                                                thrl=config['gene_expression']['lowly_expressed_threshold'],\n",
    "                                                normalization=config['gene_expression']['normalization'],\n",
    "                                                marker=config['gene_expression']['marker'])\n",
    "    \n",
    "    print('Getting Pathway Genes.........')\n",
    "    pathway_genes = get_gene_pathways(config['pathways_network']['ensemble_pathway_relation_updated'], species=config['pathways_network']['species'])\n",
    "\n",
    "\n",
    "    print('Getting Masking.........')\n",
    "    masking, masking_df, layers_node, train_x, test_x,val_x = get_masking(config['pathways_network']['pathway_names'],\n",
    "                                                        pathway_genes,\n",
    "                                                        config['pathways_network']['pathway_relation_updated'],\n",
    "                                                        train_x,\n",
    "                                                        test_x,\n",
    "                                                        val_x,\n",
    "                                                        train_y,\n",
    "                                                        config['pathways_network']['datatype'],\n",
    "                                                        config['pathways_network']['species'],\n",
    "                                                        config['pathways_network']['n_hidden_layer'])\n",
    "\n",
    "    test_cell_id = list(test_x.T.index) \n",
    "    try:\n",
    "        masking = list(masking.values())\n",
    "        layers_node = list(layers_node.values())\n",
    "    except:\n",
    "        print('already_done')\n",
    "\n",
    "\n",
    "    train_dataset = TabularDataset(train_x.T,train_y)\n",
    "    val_dataset = TabularDataset(val_x.T,y_val)\n",
    "    test_dataset = TabularDataset(test_x.T,y_test)  \n",
    "    \n",
    "    \n",
    "\n",
    "    dataloader_params = {\n",
    "    'batch_size': config['train']['batch_size'],\n",
    "    'shuffle': False\n",
    "    }\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset,**dataloader_params)\n",
    "    test_dataloader = DataLoader(test_dataset, **dataloader_params)\n",
    "    val_dataloader = DataLoader(val_dataset,**dataloader_params)\n",
    "    # Example of iterating through the DataLoader\n",
    "\n",
    "\n",
    "    pred_y_df = pd.DataFrame(data=0, index=test_x.columns, columns=list(range(2, len(masking) + 2)))\n",
    "    train_y_df = pd.DataFrame(data=0, index=train_x.columns, columns=list(range(2, len(masking) + 2)))\n",
    "    model_dict_sparse = dict()\n",
    "    model_dict_fc = dict()\n",
    "    activation_output = {}\n",
    "    now = datetime.now()\n",
    "\n",
    "# Format the date as a string\n",
    "    date_string = datetime_string = now.strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "    try:\n",
    "        os.makedirs(f'{config['model_output']['model_save_dir']}{date_string}')\n",
    "    except:\n",
    "        print(('...'))\n",
    "\n",
    "   \n",
    "\n",
    "    print('Training.........')\n",
    "    for output_layer in range(2, len(masking) + 2):\n",
    "        if config['gene_expression']['print_information']:\n",
    "            print(\"Current sub-neural network has \" + str(output_layer - 1) + \" hidden layers.\")\n",
    "        output_train, output_val,model_dict_sparse[output_layer] = model(train_dataloader,\n",
    "                                            val_dataloader,test_dataloader, test_cell_id,\n",
    "                                            layers_node,\n",
    "                                            masking,\n",
    "                                            output_layer,\n",
    "                                            model_save_dir = config['model_output']['model_save_dir'],date_string = date_string,\n",
    "                                            learning_rate=config['train']['learning_rate'],num_epochs=config['train']['epochs'],weight_decay = config['train']['weight_decay']\n",
    "                                        )  \n",
    "    '''\n",
    "    print('tranining_fully_connected_layers:')\n",
    "    for output_layer in range(2, len(masking) + 2):\n",
    "        if config['gene_expression']['print_information']:\n",
    "            print(\"Current sub-neural network has \" + str(output_layer - 1) + \" hidden layers.\")\n",
    "        output_train, output_val,model_dict_fc[output_layer] = model_fc(train_dataloader,\n",
    "                                            val_dataloader,test_dataloader, test_cell_id,\n",
    "                                            layers_node,\n",
    "                                            masking,\n",
    "                                            output_layer,\n",
    "                                            model_save_dir = config['model_output']['model_save_dir'],date_string = date_string,\n",
    "                                            learning_rate=config['train']['learning_rate'],num_epochs=config['train']['epochs'],weight_decay = config['train']['weight_decay']\n",
    "                                        )  \n",
    "    '''\n",
    "    new_parameter = {'date_string': date_string}\n",
    "    config.update(new_parameter)\n",
    "    save_path =   str(config['model_output']['model_save_dir'])+ date_string + '/config.yml'\n",
    "    with open(save_path, 'w') as file:\n",
    "        yaml.dump(config, file)\n",
    "\n",
    "        \n",
    "    for i in range(len(masking_df)):\n",
    "        masking_df[i].to_csv(str(config['model_output']['model_save_dir'])+ date_string+ '/' +f'masking_df_{i}.csv')\n",
    "    \n",
    "   \n",
    "        \n",
    "    return model_dict_sparse, val_dataloader, test_dataloader, train_dataloader, train_x, train_y, val_x, y_val, test_x, y_test, config\n",
    "\n",
    "   \n",
    "model_dict_sparse, val_dataloader, test_dataloader, train_dataloader,train_x, train_y, val_x, y_val, test_x, y_test, config = main_file(path_config= path_config)\n",
    "for j,i in model_dict_sparse.items():\n",
    "\n",
    "# Assuming 'model' is your neural network\n",
    "    torch.save(i.state_dict(), f'{config['model_output']['model_save_dir']}{config['date_string']}/model_{j}_state_dict_jupyter_notebook.pth')\n",
    "\n",
    "\n",
    "for j,i in model_dict_sparse.items():\n",
    "        \n",
    "        print(f'Hidden_Layers: {j}')\n",
    "        accuracy, loss, predicted_list, labels_list, probability_list = evaluate(i, test_dataloader)\n",
    "        print(f'Test Accuracy: {accuracy}')   \n",
    "        accuracy, loss, predicted_list, labels_list, probability_list = evaluate(i, train_dataloader)\n",
    "        print(f'Train Accuracy: {accuracy}')   \n",
    "        accuracy, loss, predicted_list, labels_list, probability_list = evaluate(i, val_dataloader)\n",
    "        print(f'Validation Accuracy: {accuracy}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a465e1-58ab-4f18-9943-54217f87d0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = TabularDataset(train_x.T,train_y)\n",
    "val_dataset = TabularDataset(val_x.T,y_val)\n",
    "test_dataset = TabularDataset(test_x.T,y_test) \n",
    "\n",
    "dataloader_params = {\n",
    "    'batch_size': 1,\n",
    "    'shuffle': False\n",
    "    }\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,**dataloader_params)\n",
    "test_dataloader = DataLoader(test_dataset, **dataloader_params)\n",
    "val_dataloader = DataLoader(val_dataset,**dataloader_params)\n",
    "\n",
    "for j,i in model_dict_sparse.items():\n",
    "        \n",
    "        print(f'Hidden_Layers: {j}')\n",
    "        accuracy, loss, predicted_list, labels_list, probability_list = evaluate(i, test_dataloader)\n",
    "        print(f'Test Accuracy: {accuracy}')   \n",
    "        accuracy, loss, predicted_list, labels_list, probability_list = evaluate(i, train_dataloader)\n",
    "        print(f'Train Accuracy: {accuracy}')   \n",
    "        accuracy, loss, predicted_list, labels_list, probability_list = evaluate(i, val_dataloader)\n",
    "        print(f'Validation Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cf86ae-23f8-429d-838e-df8abde3c2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c89375-5f2d-4175-958c-8c59fc2b1667",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['model_output']['model_save_dir'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867f09af-cd2b-45d7-ac06-29e192830d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dce6d6f-cadc-4111-bc5c-6618bec14a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8458044-062e-48b9-a957-11ed67272bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict_sparse[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5337d8b1-82eb-4825-be59-e962176e58a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''accuracy, loss, predicted_list, labels_list, probability_list = evaluate(model_dict_sparse[2], test_dataloader)\n",
    "accuracy'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e823f1ea-15ab-417a-a6ca-9976f4876d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "for features, labels in test_dataloader:\n",
    "    print(labels)\n",
    "    print(features.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6022339a-cf84-4801-b60c-666f947f77c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict_sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079d4f44-e871-4952-9ec7-a37a4e2f2140",
   "metadata": {},
   "source": [
    "# Result Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df282de-5393-4213-93f2-b894ae0760b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "def confusion_matrix_return(result):\n",
    "    cm = confusion_matrix(result['true_y'], result['pred_y'])\n",
    "    \n",
    "    # Plot the confusion matrix\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8044567b-d707-4878-a47d-09abd45a779c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "def roc_curve_return(result):\n",
    "    \n",
    "    # Calculate the ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(result['true_y'], result['probabilty'])\n",
    "    \n",
    "    # Calculate the AUC\n",
    "    auc = roc_auc_score(result['true_y'], result['probabilty'])\n",
    "    print(f'AUC: {auc}')\n",
    "    \n",
    "    # Plot the ROC curve\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a4324c-0a47-4e2e-9027-dc86272376c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# Sample DataFrame\n",
    "def return_accuracy(result):\n",
    "    df = result.copy()\n",
    "    \n",
    "    # Calculate positive accuracy (precision for positive class)\n",
    "    positive_accuracy = precision_score(df['true_y'], df['pred_y'], pos_label=1)\n",
    "    \n",
    "    # Calculate negative accuracy (precision for negative class)\n",
    "    negative_accuracy = precision_score(df['true_y'], df['pred_y'], pos_label=0)\n",
    "    \n",
    "    print(f'Positive Accuracy (Precision for positive class): {positive_accuracy}')\n",
    "    print(f'Negative Accuracy (Precision for negative class): {negative_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc4a90f-e31d-47c8-a3f7-06876d627576",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = config['model_output']['model_save_dir'] + config['date_string']+ '/' + 'test_log_'\n",
    "\n",
    "for i in range(2,config['pathways_network']['n_hidden_layer']+2):\n",
    "    print('-'*282)\n",
    "    print(f'Number of Hidden Layers: {i-1}')\n",
    "    result = pd.read_csv(f'{dir_path}{i}.csv',index_col=0)\n",
    "\n",
    "    confusion_matrix_return(result)\n",
    "    roc_curve_return(result)\n",
    "    return_accuracy(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0033209-555e-4136-8c99-bdd45b8bd2ee",
   "metadata": {},
   "source": [
    "# Model Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b688d50-c892-4781-8630-259ed9e5078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a hook function to capture the activations\n",
    "\n",
    "\n",
    "def get_activation(name, number_of_layers, config, activations):\n",
    "    def hook(model, input, output):\n",
    "        # Convert output to numpy array for easier handling, but this is optional\n",
    "        activations[name] = output.detach().numpy()\n",
    "        \n",
    "           \n",
    "    return hook\n",
    "def attaching_hook(model, dataloader):\n",
    "    \n",
    "        correct = 0\n",
    "        total = 0\n",
    "        predicted_list = []\n",
    "        probability_list = []\n",
    "        labels_list = []\n",
    "        activations_list = []\n",
    "        features_list = []\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        loss = 0\n",
    "        \n",
    "\n",
    "        with torch.no_grad():  # No need to compute gradients during evaluation\n",
    "            for sample_idx, (features, labels) in tqdm(enumerate(dataloader)):\n",
    "                \n",
    "                \n",
    "                #print(labels)\n",
    "                #print(features.shape)\n",
    "                activations = {}\n",
    "                for idx, layer in enumerate(model.layers):\n",
    "                    layer_name = f'layer_{idx}'\n",
    "                    number_of_layers = len(model.layers)\n",
    "                    activation_hook = get_activation(layer_name, number_of_layers, config, activations)\n",
    "                    \n",
    "                    layer.register_forward_hook(activation_hook)\n",
    "    \n",
    "                outputs =  model(features)\n",
    "                    #print(outputs)\n",
    "                probability = torch.sigmoid(outputs.data)\n",
    "                predicted = torch.round(torch.sigmoid(outputs.data))\n",
    "                    #print(outputs)\n",
    "                    #print(predicted)\n",
    "                #loss += criterion(outputs, labels)\n",
    "                    #_, predicted = torch.sigmoid(outputs.data)\n",
    "                predicted_list.extend(predicted.detach().numpy())\n",
    "                labels_list.extend(labels.detach().numpy())\n",
    "                probability_list.extend(probability.detach().numpy())\n",
    "                features_list.append(features)\n",
    "                total += labels.size(0)\n",
    "                x = activations\n",
    "                activations_temp = activations.copy()\n",
    "                activations_list.append(activations_temp)\n",
    "                \n",
    "                correct += (predicted == labels).sum().item()\n",
    "                '''if sample_idx == 1000:\n",
    "                    #print(activations_list)\n",
    "                    break'''\n",
    "                \n",
    "            #print(total)\n",
    "        accuracy = 100 * correct / total\n",
    "        #print(activations_list)\n",
    "        return activations_list, accuracy, predicted_list, labels_list, features_list,x\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3aad112-b683-4ca7-a7de-eda25b51adeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def return_feature_importance(model_dict_sparse, dataloader):\n",
    "    predicted_list_dict = {}\n",
    "    ground_truth_list_dict = {}\n",
    "    activations_list_dict = {}\n",
    "    accuracy_list_dict = {}\n",
    "    for i, j in model_dict_sparse.items():\n",
    "        #accuracy, loss, predicted_list, labels_list, probability_list = evaluate(j, dataloader)\n",
    "        \n",
    "        activations_list_dict[f'model_{i}_hidden_layers'], accuracy_list_dict[f'model_{i}_hidden_layers'], \\\n",
    "        predicted_list_dict[f'model_{i}_hidden_layers'], ground_truth_list_dict[f'model_{i}_hidden_layers'], \\\n",
    "        feature_list,x= attaching_hook(j, dataloader)\n",
    "        print(f'{i} Layers: {accuracy_list_dict[f'model_{i}_hidden_layers']}')\n",
    "        \n",
    "    return activations_list_dict, accuracy_list_dict, predicted_list_dict, ground_truth_list_dict, feature_list,x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557dec82-4b19-4555-aa56-fb89c41f7d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4 = dict()\n",
    "model_4[4] = model_dict_sparse[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb736be6-d759-4166-9327-ffa320f2877e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21569bf-6144-4f5d-bccb-7336562f40d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "activations_list_dict, accuracy_list_dict, predicted_list_dict, ground_truth_list_dict, feature_list,x = return_feature_importance(model_4, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b76f36-8d00-4f25-845c-ce4dfda18476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_feature_importance_list(activations_list_dict, model_name):\n",
    "    m=0\n",
    "    for k in tqdm(activations_list_dict[model_name]):\n",
    "        #print(k)\n",
    "        if m == 0:\n",
    "            layer_dict = {key: [] for key in list(k.keys())}\n",
    "        m = m+1\n",
    "        \n",
    "        #print(k['0_layer_0'])\n",
    "        for z in list(layer_dict.keys()):\n",
    "            #print(k)\n",
    "            layer_dict[z].append(k[z][0])\n",
    "\n",
    "    return layer_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fd88a9-b562-4c01-b365-ea928e2d2628",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "layer_dict = return_feature_importance_list(activations_list_dict, 'model_2_hidden_layers')\n",
    "layer_node_name_dict = {}\n",
    "for i in range(config['pathways_network']['n_hidden_layer']+1):\n",
    "    nodes_df = pd.read_csv(config['model_output']['model_save_dir'] + config['date_string'] + '/' + 'masking_df_' + \\\n",
    "    f'{config['pathways_network']['n_hidden_layer']-i}'+ '.csv', index_col = 0)\n",
    "    layer_node_name_dict[f'layer_{i-1}'] = nodes_df.columns\n",
    "    \n",
    "\n",
    "layer_node_name_dict['final_layer'] = ['last_layer']\n",
    "\n",
    "li = []\n",
    "for i in layer_dict['layer_0']:\n",
    "    li.append(list(i))\n",
    "    \n",
    "layer_0_df = pd.DataFrame(li,index = test_x.columns, columns=layer_node_name_dict['layer_0'])\n",
    "layer_0_df\n",
    "\n",
    "li = []\n",
    "for i in layer_dict['layer_1']:\n",
    "    li.append(list(i))\n",
    "    \n",
    "last_layer_df = pd.DataFrame(li,index = test_x.columns, columns = layer_node_name_dict['final_layer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce99c197-9654-42aa-9dc4-8785e258c4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_0_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281141a4-2957-4d7f-8d1e-bbd51d4c63c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_0_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c716d4e2-24f3-4971-9931-fcc4289ca805",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_layer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b12b52-6671-401a-bbc2-1f7c2ed45d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb1c306-28a7-4518-ad38-2012bfa8d730",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in predicted_list_dict.items():\n",
    "    flattened_list = [item for sublist in list(predicted_list_dict[i]) for item in sublist]\n",
    "    predicted_list_dict[i] = flattened_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fda485a-1ff5-4b97-b5b9-db22c10d9db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_list_dict['model_2_hidden_layers'][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45df74f-a602-4488-97e7-f72ba9e5f73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_list_dict['model_4_hidden_layers'][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2382371-4dd7-421f-b4f0-150e0d9eae8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in ground_truth_list_dict.items():\n",
    "    flattened_list = [item for sublist in list(ground_truth_list_dict[i]) for item in sublist]\n",
    "    ground_truth_list_dict[i] = flattened_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16c0920-cc15-4394-a3ca-e498c01fd912",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_list_dict['model_2_hidden_layers'][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed02ed48-b781-4ef9-9c11-ea1d467f63c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_list_dict['model_4_hidden_layers'][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d693f4e1-59b3-4467-b28d-1b40a4e698cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_list_dict['model_2_hidden_layers'][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185c2410-67b8-4d30-b781-3522bd05ea9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_0_df['predicted'] = predicted_list_dict['model_2_hidden_layers']\n",
    "layer_0_df['ground_truth'] = ground_truth_list_dict['model_2_hidden_layers']\n",
    "layer_0_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fb5fa3-42f9-44e2-805d-79e9872f8d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_predicted_layer_0_df = layer_0_df[layer_0_df.predicted == layer_0_df.ground_truth]\n",
    "correct_predicted_layer_0_df_AD = correct_predicted_layer_0_df[correct_predicted_layer_0_df.predicted == 1]\n",
    "correct_predicted_layer_0_df_Control = correct_predicted_layer_0_df[correct_predicted_layer_0_df.predicted == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d92a59b-3f0c-4141-9608-7cd3ecc622ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_layer_0 = pd.concat([correct_predicted_layer_0_df_AD, correct_predicted_layer_0_df_Control])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e01a08-09c7-4d27-b885-0203cea068a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_layer_0['predicted'] = combined_layer_0['predicted'].replace({1: 'AD', 0: 'Control'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a8c710-df8a-47e5-a911-a0c97c963589",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(combined_layer_0.predicted)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb798ea8-65fc-4f9f-8117-b6bb4361761f",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_layer_0 .drop(columns=['predicted','ground_truth'], inplace = True)\n",
    "combined_layer_0 .head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aa7433-ca7d-4566-908b-c42efa096b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''pathways_list = []\n",
    "p_value_list = []\n",
    "\n",
    "for i in correct_predicted_layer_1_df_Control.columns:\n",
    "    t_stat, p_value = stats.ttest_ind(correct_predicted_layer_1_df_Control[i].tolist(), correct_predicted_layer_1_df_AD[i].tolist())\n",
    "    print(f\"T-statistic: {t_stat}\")\n",
    "    print(f\"P-value: {p_value}\")\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.boxplot([correct_predicted_layer_1_df_Control[i].tolist(), correct_predicted_layer_1_df_AD[i].tolist()], labels=['Control', 'AD'], patch_artist=True, \n",
    "                boxprops=dict(facecolor='lightblue'))\n",
    "    \n",
    "    # Add title and labels\n",
    "    plt.title('Box Plot of Two Distributions')\n",
    "    plt.ylabel('Values')\n",
    "    pathways_list.append(i)\n",
    "    p_value_list.append(p_value)\n",
    "    # Display the plot\n",
    "    plt.show()'''\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae06039-d415-4f32-ac1d-82778e03148f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import umap\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "\n",
    "# Example embeddings (let's assume these are your high-dimensional embeddings)\n",
    "np.random.seed(42)\n",
    "embeddings = combined_layer_0.values \n",
    "#labels = np.random.choice(['A', 'B', 'C'], size=100)  # Example labels\n",
    "labels = labels\n",
    "# Step 1: Reduce dimensionality using UMAP\n",
    "reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "umap_embeddings = reducer.fit_transform(embeddings)\n",
    "\n",
    "# Step 2: Create a DataFrame with the reduced dimensions and labels\n",
    "df_umap = pd.DataFrame({\n",
    "    'UMAP1': umap_embeddings[:, 0],\n",
    "    'UMAP2': umap_embeddings[:, 1],\n",
    "    'Label': labels\n",
    "})\n",
    "\n",
    "# Step 3: Plot using Plotly\n",
    "fig = px.scatter(\n",
    "    df_umap,\n",
    "    x='UMAP1',\n",
    "    y='UMAP2',\n",
    "    color='Label',\n",
    "    title='UMAP Projection of Pathways Importance',\n",
    "    labels={'UMAP1': 'UMAP Dimension 1', 'UMAP2': 'UMAP Dimension 2'}\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ba0c25-7f99-4a46-a673-d38917f84509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assuming `combined_layer_0` and `labels` are already defined\n",
    "\n",
    "# Step 1: Reduce dimensionality using PCA\n",
    "pca = PCA(n_components=2)  # We want to reduce to 2 dimensions\n",
    "pca_embeddings = pca.fit_transform(combined_layer_0.values)\n",
    "\n",
    "# Step 2: Create a DataFrame with the reduced dimensions and labels\n",
    "df_pca = pd.DataFrame({\n",
    "    'PCA1': pca_embeddings[:, 0],\n",
    "    'PCA2': pca_embeddings[:, 1],\n",
    "    'Label': labels\n",
    "})\n",
    "\n",
    "# Step 3: Plot using Plotly\n",
    "fig = px.scatter(\n",
    "    df_pca,\n",
    "    x='PCA1',\n",
    "    y='PCA2',\n",
    "    color='Label',\n",
    "    title=\"Tained Model's Embedding Vector of AD and Control samples\",\n",
    "    labels={'PCA1': 'PCA Dimension 1', 'PCA2': 'PCA Dimension 2'},\n",
    "    width=800,   # Set plot width\n",
    "    height=600   # Set plot height\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e174f4-6c08-41aa-9eab-298323e18672",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingfig = px.scatter(\n",
    "    df_umap,\n",
    "    x='UMAP1',\n",
    "    y='UMAP2',\n",
    "    color='Label',\n",
    "    title='UMAP Projection of Pathways Importance',\n",
    "    labels={'UMAP1': 'UMAP Dimension 1', 'UMAP2': 'UMAP Dimension 2'},\n",
    "    width=500,   # Width of the plot in pixels\n",
    "    height=6000 \n",
    ")\n",
    "\n",
    "# Show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7983e24-8dfe-467a-bd5b-58a4a4197def",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c731cc85-f9a3-4342-b79f-d253d67ba10c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4c6d50-6023-4daf-9608-cafb5f7e167a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pathways_list, p_value_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdcda2f-36e9-468e-99d2-40b7b5a4ede6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9838f8-2115-40c2-b9a7-8f00d98a5655",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "layer_dict = return_feature_importance_list(activations_list_dict, 'model_3_hidden_layers')\n",
    "layer_node_name_dict = {}\n",
    "for i in range(config['pathways_network']['n_hidden_layer']+1):\n",
    "    nodes_df = pd.read_csv(config['model_output']['model_save_dir'] + config['date_string'] + '/' + 'masking_df_' + \\\n",
    "    f'{config['pathways_network']['n_hidden_layer']-i}'+ '.csv', index_col = 0)\n",
    "    layer_node_name_dict[f'layer_{i-1}'] = nodes_df.columns\n",
    "    \n",
    "\n",
    "layer_node_name_dict['final_layer'] = ['last_layer']\n",
    "\n",
    "li = []\n",
    "for i in layer_dict['layer_0']:\n",
    "    li.append(list(i))    \n",
    "layer_0_df = pd.DataFrame(li,index = test_x.columns, columns=layer_node_name_dict['layer_0'])\n",
    "\n",
    "li = []\n",
    "for i in layer_dict['layer_1']:\n",
    "    li.append(list(i)) \n",
    "layer_1_df = pd.DataFrame(li,index = test_x.columns, columns=layer_node_name_dict['layer_1'])\n",
    "\n",
    "li = []\n",
    "for i in layer_dict['layer_2']:\n",
    "    li.append(list(i))   \n",
    "last_layer_df = pd.DataFrame(li,index = test_x.columns, columns=layer_node_name_dict['final_layer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329036d6-7f40-49ac-bbcf-a8ef4cd50a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cd6899-ba80-4b8d-9dee-1130caaf81da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "layer_dict = return_feature_importance_list(activations_list_dict, 'model_4_hidden_layers')\n",
    "layer_node_name_dict = {}\n",
    "for i in range(config['pathways_network']['n_hidden_layer']+1):\n",
    "    nodes_df = pd.read_csv(config['model_output']['model_save_dir'] + config['date_string'] + '/' + 'masking_df_' + \\\n",
    "    f'{config['pathways_network']['n_hidden_layer']-i}'+ '.csv', index_col = 0)\n",
    "    layer_node_name_dict[f'layer_{i-1}'] = nodes_df.columns\n",
    "    \n",
    "\n",
    "layer_node_name_dict['final_layer'] = ['last_layer']\n",
    "\n",
    "li = []\n",
    "for i in layer_dict['layer_0']:\n",
    "    li.append(list(i))    \n",
    "layer_0_df = pd.DataFrame(li,index = test_x.columns, columns=layer_node_name_dict['layer_0'])\n",
    "\n",
    "li = []\n",
    "for i in layer_dict['layer_1']:\n",
    "    li.append(list(i)) \n",
    "layer_1_df = pd.DataFrame(li,index = test_x.columns, columns=layer_node_name_dict['layer_1'])\n",
    "\n",
    "li = []\n",
    "for i in layer_dict['layer_2']:\n",
    "    li.append(list(i)) \n",
    "layer_2_df = pd.DataFrame(li,index = test_x.columns, columns=layer_node_name_dict['layer_2'])\n",
    "\n",
    "li = []\n",
    "for i in layer_dict['layer_3']:\n",
    "    li.append(list(i))   \n",
    "last_layer_df = pd.DataFrame(li,index = test_x.columns, columns=layer_node_name_dict['final_layer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efd1c54-7bc1-473b-b436-8b4f0acccb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_2_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8439581-0287-4c43-a5cb-558e36a1a321",
   "metadata": {},
   "source": [
    "# layer_2_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ec7de6-23c1-4bf2-993e-fad6c062d449",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30001e3c-d3e9-4f01-a289-03b430725af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2628cf21-4400-4f14-95e8-8bf4d2c9dfd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
