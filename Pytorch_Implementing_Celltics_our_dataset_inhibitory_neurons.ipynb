{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d5cdb53-97f5-4257-81cd-bddeb027f895",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-03 12:28:29.671862: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-03 12:28:29.681246: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-03 12:28:29.693278: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-03 12:28:29.693297: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-03 12:28:29.702288: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-03 12:28:30.093096: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from gene_expression import *\n",
    "from pathway_hierarchy import *\n",
    "from utils import *\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "828d4bb0-72b2-48a4-aec4-27f93b30497f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import argparse\n",
    "from utils import *\n",
    "from gene_expression import *\n",
    "from pathway_hierarchy import *\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from custom_neural_network import *\n",
    "from custom_fc_network import *\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import copy\n",
    "import pickle\n",
    "import random\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "model_dct = dict()\n",
    "\n",
    "# Hook function\n",
    "def hook_fn(module, input, output, layer_name):\n",
    "    global model_dct\n",
    "    input_list = [i.detach().cpu().numpy().tolist() for i in input]\n",
    "    output_list = output.detach().cpu().numpy().tolist()\n",
    "    \n",
    "    # If the layer name is not in the dictionary, create a new list for it\n",
    "    if layer_name not in model_dct:\n",
    "        model_dct[layer_name] = []\n",
    "\n",
    "    # Append the activations to the corresponding layer list\n",
    "    model_dct[layer_name].append({\n",
    "        'input': input_list,\n",
    "        'output': output_list\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "# Define the file path for the CSV file\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, count_matrix, label):\n",
    "        # Read the CSV file\n",
    "        self.data = count_matrix\n",
    "        # Separate features and target\n",
    "        self.features = self.data.values\n",
    "        self.target = label.values\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get features and target for a given index\n",
    "        features = torch.tensor(self.features[idx], dtype=torch.float32)\n",
    "        target = torch.tensor(self.target[idx], dtype=torch.float32)\n",
    "        return features, target\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predicted_list = []\n",
    "    probability_list = []\n",
    "    labels_list = []\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    loss = 0\n",
    "    with torch.no_grad():  # No need to compute gradients during evaluation\n",
    "        for features, labels in dataloader:\n",
    "            outputs = model(features)\n",
    "            #print(outputs)\n",
    "            probability = torch.sigmoid(outputs.data)\n",
    "            predicted = torch.round(torch.sigmoid(outputs.data))\n",
    "            #print(outputs)\n",
    "            #print(predicted)\n",
    "            loss += criterion(outputs, labels)\n",
    "            #_, predicted = torch.sigmoid(outputs.data)\n",
    "            predicted_list.extend(predicted)\n",
    "            labels_list.extend(labels)\n",
    "            probability_list.extend(probability)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    #print(total)\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy, loss, predicted_list, labels_list, probability_list\n",
    "\n",
    "def save_model(model_nn,model_path, model_state_dict_path):\n",
    "    \n",
    "    model_nn.eval()\n",
    "    torch.save(model_nn, model_path)\n",
    "    torch.save(model_nn.state_dict(), model_state_dict_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def model_fc(train_dataloader , val_dataloader, test_dataloader, test_cell_id, layers_node, masking, output_layer,model_save_dir, date_string, learning_rate=0.001, num_epochs=50, weight_decay = 0):\n",
    "\n",
    "    model_nn = CustomfcNetwork(layers_node, output_layer, masking)\n",
    "    optimizer = optim.AdamW(model_nn.parameters(), lr=learning_rate,weight_decay = weight_decay )  # Using SGD with momentum\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=5, verbose=True)\n",
    "    patience = 20\n",
    "    best_val_accuracy = 0.0\n",
    "    epochs_no_improve = 0\n",
    "    early_stop = False\n",
    "    csv_file_path = f'{model_save_dir}{date_string}/fc_training_log_{output_layer}.csv'\n",
    "\n",
    "    try:\n",
    "        os.makedirs(f'{model_save_dir}{date_string}')\n",
    "    except:\n",
    "        print(('...'))\n",
    "\n",
    "    with open(csv_file_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Epoch', 'Train_Loss', 'Train_accuracy','Validation_Loss','Val_accuracy'])\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        if early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        epoch_cost = 0.\n",
    "        \n",
    "        total_loss = 0\n",
    "        for batch_features,batch_targets in train_dataloader:\n",
    "            outputs = model_nn(batch_features)\n",
    "            #print(outputs)\n",
    "            #print(batch_targets)\n",
    "            #print(outputs)\n",
    "            loss = criterion(outputs, batch_targets)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "        \n",
    "        train_accuracy, train_loss, predicted_list_train, labels_list_train, train_probability_list = evaluate(model_nn, train_dataloader)\n",
    "        val_accuracy, val_loss, predicted_list_val, labels_list_val, val_probability_list = evaluate(model_nn, val_dataloader)\n",
    "        #scheduler.step(val_accuracy)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss.item():.4f}, Train_accuracy: {train_accuracy}, Val Loss: {val_loss.item():.4f}, Val_accuracy: {val_accuracy}')\n",
    "        with open(csv_file_path, mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([epoch + 1, loss.item(), train_accuracy, val_loss.item(), val_accuracy])\n",
    "        \n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            epochs_no_improve = 0\n",
    "        # Save the best model\n",
    "            model_path = f'{model_save_dir}{date_string}/fc_best_model_{output_layer}.pth'\n",
    "            model_state_dict_path = f'{model_save_dir}{date_string}/fc_best_model_{output_layer}_state_dict.pth'\n",
    "            save_model(model_nn, model_path, model_state_dict_path)\n",
    "            best_model_nn = copy.deepcopy(model_nn)\n",
    "            #torch.save(model_nn, f'{model_save_dir}{date_string}/fc_best_model_{output_layer}.pth')\n",
    "            #torch.save(model_nn.state_dict(), f'{model_save_dir}{date_string}/fc_best_model_{output_layer}_state_dict.pth')\n",
    "            print('Model saved.')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "    \n",
    "        # Early stopping\n",
    "        '''if epochs_no_improve >= patience:\n",
    "            early_stop = True\n",
    "            print(\"Early stopping triggered\")'''\n",
    "        \n",
    "    \n",
    "    train_accuracy, train_loss, predicted_list_train, labels_list_train, train_probability_list = evaluate(best_model_nn, train_dataloader)\n",
    "    val_accuracy, val_loss, predicted_list_val, labels_list_val, val_probability_list = evaluate(best_model_nn, val_dataloader)\n",
    "    test_accuracy, test_loss, predicted_list_test, labels_list_test, test_probability_list = evaluate(best_model_nn, test_dataloader)\n",
    "    print('Test Accucary', test_accuracy)\n",
    "    output_train = (predicted_list_train, labels_list_train)\n",
    "    output_val = (predicted_list_val, labels_list_val)\n",
    "\n",
    "    labels_list_test = [m.item() for m in labels_list_test]\n",
    "    predicted_list_test = [m.item() for m in predicted_list_test]\n",
    "    test_probability_list = [m.item() for m in test_probability_list]\n",
    "\n",
    "\n",
    "    test_df = pd.DataFrame({'cell_id': test_cell_id, 'true_y': labels_list_test, 'pred_y': predicted_list_test, 'probabilty': test_probability_list})\n",
    "    csv_file_path = f'{model_save_dir}{date_string}/fc_test_log_{output_layer}.csv'\n",
    "    test_df.to_csv(csv_file_path)\n",
    "    #torch.save(model_nn, f'{model_save_dir}{date_string}/fc_last_epoch_model_{output_layer}.pth')\n",
    "    return output_train, output_val,best_model_nn\n",
    "\n",
    "\n",
    "\n",
    "def model(train_dataloader , val_dataloader, test_dataloader, test_cell_id, layers_node, masking, output_layer,model_save_dir, date_string, learning_rate=0.001, num_epochs=50, weight_decay = 0):\n",
    "\n",
    "    model_nn = CustomNetwork(layers_node, output_layer, masking)\n",
    "    optimizer = optim.AdamW(model_nn.parameters(), lr=learning_rate,weight_decay = weight_decay )  # Using SGD with momentum\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=5, verbose=True)\n",
    "    patience = 20\n",
    "    best_val_accuracy = 0.0\n",
    "    epochs_no_improve = 0\n",
    "    early_stop = False\n",
    "    csv_file_path = f'{model_save_dir}{date_string}/training_log_{output_layer}.csv'\n",
    "\n",
    "    try:\n",
    "        os.makedirs(f'{model_save_dir}{date_string}')\n",
    "    except:\n",
    "        print(('...'))\n",
    "\n",
    "    with open(csv_file_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Epoch', 'Train_Loss', 'Train_accuracy','Validation_Loss','Val_accuracy'])\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        if early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        epoch_cost = 0.\n",
    "        \n",
    "        total_loss = 0\n",
    "        for batch_features,batch_targets in train_dataloader:\n",
    "            \n",
    "            #print(outputs)\n",
    "            #print(batch_targets)\n",
    "            #print(outputs)\n",
    "            \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model_nn(batch_features)\n",
    "            loss = criterion(outputs, batch_targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "        \n",
    "        train_accuracy, train_loss, predicted_list_train, labels_list_train, train_probability_list = evaluate(model_nn, train_dataloader)\n",
    "        val_accuracy, val_loss, predicted_list_val, labels_list_val, val_probability_list = evaluate(model_nn, val_dataloader)\n",
    "        #scheduler.step(val_accuracy)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss.item():.4f}, Train_accuracy: {train_accuracy}, Val Loss: {val_loss.item():.4f}, Val_accuracy: {val_accuracy}')\n",
    "        with open(csv_file_path, mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([epoch + 1, loss.item(), train_accuracy, val_loss.item(), val_accuracy])\n",
    "        \n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            epochs_no_improve = 0\n",
    "        # Save the best model\n",
    "            model_path = f'{model_save_dir}{date_string}/best_model_{output_layer}.pth'\n",
    "            model_state_dict_path = f'{model_save_dir}{date_string}/best_model_{output_layer}_state_dict.pth'\n",
    "            save_model(model_nn, model_path, model_state_dict_path)\n",
    "            best_model_nn = copy.deepcopy(model_nn)\n",
    "            #torch.save(model_nn, f'{model_save_dir}{date_string}/best_model_{output_layer}.pth')\n",
    "            #torch.save(model_nn.state_dict(), f'{model_save_dir}{date_string}/best_model_{output_layer}_state_dict.pth')\n",
    "            print('Model saved.')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "    \n",
    "        # Early stopping\n",
    "        '''if epochs_no_improve >= patience:\n",
    "            early_stop = True\n",
    "            print(\"Early stopping triggered\")'''\n",
    "        \n",
    "    \n",
    "    train_accuracy, train_loss, predicted_list_train, labels_list_train, train_probability_list = evaluate(best_model_nn, train_dataloader)\n",
    "    val_accuracy, val_loss, predicted_list_val, labels_list_val, val_probability_list = evaluate(best_model_nn, val_dataloader)\n",
    "    test_accuracy, test_loss, predicted_list_test, labels_list_test, test_probability_list = evaluate(best_model_nn, test_dataloader)\n",
    "    print('Test Accucary', test_accuracy)\n",
    "    output_train = (predicted_list_train, labels_list_train)\n",
    "    output_val = (predicted_list_val, labels_list_val)\n",
    "\n",
    "    labels_list_test = [m.item() for m in labels_list_test]\n",
    "    predicted_list_test = [m.item() for m in predicted_list_test]\n",
    "    test_probability_list = [m.item() for m in test_probability_list]\n",
    "\n",
    "\n",
    "    test_df = pd.DataFrame({'cell_id': test_cell_id, 'true_y': labels_list_test, 'pred_y': predicted_list_test, 'probabilty': test_probability_list})\n",
    "    csv_file_path = f'{model_save_dir}{date_string}/test_log_{output_layer}.csv'\n",
    "    test_df.to_csv(csv_file_path)\n",
    "    #torch.save(model_nn, f'{model_save_dir}{date_string}/last_epoch_model_{output_layer}.pth')\n",
    "    return output_train, output_val,best_model_nn\n",
    "\n",
    "\n",
    "def load_config(config_file):\n",
    "    with open(config_file, 'r') as file:\n",
    "        return yaml.safe_load(file)\n",
    "\n",
    "\n",
    "def main_file():\n",
    "\n",
    "    '''parser = argparse.ArgumentParser(description='Sample application with config and argparse')\n",
    "    parser.add_argument('--config', type=str, default='config.yml', help='Path to the configuration file')\n",
    "    args = parser.parse_args()'''\n",
    "\n",
    "    config = load_config('config.yml')\n",
    "    train = pd.read_csv(config['dataset']['train'],index_col=0)\n",
    "    test = pd.read_csv(config['dataset']['test'],index_col=0)\n",
    "    val = pd.read_csv(config['dataset']['val'],index_col=0)\n",
    "\n",
    "    y_train = pd.read_csv(config['dataset']['y_train'])\n",
    "    y_test = pd.read_csv(config['dataset']['y_test'])\n",
    "    y_val = pd.read_csv(config['dataset']['y_val'])\n",
    "  \n",
    "\n",
    "\n",
    "    r_data_tmp = train.T\n",
    "    q_data_tmp = test.T\n",
    "    v_data_tmp = val.T\n",
    "    r_label_tmp = y_train\n",
    "\n",
    "    print('Getting Marker Genes.......')\n",
    "    train_x, test_x, val_x, train_y = get_expression(r_data_tmp,\n",
    "                                                q_data_tmp,\n",
    "                                                v_data_tmp,\n",
    "                                                r_label_tmp,\n",
    "                                                thrh=config['gene_expression']['highly_expressed_threshold'],\n",
    "                                                thrl=config['gene_expression']['lowly_expressed_threshold'],\n",
    "                                                normalization=config['gene_expression']['normalization'],\n",
    "                                                marker=config['gene_expression']['marker'])\n",
    "    \n",
    "    print('Getting Pathway Genes.........')\n",
    "    pathway_genes = get_gene_pathways(config['pathways_network']['ensemble_pathway_relation'], species=config['pathways_network']['species'])\n",
    "\n",
    "\n",
    "    print('Getting Masking.........')\n",
    "    masking, masking_df, layers_node, train_x, test_x,val_x = get_masking(config['pathways_network']['pathway_names'],\n",
    "                                                        pathway_genes,\n",
    "                                                        config['pathways_network']['pathway_relation'],\n",
    "                                                        train_x,\n",
    "                                                        test_x,\n",
    "                                                        val_x,\n",
    "                                                        train_y,\n",
    "                                                        config['pathways_network']['datatype'],\n",
    "                                                        config['pathways_network']['species'],\n",
    "                                                        config['pathways_network']['n_hidden_layer'])\n",
    "\n",
    "    test_cell_id = list(test_x.T.index) \n",
    "    try:\n",
    "        masking = list(masking.values())\n",
    "        layers_node = list(layers_node.values())\n",
    "    except:\n",
    "        print('already_done')\n",
    "\n",
    "\n",
    "    train_dataset = TabularDataset(train_x.T,train_y)\n",
    "    val_dataset = TabularDataset(val_x.T,y_val)\n",
    "    test_dataset = TabularDataset(test_x.T,y_test)  \n",
    "    \n",
    "    \n",
    "\n",
    "    dataloader_params = {\n",
    "    'batch_size': config['train']['batch_size'],\n",
    "    'shuffle': False\n",
    "    }\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset,**dataloader_params)\n",
    "    test_dataloader = DataLoader(test_dataset, **dataloader_params)\n",
    "    val_dataloader = DataLoader(val_dataset,**dataloader_params)\n",
    "    # Example of iterating through the DataLoader\n",
    "\n",
    "\n",
    "    pred_y_df = pd.DataFrame(data=0, index=test_x.columns, columns=list(range(2, len(masking) + 2)))\n",
    "    train_y_df = pd.DataFrame(data=0, index=train_x.columns, columns=list(range(2, len(masking) + 2)))\n",
    "    model_dict_sparse = dict()\n",
    "    model_dict_fc = dict()\n",
    "    activation_output = {}\n",
    "    now = datetime.now()\n",
    "\n",
    "# Format the date as a string\n",
    "    date_string = datetime_string = now.strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "    try:\n",
    "        os.makedirs(f'{config['model_output']['model_save_dir']}{date_string}')\n",
    "    except:\n",
    "        print(('...'))\n",
    "\n",
    "   \n",
    "\n",
    "    print('Training.........')\n",
    "    for output_layer in range(2, len(masking) + 2):\n",
    "        if config['gene_expression']['print_information']:\n",
    "            print(\"Current sub-neural network has \" + str(output_layer - 1) + \" hidden layers.\")\n",
    "        output_train, output_val,model_dict_sparse[output_layer] = model(train_dataloader,\n",
    "                                            val_dataloader,test_dataloader, test_cell_id,\n",
    "                                            layers_node,\n",
    "                                            masking,\n",
    "                                            output_layer,\n",
    "                                            model_save_dir = config['model_output']['model_save_dir'],date_string = date_string,\n",
    "                                            learning_rate=config['train']['learning_rate'],num_epochs=config['train']['epochs'],weight_decay = config['train']['weight_decay']\n",
    "                                        )  \n",
    "\n",
    "    print('tranining_fully_connected_layers:')\n",
    "    for output_layer in range(2, len(masking) + 2):\n",
    "        if config['gene_expression']['print_information']:\n",
    "            print(\"Current sub-neural network has \" + str(output_layer - 1) + \" hidden layers.\")\n",
    "        output_train, output_val,model_dict_fc[output_layer] = model_fc(train_dataloader,\n",
    "                                            val_dataloader,test_dataloader, test_cell_id,\n",
    "                                            layers_node,\n",
    "                                            masking,\n",
    "                                            output_layer,\n",
    "                                            model_save_dir = config['model_output']['model_save_dir'],date_string = date_string,\n",
    "                                            learning_rate=config['train']['learning_rate'],num_epochs=config['train']['epochs'],weight_decay = config['train']['weight_decay']\n",
    "                                        )  \n",
    "        \n",
    "    new_parameter = {'date_string': date_string}\n",
    "    config.update(new_parameter)\n",
    "    save_path =   str(config['model_output']['model_save_dir'])+ date_string + '/config.yml'\n",
    "    with open(save_path, 'w') as file:\n",
    "        yaml.dump(config, file)\n",
    "\n",
    "        \n",
    "    for i in range(len(masking_df)):\n",
    "        masking_df[i].to_csv(str(config['model_output']['model_save_dir'])+ date_string+ '/' +f'masking_df_{i}.csv')\n",
    "    \n",
    "    dataloader_params = {\n",
    "    'batch_size': 1,\n",
    "    'shuffle': False\n",
    "    }\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset,**dataloader_params)\n",
    "    test_dataloader = DataLoader(test_dataset, **dataloader_params)\n",
    "    val_dataloader = DataLoader(val_dataset,**dataloader_params)\n",
    "    \n",
    "    for j,i in model_dict_sparse.items():\n",
    "        \n",
    "        for name, layer in enumerate(i.children()):\n",
    "            layer_name = 'fc'+str(name+1)\n",
    "            layer.register_forward_hook(lambda module, input, output, name=layer_name: hook_fn(module, input, output, name))\n",
    "\n",
    "\n",
    "\n",
    "        accuracy, loss, predicted_list, labels_list, probability_list = evaluate(i, test_dataloader)\n",
    "        print(j)\n",
    "        print(f'Test Accuracy: {accuracy}')   \n",
    "        accuracy, loss, predicted_list, labels_list, probability_list = evaluate(i, train_dataloader)\n",
    "        print(f'Train Accuracy: {accuracy}')   \n",
    "        accuracy, loss, predicted_list, labels_list, probability_list = evaluate(i, val_dataloader)\n",
    "        print(f'Validation Accuracy: {accuracy}') \n",
    "\n",
    "        '''with open('model_activations_train_test.csv', 'w', newline='') as csvfile:\n",
    "            fieldnames = ['layer', 'input', 'output']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            \n",
    "            writer.writeheader()\n",
    "            for layer_name, activations in model_dct.items():\n",
    "                for activation in activations:\n",
    "                    writer.writerow({\n",
    "                        'layer': layer_name,\n",
    "                        'input': activation['input'],\n",
    "                        'output': activation['output']\n",
    "                    })'''\n",
    "        \n",
    "        break\n",
    "        \n",
    "    return model_dict_sparse, test_dataloader, train_dataloader, config\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "                                            \n",
    "\n",
    "\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "06350243-c2e9-4cc7-8304-3604395ebbad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Marker Genes.......\n",
      "1125\n",
      "1125\n",
      "2250\n",
      "2250\n",
      "                    0                1\n",
      "0     ENSG00000101210  ENSG00000172270\n",
      "1     ENSG00000099622  ENSG00000141905\n",
      "2     ENSG00000105278  ENSG00000167658\n",
      "3     ENSG00000178951  ENSG00000089847\n",
      "4     ENSG00000141985  ENSG00000127663\n",
      "...               ...              ...\n",
      "1120  ENSG00000285395  ENSG00000103316\n",
      "1121  ENSG00000140740  ENSG00000284218\n",
      "1122  ENSG00000122254  ENSG00000103365\n",
      "1123  ENSG00000006116  ENSG00000182601\n",
      "1124  ENSG00000077235  ENSG00000171208\n",
      "\n",
      "[1125 rows x 2 columns]\n",
      "                    0                1\n",
      "0     ENSG00000101210  ENSG00000172270\n",
      "1     ENSG00000099622  ENSG00000141905\n",
      "2     ENSG00000105278  ENSG00000167658\n",
      "3     ENSG00000178951  ENSG00000089847\n",
      "4     ENSG00000141985  ENSG00000127663\n",
      "...               ...              ...\n",
      "1120  ENSG00000285395  ENSG00000103316\n",
      "1121  ENSG00000140740  ENSG00000284218\n",
      "1122  ENSG00000122254  ENSG00000103365\n",
      "1123  ENSG00000006116  ENSG00000182601\n",
      "1124  ENSG00000077235  ENSG00000171208\n",
      "\n",
      "[1125 rows x 2 columns]\n",
      "Getting Pathway Genes.........\n",
      "Getting Masking.........\n",
      "HSA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anwer/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training.........\n",
      "Current sub-neural network has 1 hidden layers.\n",
      "...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Train Loss: 4.8215, Train_accuracy: 55.59815406460774, Val Loss: 2.0674, Val_accuracy: 55.484558040468585\n",
      "Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accucary 55.51533219761499\n",
      "Current sub-neural network has 2 hidden layers.\n",
      "...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Train Loss: 4.8551, Train_accuracy: 49.82605608803692, Val Loss: 2.0810, Val_accuracy: 50.24494142705005\n",
      "Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accucary 50.27683134582624\n",
      "Current sub-neural network has 3 hidden layers.\n",
      "...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Train Loss: 4.8596, Train_accuracy: 49.82605608803692, Val Loss: 2.0831, Val_accuracy: 50.24494142705005\n",
      "Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accucary 50.27683134582624\n",
      "tranining_fully_connected_layers:\n",
      "Current sub-neural network has 1 hidden layers.\n",
      "...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Train Loss: 4.6664, Train_accuracy: 57.706780262690806, Val Loss: 2.0036, Val_accuracy: 56.48562300319489\n",
      "Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accucary 57.389267461669505\n",
      "Current sub-neural network has 2 hidden layers.\n",
      "...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Train Loss: 4.5107, Train_accuracy: 61.49094781682641, Val Loss: 1.9456, Val_accuracy: 60.298189563365284\n",
      "Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accucary 60.83901192504259\n",
      "Current sub-neural network has 3 hidden layers.\n",
      "...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Train Loss: 4.2285, Train_accuracy: 69.20127795527156, Val Loss: 1.8249, Val_accuracy: 69.05218317358893\n",
      "Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accucary 68.99488926746167\n",
      "2\n",
      "Test Accuracy: 55.51533219761499\n",
      "Train Accuracy: 55.59815406460774\n",
      "Validation Accuracy: 55.484558040468585\n"
     ]
    }
   ],
   "source": [
    "model_dict_sparse, test_dataloader, train_dataloader, config = main_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "45d2b672-8d76-418e-9655-42237bc6a3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Test Accuracy: 55.51533219761499\n",
      "Train Accuracy: 55.59815406460774\n",
      "3\n",
      "Test Accuracy: 50.27683134582624\n",
      "Train Accuracy: 49.82605608803692\n",
      "4\n",
      "Test Accuracy: 50.27683134582624\n",
      "Train Accuracy: 49.82605608803692\n"
     ]
    }
   ],
   "source": [
    "for j,i in model_dict_sparse.items():\n",
    "        \n",
    "        '''for name, layer in enumerate(i.children()):\n",
    "            layer_name = 'fc'+str(name+1)\n",
    "            layer.register_forward_hook(lambda module, input, output, name=layer_name: hook_fn(module, input, output, name))'''\n",
    "\n",
    "\n",
    "\n",
    "        accuracy, loss, predicted_list, labels_list, probability_list = evaluate(i, test_dataloader)\n",
    "        print(j)\n",
    "        print(f'Test Accuracy: {accuracy}')   \n",
    "        accuracy, loss, predicted_list, labels_list, probability_list = evaluate(i, train_dataloader)\n",
    "        print(f'Train Accuracy: {accuracy}')   \n",
    "        '''accuracy, loss, predicted_list, labels_list, probability_list = evaluate(i, val_dataloader)\n",
    "        print(f'Validation Accuracy: {accuracy}') '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebdcd46a-27e3-46e5-84c6-81b1e2cad65f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: CustomNetwork(\n",
       "   (layers): ModuleList(\n",
       "     (0): Linear(in_features=582, out_features=356, bias=False)\n",
       "     (1): Linear(in_features=356, out_features=1, bias=True)\n",
       "   )\n",
       " ),\n",
       " 3: CustomNetwork(\n",
       "   (layers): ModuleList(\n",
       "     (0): Linear(in_features=582, out_features=356, bias=False)\n",
       "     (1): Linear(in_features=356, out_features=134, bias=False)\n",
       "     (2): Linear(in_features=134, out_features=1, bias=True)\n",
       "   )\n",
       " ),\n",
       " 4: CustomNetwork(\n",
       "   (layers): ModuleList(\n",
       "     (0): Linear(in_features=582, out_features=356, bias=False)\n",
       "     (1): Linear(in_features=356, out_features=134, bias=False)\n",
       "     (2): Linear(in_features=134, out_features=29, bias=False)\n",
       "     (3): Linear(in_features=29, out_features=1, bias=True)\n",
       "   )\n",
       " )}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dict_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd39dae9-ef4d-4955-a595-ebd509816c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': {'train': '/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/Preprocessed_data/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/train.csv',\n",
       "  'test': '/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/Preprocessed_data/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/test.csv',\n",
       "  'val': '/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/Preprocessed_data/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/val.csv',\n",
       "  'y_train': '/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/Preprocessed_data/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/y_train.csv',\n",
       "  'y_test': '/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/Preprocessed_data/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/y_test.csv',\n",
       "  'y_val': '/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/Preprocessed_data/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/y_val.csv'},\n",
       " 'model_output': {'model_save_dir': '/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/model_save/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/'},\n",
       " 'train': {'epochs': 50,\n",
       "  'learning_rate': 0.001,\n",
       "  'weight_decay': 0.0001,\n",
       "  'batch_size': 2048},\n",
       " 'gene_expression': {'highly_expressed_threshold': 0.95,\n",
       "  'lowly_expressed_threshold': 0.95,\n",
       "  'normalization': True,\n",
       "  'marker': True,\n",
       "  'print_information': True},\n",
       " 'pathways_network': {'species': 'human',\n",
       "  'n_hidden_layer': 3,\n",
       "  'pathway_relation': '../../usman/CellTICS/reactome/ReactomePathwaysRelation.txt',\n",
       "  'pathway_names': '../../usman/CellTICS/reactome/ReactomePathways.txt',\n",
       "  'ensemble_pathway_relation': '../../usman/CellTICS/reactome/Ensembl2Reactome_All_Levels.txt',\n",
       "  'datatype': 'diagnosis'},\n",
       " 'date_string': '2024_08_04_09_14_03'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92837538-2071-4c57-bbec-2b80ed06fd1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/model_save/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/2024_08_04_09_14_03/'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['model_output']['model_save_dir']+ config['date_string']+'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08f4f5e1-65f9-4578-b028-29f7fc367a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j,i in model_dict_sparse.items():\n",
    "\n",
    "# Assuming 'model' is your neural network\n",
    "    torch.save(i.state_dict(), f'{config['model_output']['model_save_dir']}{config['date_string']}/model_{j}_state_dict_jupyter_notebook.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9fc03fb3-f2fa-41b3-958c-37e366ec412b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "#buffer = io.BytesIO(model_data)\n",
    "\n",
    "model_2= torch.load('/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/model_save/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/2024_08_04_09_14_03//model_2_state_dict_jupyter_notebook.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "89a3dcb1-1388-4919-8532-22318af3b465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('layers.0.weight',\n",
       "              tensor([[-0., 0., 0.,  ..., -0., -0., 0.],\n",
       "                      [-0., -0., 0.,  ..., -0., -0., 0.],\n",
       "                      [-0., 0., 0.,  ..., 0., -0., 0.],\n",
       "                      ...,\n",
       "                      [-0., 0., 0.,  ..., 0., -0., 0.],\n",
       "                      [0., -0., -0.,  ..., -0., 0., -0.],\n",
       "                      [-0., -0., 0.,  ..., -0., 0., -0.]])),\n",
       "             ('layers.1.weight',\n",
       "              tensor([[-1.3116e-01, -1.0516e-01, -6.4996e-02, -2.5069e-01,  1.8438e-01,\n",
       "                        3.8274e-02, -1.7296e-01,  3.2673e-03,  1.6031e-01, -2.3190e-02,\n",
       "                        3.5430e-02, -1.7787e-01,  1.8825e-01, -6.6232e-03,  7.4582e-02,\n",
       "                       -2.9358e-01, -4.3745e-03,  3.6318e-02, -2.4986e-01, -3.0154e-03,\n",
       "                       -5.2295e-02, -9.7599e-02, -3.4880e-01,  8.7646e-03, -1.6985e-01,\n",
       "                        3.1090e-02,  4.1819e-02, -2.0573e-01, -2.7015e-02,  1.3528e-02,\n",
       "                        1.0335e-02,  1.5623e-01, -1.3668e-01, -8.6235e-02, -4.6132e-02,\n",
       "                       -1.0665e-01,  2.5028e-01,  8.2761e-02, -1.3398e-01, -2.1150e-01,\n",
       "                        1.7943e-02, -2.8306e-01,  3.5656e-01, -7.6278e-02, -1.0384e-01,\n",
       "                       -1.1037e-01, -2.5840e-03, -4.1085e-02,  2.0511e-01, -2.5892e-01,\n",
       "                       -1.0389e-01, -1.8096e-02, -1.5778e-01, -1.2939e-01, -2.9702e-04,\n",
       "                        1.3670e-01, -1.5485e-01,  5.6017e-02, -4.5186e-02, -2.1758e-01,\n",
       "                        1.9472e-01, -1.9895e-01, -1.1565e-01,  3.3664e-02, -2.6155e-01,\n",
       "                        1.5127e-01,  3.9269e-01, -2.6482e-01, -1.2881e-01,  4.8615e-02,\n",
       "                       -2.3162e-01,  2.2908e-01, -2.5703e-01, -9.8018e-02, -3.0450e-01,\n",
       "                       -2.4761e-02, -3.3299e-02,  7.5300e-02,  3.7611e-02,  2.5441e-02,\n",
       "                        1.5974e-01, -1.5107e-02, -2.0112e-01,  6.4118e-02, -2.2307e-01,\n",
       "                        1.4751e-01, -1.4785e-01, -3.4074e-01,  8.1712e-02, -4.3351e-02,\n",
       "                       -2.5296e-01,  2.4967e-02,  2.3336e-02, -1.4996e-01, -2.3613e-01,\n",
       "                       -3.7656e-01, -5.0436e-02, -1.3937e-01,  1.9226e-01, -3.1904e-02,\n",
       "                        1.9797e-01,  1.4616e-01, -3.6032e-01,  3.0647e-02,  2.1271e-01,\n",
       "                       -3.3335e-01, -1.1642e-01,  9.0645e-02,  1.4707e-01,  1.5379e-02,\n",
       "                       -1.9624e-01,  1.1979e-01, -4.0942e-02, -5.7790e-03, -2.9915e-01,\n",
       "                       -1.9028e-01,  2.1570e-02,  9.6800e-02,  1.3758e-01,  1.7420e-01,\n",
       "                       -1.9045e-01,  9.9033e-02, -1.6638e-01, -8.8647e-02, -9.8911e-02,\n",
       "                        1.3297e-01, -2.4583e-01,  1.9330e-01, -1.2462e-01, -1.2116e-01,\n",
       "                       -2.2984e-01, -1.7884e-01,  4.9538e-03,  2.0699e-01,  2.2506e-03,\n",
       "                        3.3621e-03, -1.6270e-02,  6.3733e-02,  1.3999e-01,  1.5152e-01,\n",
       "                       -9.3050e-02, -2.1527e-01,  1.8704e-03, -9.0649e-02,  1.3749e-02,\n",
       "                        2.3243e-01,  2.2031e-01, -1.7323e-02, -1.3524e-01,  1.1130e-02,\n",
       "                       -2.0052e-02,  1.8057e-01, -3.6982e-01,  3.7368e-02, -9.6539e-02,\n",
       "                       -1.0549e-01,  1.9610e-01, -4.9053e-02, -2.9567e-01,  1.1140e-01,\n",
       "                        2.2781e-01, -1.3134e-01,  9.4933e-03, -3.1612e-01,  4.8539e-02,\n",
       "                       -4.8723e-02, -6.0552e-02, -2.4209e-01, -2.3722e-01, -9.8044e-02,\n",
       "                       -2.4943e-01,  2.8393e-01,  9.7186e-02,  1.4960e-02, -3.3781e-01,\n",
       "                       -3.3887e-02,  2.2379e-02,  1.8183e-02, -5.4697e-02,  3.7118e-01,\n",
       "                        6.8498e-03,  1.7861e-02,  3.2873e-02,  1.4564e-02, -1.6984e-01,\n",
       "                       -1.7066e-02, -1.7798e-01,  2.5348e-01, -1.7167e-01,  2.1883e-01,\n",
       "                        2.1894e-01, -2.5419e-01, -2.9113e-02, -2.6782e-02,  1.6828e-01,\n",
       "                        5.4134e-02, -1.5748e-01, -9.4665e-02,  2.2210e-02, -1.4015e-01,\n",
       "                       -1.2758e-01, -1.6322e-01, -4.7880e-02,  3.8906e-02,  1.7839e-02,\n",
       "                       -1.4930e-02,  3.6862e-01,  4.8419e-02, -1.5821e-01, -3.2601e-02,\n",
       "                       -3.0431e-01, -1.7886e-01, -4.4351e-03, -2.7987e-01,  9.1090e-03,\n",
       "                        1.0537e-01,  2.5934e-01, -1.5868e-01, -1.6812e-01,  1.7000e-02,\n",
       "                       -6.0238e-02, -5.0325e-02,  1.3287e-01,  3.9202e-02,  5.2408e-02,\n",
       "                       -1.9872e-01, -1.1820e-01, -3.8946e-02, -1.8903e-01, -2.5686e-02,\n",
       "                       -2.0091e-01,  2.4857e-02, -5.2582e-02,  1.6018e-01,  9.8203e-02,\n",
       "                        1.1396e-01,  1.7601e-01,  2.0068e-02,  2.2352e-01,  3.3895e-02,\n",
       "                        8.4266e-02, -8.9212e-03,  5.0120e-03,  4.0373e-02,  4.0930e-02,\n",
       "                       -8.1099e-02, -4.2869e-02,  3.7092e-02, -1.8236e-01, -2.4039e-02,\n",
       "                        1.8055e-01, -1.3768e-01, -5.1443e-02, -1.2325e-01, -3.7819e-02,\n",
       "                       -1.9504e-01,  1.9361e-01,  1.5900e-01, -2.2917e-02,  1.3212e-01,\n",
       "                       -2.5946e-02,  1.3815e-01, -2.4546e-01,  3.1734e-01,  1.2548e-01,\n",
       "                       -1.6856e-01,  2.3920e-02, -1.1849e-01,  4.6018e-02, -1.2681e-01,\n",
       "                       -2.4730e-02, -2.1248e-02, -1.5184e-01,  1.2731e-01, -2.5187e-01,\n",
       "                       -2.4985e-01, -2.0772e-02,  1.6314e-01,  2.1617e-01,  1.0589e-01,\n",
       "                        4.9637e-02, -1.2481e-01, -1.9570e-01,  1.2009e-01, -2.0000e-02,\n",
       "                       -7.5369e-02, -3.6133e-02, -2.0581e-01, -1.1868e-02,  1.1595e-01,\n",
       "                       -2.3598e-02, -3.6430e-03,  1.3154e-01, -3.7404e-02, -1.0250e-01,\n",
       "                       -7.4699e-02, -1.8633e-01,  2.0087e-01, -1.2359e-01, -1.0681e-02,\n",
       "                       -3.4879e-01, -2.1710e-02, -1.6875e-01, -2.1863e-02, -1.2722e-04,\n",
       "                       -2.1290e-01, -2.7671e-01,  6.5145e-02,  6.4905e-02,  3.8096e-02,\n",
       "                       -1.2981e-01,  3.2434e-01, -2.2665e-01, -1.1787e-01,  5.4196e-03,\n",
       "                        1.5723e-02,  5.1469e-03, -4.8237e-02, -1.4786e-01,  2.5394e-02,\n",
       "                        3.5050e-02,  6.9304e-02,  2.9839e-02,  1.1020e-01, -2.3707e-01,\n",
       "                        8.1304e-02, -3.0363e-01, -7.9545e-02, -1.1827e-01, -1.5128e-01,\n",
       "                        7.9678e-02, -2.4462e-01,  4.2871e-02,  2.6531e-01, -2.6205e-02,\n",
       "                        1.0759e-01, -2.0768e-01, -4.0479e-02, -1.8229e-01,  8.0962e-02,\n",
       "                       -3.0042e-01, -2.8287e-02,  1.3006e-01,  1.4210e-01, -2.9546e-03,\n",
       "                       -1.8719e-01,  3.0843e-01,  2.8184e-01,  6.3904e-02,  1.3898e-02,\n",
       "                       -1.9359e-02,  2.3425e-01,  1.5409e-02, -1.8032e-01,  9.6409e-02,\n",
       "                       -4.3016e-02]])),\n",
       "             ('layers.1.bias', tensor([-0.1046]))])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1337dbaa-8e9a-4fb1-a14c-644ccae6e860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = model_dict_sparse[2]\n",
    "m.load_state_dict(model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e568884b-5790-4145-aae8-206e137f2ed6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75.74531516183987"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy, loss, predicted_list, labels_list, probability_list = evaluate(m , test_dataloader)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f99d48-aff4-4fcc-a907-e42615ea3c43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad23bd3a-7053-4dbd-8fd3-e2afa2788b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 14:11:36.224737: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-31 14:11:36.227061: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-31 14:11:36.227177: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f67b552-3b72-4f66-8480-cd51321746bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35d2b45d-6ae0-4027-bf66-fe4a3ff6272a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_2122442/337460670.py:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 14:11:36.242744: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 14:11:36.242868: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-31 14:11:36.242962: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-31 14:11:36.410684: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-31 14:11:36.410822: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-31 14:11:36.410920: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-31 14:11:36.411180: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /device:GPU:0 with 46184 MB memory:  -> device: 0, name: NVIDIA RTX 6000 Ada Generation, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0b9768d-f539-432e-aac3-7fae8725b8ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2b271dd-a27d-46c2-a6a0-599b56652736",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdata = pd.read_csv('../../usman/Single_Cell_Microglia_Project/preprocessed_data/inhibitory_neuron/train_test_set/train_count_matrix.csv', index_col=0)\n",
    "qdata = pd.read_csv('../../usman/Single_Cell_Microglia_Project/preprocessed_data/inhibitory_neuron/train_test_set/test_count_matrix.csv', index_col=0)\n",
    "rlabel = pd.read_csv('../../usman/Single_Cell_Microglia_Project/preprocessed_data/inhibitory_neuron/train_test_set/train_label.csv')\n",
    "qlabel = pd.read_csv('../../usman/Single_Cell_Microglia_Project/preprocessed_data/inhibitory_neuron/train_test_set/test_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f78476-e380-4781-84ee-85c7eaf73454",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_cell_metadata = pd.read_csv('../../usman/Single_Cell_Microglia_Project/preprocessed_data/inhibitory_neuron/metadata_inhibitory_neurons.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e49cc13-d416-4952-90ad-0e0299280b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_cell_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bd28fc-a41f-4979-a37c-4ebc6036fe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying Train Label is correct\n",
    "\n",
    "list(single_cell_metadata.set_index('cell_id').loc[rdata.columns,].clinical_pathological_AD) == list(rlabel.diagnosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef71fcf3-c70c-4aab-959a-4cf401e3c23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying Test Label is correct\n",
    "\n",
    "list(single_cell_metadata.set_index('cell_id').loc[qdata.columns,].clinical_pathological_AD) == list(qlabel.diagnosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456392b6-e593-4ff1-953b-14605a5b90cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdata = rdata.T\n",
    "qdata = qdata.T\n",
    "rdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5903d618-ed12-446f-9d7b-8507819a152d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdata['diagnosis'] =  list(rlabel.diagnosis)\n",
    "qdata['diagnosis'] =  list(qlabel.diagnosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e65f9e-a6f5-4332-831a-3400bafe8a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdata.diagnosis.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ed4a30-1a3b-4657-a955-98d0d61db7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdata_neg = rdata[rdata.diagnosis == 'NCI_with_No_Plaques'].iloc[0:24372,]\n",
    "rdata_pos = rdata[rdata.diagnosis == 'AD_with_Plaques'].iloc[0:24372,]\n",
    "rdata = pd.concat([rdata_neg,rdata_pos])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a0cf24-3b00-4b34-ae43-be84c38b436b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ef6d9b-0fd1-4976-8ea4-b9c4e560a201",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdata = rdata.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1c67bb-ec0b-4543-bf7e-20d690d5f409",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4224a563-4b37-41cc-a0c6-19603c3978b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rlabel = pd.DataFrame(rdata.diagnosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ceed006-fb1e-4bb6-a280-b4a80afc8bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rlabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf296654-9ecd-40e7-8f4d-d7324b9d563a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdata = rdata.drop(columns=['diagnosis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad4e81b-3de4-493e-a880-595df299b1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdata.diagnosis.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee8929c-5124-4c2f-b5a7-5ec0b4ab1ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdata_neg = qdata[qdata.diagnosis == 'NCI_with_No_Plaques'].iloc[0:3442,]\n",
    "qdata_pos =qdata[qdata.diagnosis == 'AD_with_Plaques'].iloc[0:3442,]\n",
    "qdata = pd.concat([qdata_neg,qdata_pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9166d3-49f0-4369-a58e-c12d74f4efd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a8cf1e-fac9-41d6-abc4-5e79ded58381",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdata = qdata.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0672cf56-8f9c-4b8e-a5b2-900d525a2277",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddca9ddc-55eb-47a4-9be8-deba3705d88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "qlabel = pd.DataFrame(qdata.diagnosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97811f88-7fdc-4fec-b631-531cf100b86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "qlabel.reset_index(drop = True, inplace = True)\n",
    "rlabel.reset_index(drop = True, inplace = True)\n",
    "rlabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e977db80-d292-47f0-a2ff-0494b9ce04ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdata = qdata.drop(columns=['diagnosis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea21e13e-bf27-4edc-b722-c78c3c12ff0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62828a1-0bb4-4b0b-a01f-1531cc232114",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdata = rdata.T\n",
    "qdata = qdata.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebb908d-164c-466d-999e-2530af6cb48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_data_tmp = rdata.copy()\n",
    "q_data_tmp = qdata.copy()\n",
    "r_label_tmp = rlabel.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07757c6b-64c2-430e-b906-2d42e7202296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def fphi(x):\n",
    "  zt = (1 - (x + 1).div(x.max(axis=1) + 1, axis=0)).sum(axis=1) / (x.shape[1] - 1)\n",
    "  zp = (x + 1).div(x.max(axis=1) + 1, axis=0).mul(zt, axis=0)\n",
    "  return zp\n",
    "\n",
    "\n",
    "def frho(x):\n",
    "  zt = (1 - (1 / (x + 1)).mul(x.min(axis=1) + 1, axis=0)).sum(axis=1) / (x.shape[1] - 1)\n",
    "  zp = (1 / (x + 1)).mul(x.min(axis=1) + 1, axis=0).mul(zt, axis=0)\n",
    "  return zp\n",
    "\n",
    "\n",
    "def fmkg(phi, rho, thr1=0.95, thr2=0.9):\n",
    "  #print(phi)\n",
    "  #print(rho)\n",
    "  gnm = phi.index.values\n",
    "  ctpnm = phi.columns.values\n",
    "  phi = np.array(phi)\n",
    "  rho = np.array(rho)\n",
    "  nummkg1 = round((1 - thr1) * phi.shape[0])\n",
    "  nummkg2 = round((1 - thr2) * phi.shape[0])\n",
    "  print(nummkg1)\n",
    "  print(nummkg2)\n",
    "  alpha = []\n",
    "  beta = []\n",
    "  for i in range(0, phi.shape[1]):\n",
    "    alpha = np.append(alpha, np.quantile(phi[:, i], thr1))\n",
    "    beta = np.append(beta, np.quantile(rho[:, i], thr2))\n",
    "  mkh = []\n",
    "  mkl = []\n",
    "  for i in range(0, phi.shape[1]):\n",
    "    mkh = np.concatenate([mkh, gnm[phi[:, i] >= alpha[i]][0:nummkg1]], axis=0)\n",
    "    mkl = np.concatenate([mkl, gnm[rho[:, i] >= beta[i]][0:nummkg2]], axis=0)\n",
    "  \n",
    "  print(len(mkh))\n",
    "  print(len(mkl))\n",
    "  mkh = mkh.reshape(nummkg1, phi.shape[1])\n",
    "  mkl = mkl.reshape(nummkg2, rho.shape[1])\n",
    "  mkh = pd.DataFrame(mkh, columns=ctpnm)\n",
    "  mkl = pd.DataFrame(mkl, columns=ctpnm)\n",
    "  print(mkh)\n",
    "  print(mkl)\n",
    "  return mkh, mkl\n",
    "\n",
    "\n",
    "def get_expression(rdata, qdata, rlabel, thrh=0.95, thrl=0.9, normalization=True, marker=True):\n",
    "    # calculate sum of cell type\n",
    "    rulabel = rlabel.iloc[:, 0].unique()\n",
    "    rdt = pd.DataFrame(data=None, columns=None)\n",
    "    for l in rulabel:\n",
    "      rdata_l = rdata.iloc[:, rlabel[(rlabel[\"diagnosis\"] == l)].index.tolist()] #row value in rlabel, is column in rdata\n",
    "      zs = rdata_l.apply(lambda x: x.sum(), axis=1)\n",
    "      rdt = pd.concat([rdt, pd.DataFrame(data=zs, columns=[l])], axis=1)\n",
    "      \n",
    "\n",
    "    # normalization\n",
    "    if normalization:\n",
    "        rdt_df = rdt\n",
    "        rdata_df = rdata\n",
    "        qdata_df = qdata\n",
    "        rdt = np.array(rdt_df, dtype=np.float32)\n",
    "        rdata = np.array(rdata_df, dtype=np.float32)\n",
    "        qdata = np.array(qdata_df, dtype=np.float32)\n",
    "        rdt = np.divide(rdt, np.sum(rdt, axis=0, keepdims=True)) * 10000\n",
    "        rdata = np.log2(np.divide(rdata, np.sum(rdata, axis=0, keepdims=True)) * 10000 + 1)\n",
    "        qdata = np.log2(np.divide(qdata, np.sum(qdata, axis=0, keepdims=True)) * 10000 + 1)\n",
    "\n",
    "        rdt = pd.DataFrame(data=rdt, columns=rdt_df.columns, index=rdt_df.index)\n",
    "        rdata = pd.DataFrame(data=rdata, columns=rdata_df.columns, index=rdata_df.index)\n",
    "        qdata = pd.DataFrame(data=qdata, columns=qdata_df.columns, index=qdata_df.index)\n",
    "    #print(rdt.head())\n",
    "    #print(rdata.head())\n",
    "    # match gene ensembl ids between the query data and the reference data\n",
    "    nm = qdata.index.tolist()\n",
    "    zg = pd.DataFrame(data=nm, columns=['gene'])\n",
    "    gid = []\n",
    "    for i in range(0, len(nm)):\n",
    "      if nm[i] not in rdata.index.tolist():\n",
    "        zg.iloc[i] = ''\n",
    "      else:\n",
    "        gid = gid + [nm[i]]\n",
    "    qdata = qdata.loc[gid, :]\n",
    "    rdata = rdata.loc[gid, :]\n",
    "    rdt = rdt.loc[gid, :]\n",
    "\n",
    "    rlabel.index = rdata.columns\n",
    "\n",
    "    if not marker:\n",
    "        train_x = rdata\n",
    "        train_y = rlabel\n",
    "        test_x = qdata\n",
    "    else:\n",
    "        # calculate important genes\n",
    "        phi = fphi(rdt)\n",
    "        rho = frho(rdt)\n",
    "        mkh, mkl = fmkg(phi, rho, thrh, thrl)\n",
    "        mkg = np.unique(np.append(np.array(mkh).reshape(1, -1), np.array(mkl).reshape(1, -1)))\n",
    "\n",
    "        # return training set and testing set\n",
    "        train_x = rdata.loc[mkg, :]\n",
    "        train_y = rlabel\n",
    "        test_x = qdata.loc[mkg, :]\n",
    "    return train_x, test_x, train_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a228079-0871-4834-8233-f0f00591ad0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "r_data_tmp = rdata.copy()\n",
    "q_data_tmp = qdata.copy()\n",
    "r_label_tmp = rlabel.copy()\n",
    "train_x, test_x, train_y = get_expression(r_data_tmp,\n",
    "                                              q_data_tmp,\n",
    "                                              r_label_tmp,\n",
    "                                              thrh=0.90,\n",
    "                                              thrl=0.90,\n",
    "                                              normalization=True,\n",
    "                                              marker=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2d6ce4-dfb7-469f-9033-bbc7faf03e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f88ac4-c927-4098-8f17-f4e282df95d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2aef732-0480-4718-94b2-30736798af83",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y.diagnosis.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc167a29-f834-4964-b059-13ebb6842183",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9b4920-7f10-419f-b404-1cd72ad30171",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembl_pathway_relation = '../../usman/CellTICS/reactome/Ensembl2Reactome_All_Levels.txt'\n",
    "species = 'human'\n",
    "pathway_names = '../../usman/CellTICS/reactome/ReactomePathways.txt'\n",
    "pathway_relation = '../../usman/CellTICS/reactome/ReactomePathwaysRelation.txt'\n",
    "datatype = 'diagnosis'\n",
    "n_hidden = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea92cfcc-1f48-48a4-a0c8-760b5961ed0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathway_genes = get_gene_pathways(ensembl_pathway_relation, species=species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c9f9e2-935d-46a9-b86f-825123ffeeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set(pathway_genes.gene)-set(test_x.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83367bb5-95eb-4715-928b-37ec381acfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "masking, layers_node, train_x, test_x = get_masking(pathway_names,\n",
    "                                                        pathway_genes,\n",
    "                                                        pathway_relation,\n",
    "                                                        train_x,\n",
    "                                                        test_x,\n",
    "                                                        train_y,\n",
    "                                                        datatype,\n",
    "                                                        species,\n",
    "                                                        n_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947a0dbb-5fe7-48d5-80f4-964d2efbd8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "masking[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4d808a-43a6-4d1f-87f8-5cfa7e16e469",
   "metadata": {},
   "outputs": [],
   "source": [
    "masking[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f4978a-7822-4d48-afa2-708f9a2fb2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3a749c-7072-4c71-beb6-21057380c770",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6365ab2d-46b8-473a-8b75-e8b54ceab56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(test_x.index).to_csv('../../preprocessed_data/inhibitory_neuron/marker_genes.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c1f84b-3130-4129-b807-064c217d4a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a99354-1bad-4216-9cc7-12840888017b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc8ec16-f4a2-44bf-b7c9-7da47f9b77fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosis_mapping = {'AD_with_Plaques': 1, 'NCI_with_No_Plaques': 0}\n",
    "train_y['diagnosis_binary'] = train_y['diagnosis'].map(diagnosis_mapping)\n",
    "\n",
    "# Display the first few rows to verify the mapping\n",
    "print(train_y.head())\n",
    "train_y.drop(columns=['diagnosis'], inplace=True)\n",
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9be8f3-a46a-4ae1-9a1d-67cccac99006",
   "metadata": {},
   "outputs": [],
   "source": [
    "qlabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732b2a38-d803-4b4f-ac4e-c019be91a7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    diagnosis_mapping = {'AD_with_Plaques': 1, 'NCI_with_No_Plaques': 0}\n",
    "    qlabel['diagnosis_binary'] = qlabel['diagnosis'].map(diagnosis_mapping)\n",
    "    \n",
    "    # Display the first few rows to verify the mapping\n",
    "    print(qlabel.head())\n",
    "    qlabel.drop(columns=['diagnosis'], inplace=True)\n",
    "    qlabel\n",
    "\n",
    "except:\n",
    "    print('Previously Run Before')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03936a4-ef5b-4cc2-86aa-2287d1cd6b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, count_matrix, label):\n",
    "        # Read the CSV file\n",
    "        self.data = count_matrix\n",
    "        # Separate features and target\n",
    "        self.features = self.data.values\n",
    "        self.target = label.values\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get features and target for a given index\n",
    "        features = torch.tensor(self.features[idx], dtype=torch.float32)\n",
    "        target = torch.tensor(self.target[idx], dtype=torch.float32)\n",
    "        return features, target\n",
    "\n",
    "# Example usage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1901d316-2738-464b-9074-41a0ef2ca039",
   "metadata": {},
   "outputs": [],
   "source": [
    "qlabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181ee5b3-4d91-4d14-8f61-5c9daab89b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3e1ccd-08c2-45eb-b30b-8531d07852a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_x = train_x.T.iloc[40000:,]\n",
    "val_y = train_y.iloc[40000:,]\n",
    "train_x = train_x.T.iloc[0:40000,]\n",
    "train_y = train_y.iloc[0:40000,]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9ae3f2-675c-45b6-8893-4238202fa2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TabularDataset(train_x,train_y)\n",
    "val_dataset = TabularDataset(val_x,val_y)\n",
    "test_dataset = TabularDataset(test_x.T,qlabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df832a5b-69e3-4a78-8688-da267b38db30",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b058625e-5d59-4bc3-8359-766f22f55474",
   "metadata": {},
   "outputs": [],
   "source": [
    "qlabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0755769-eed7-4fbb-aa59-ff1302fe8d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "masking[0].shape, len(layers_node[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6b3d48-9c50-4b05-8e1a-3884cd8d66b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "masking[1].shape, len(layers_node[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ab2870-e147-4900-82b5-ee28eddc10f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "masking[2].shape, len(layers_node[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203eaf52-825c-49aa-8ad3-3359958fd0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "masking = list(masking.values())\n",
    "layers_node = list(layers_node.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b3edc4-e47e-433b-bbef-77aa6289184e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "#for output_layer in range(2, len(masking) + 2):       -->n\n",
    "class CustomNetwork(nn.Module):\n",
    "    def __init__(self, layers_node, n, masking):\n",
    "        super(CustomNetwork, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.masks = []\n",
    "        self.gamma = 0.0001\n",
    "\n",
    "        for i in range(n-1):\n",
    "            #print(-1-i)\n",
    "            #print(layers_node[-1-i])\n",
    "            in_features = len(layers_node[-1-i])\n",
    "            out_features = len(layers_node[-1-i-1])\n",
    "            self.layers.append(nn.Linear(in_features, out_features, bias=False))\n",
    "            self.masks.append(masking[-1-i])\n",
    "\n",
    "        self.layers.append(nn.Linear(len(layers_node[-n]), len(layers_node[0])-1))\n",
    "        self.masks = [torch.tensor(mask, dtype=torch.float32) for mask in self.masks]\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers[:-1]):\n",
    "            mask = self.masks[i]\n",
    "            #print(mask[0].shape)\n",
    "            layer.weight.data *= mask\n",
    "            #print(layer(x).shape)\n",
    "            x = layer(x)\n",
    "            \n",
    "            '''\n",
    "            x = x.unsqueeze(2)\n",
    "            print('x shape before multi: ',x.shape)\n",
    "            print('shape of mask: ', mask.shape )\n",
    "            x =  x*mask\n",
    "            x = x.squeeze(2)\n",
    "            x = torch.tanh(x)\n",
    "            '''\n",
    "            x = torch.relu(x)\n",
    "            #print('x shape: ',x.shape)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        x = self.layers[-1](x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec154e2-cebc-42c3-b0d7-9e8bdd046bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predicted_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    with torch.no_grad():  # No need to compute gradients during evaluation\n",
    "        for features, labels in dataloader:\n",
    "            outputs = model(features)\n",
    "            #print(outputs)\n",
    "            predicted = torch.round(torch.sigmoid(outputs.data))\n",
    "            #print(outputs)\n",
    "            #print(predicted)\n",
    "            #_, predicted = torch.sigmoid(outputs.data)\n",
    "            predicted_list.append(predicted)\n",
    "            labels_list.append(labels)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    #print(total)\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy, predicted_list, labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b46c87-92a9-41b4-988b-8d2fce7cc2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "def model(train_dataloader , val_dataloader, layers_node, masking, output_layer, learning_rate=0.001, num_epochs=50):\n",
    "\n",
    "    model = CustomNetwork(layers_node, output_layer, masking)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Using SGD with momentum\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=5, verbose=True)\n",
    "    patience = 20\n",
    "    best_val_accuracy = 0.0\n",
    "    epochs_no_improve = 0\n",
    "    early_stop = False\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        if early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        epoch_cost = 0.\n",
    "        \n",
    "        for batch_features,batch_targets in train_dataloader:\n",
    "            outputs = model(batch_features)\n",
    "            #print(outputs)\n",
    "            #print(batch_targets)\n",
    "            #print(outputs)\n",
    "            loss = criterion(outputs, batch_targets)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        \n",
    "        train_accuracy, predicted_list_train, labels_list_train = evaluate(model, train_dataloader)\n",
    "        val_accuracy, predicted_list_val, labels_list_val = evaluate(model, val_dataloader)\n",
    "        scheduler.step(val_accuracy)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Train_accuracy: {train_accuracy}, Val_accuracy: {val_accuracy}')\n",
    "\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            epochs_no_improve = 0\n",
    "        # Save the best model\n",
    "            torch.save(model.state_dict(), f'best_model_{output_layer}.pth')\n",
    "            print('Model saved.')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "    \n",
    "        # Early stopping\n",
    "        if epochs_no_improve >= patience:\n",
    "            early_stop = True\n",
    "            print(\"Early stopping triggered\")\n",
    "        \n",
    "    \n",
    "    train_accuracy, predicted_list_train, labels_list_train = evaluate(model, train_dataloader)\n",
    "    val_accuracy, predicted_list_val, labels_list_val = evaluate(model, val_dataloader)\n",
    "    \n",
    "    output_train = (predicted_list_train, labels_list_train)\n",
    "    output_val = (predicted_list_val, labels_list_val)\n",
    "    \n",
    "    return output_train, output_val,model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b7950f-3b88-4e40-a104-b7608b808a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.round(torch.sigmoid(torch.tensor([0.4,0.8])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a3d963-ef52-4af7-8ff2-b3d1e5d1849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#from neural_network import *\n",
    "print_information = True\n",
    "num_epochs = 500\n",
    "learning_rate = 0.01\n",
    "batch_size = 2048\n",
    "l2_regularization = 0.00001\n",
    "print_cost = True\n",
    "\n",
    "\n",
    "# Create the DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle= True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle= True)\n",
    "# Example of iterating through the DataLoader\n",
    "\n",
    "\n",
    "pred_y_df = pd.DataFrame(data=0, index=test_x.columns, columns=list(range(2, len(masking) + 2)))\n",
    "train_y_df = pd.DataFrame(data=0, index=train_x.columns, columns=list(range(2, len(masking) + 2)))\n",
    "#masking = list(masking.values())\n",
    "model_dict = dict()\n",
    "#layers_node = list(layers_node.values())\n",
    "activation_output = {}\n",
    "for output_layer in range(2, len(masking) + 2):\n",
    "    if print_information:\n",
    "        print(\"Current sub-neural network has \" + str(output_layer - 1) + \" hidden layers.\")\n",
    "    output_train, output_test,model_dict[output_layer] = model(train_dataloader,\n",
    "                                          val_dataloader,\n",
    "                                          layers_node,\n",
    "                                          masking,\n",
    "                                          output_layer,\n",
    "                                          learning_rate=learning_rate,num_epochs=num_epochs\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee465466-3124-4032-ab3e-56f9acb94294",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d364a0a-f5c1-49a0-93af-6dceef5494a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4704e75b-97c7-4e7a-8c75-85b0305c1994",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict[2].layers[0].weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "af1c28b8-cb97-49bf-8f9c-bc3e098fd863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 135)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masking[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90259b8-8120-411c-9643-1a16696a3781",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8075db07-a146-474b-a513-9ed6a8a053b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb -Uq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7441ad-be5e-462a-a082-4f67c9b2b931",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85feecb5-4c2e-4fea-ab3e-c5609f6579ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict[2].layers[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "89972661-b084-44b8-8254-da5cd9845183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(390, 1242)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masking[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8cf46b65-a7bd-4b84-bc8b-0797a8776c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.0000, -0.0000, -0.0000,  ..., -0.0000, -0.0000, 0.0000],\n",
       "        [-0.0000, 0.0000, -0.0000,  ..., -0.0000, 0.0000, 0.0000],\n",
       "        [-0.0000, -0.0000, 0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
       "        ...,\n",
       "        [-0.0000, 0.0674, -0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, -0.0000, -0.0000]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dict[2].layers[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "666ed806-a2de-4f4b-adce-c5a9d3c4cd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_888033/1061644972.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.nonzero(torch.tensor(model_dict[4].layers[2].weight)).squeeze()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[  0,  27],\n",
       "        [  0,  33],\n",
       "        [  0,  37],\n",
       "        [  0,  42],\n",
       "        [  0,  98],\n",
       "        [  0, 100],\n",
       "        [  0, 112],\n",
       "        [  0, 127],\n",
       "        [  1,   8],\n",
       "        [  1,  23],\n",
       "        [  1,  25],\n",
       "        [  1,  32],\n",
       "        [  1,  44],\n",
       "        [  1,  53],\n",
       "        [  1,  56],\n",
       "        [  1,  73],\n",
       "        [  1,  83],\n",
       "        [  1,  92],\n",
       "        [  1,  93],\n",
       "        [  1,  97],\n",
       "        [  1, 105],\n",
       "        [  1, 107],\n",
       "        [  1, 129],\n",
       "        [  1, 141],\n",
       "        [  2, 106],\n",
       "        [  3,   3],\n",
       "        [  3,  13],\n",
       "        [  3,  18],\n",
       "        [  3,  28],\n",
       "        [  3,  41],\n",
       "        [  3,  59],\n",
       "        [  3,  71],\n",
       "        [  3,  79],\n",
       "        [  3,  95],\n",
       "        [  3, 104],\n",
       "        [  3, 110],\n",
       "        [  3, 111],\n",
       "        [  3, 126],\n",
       "        [  3, 137],\n",
       "        [  3, 138],\n",
       "        [  4, 108],\n",
       "        [  5,  31],\n",
       "        [  5,  47],\n",
       "        [  5,  54],\n",
       "        [  5,  55],\n",
       "        [  5,  63],\n",
       "        [  5,  68],\n",
       "        [  5,  96],\n",
       "        [  6,   7],\n",
       "        [  6,  20],\n",
       "        [  6, 119],\n",
       "        [  7,   6],\n",
       "        [  7,   9],\n",
       "        [  8,   2],\n",
       "        [  8, 109],\n",
       "        [  9,  52],\n",
       "        [  9, 122],\n",
       "        [ 10,  17],\n",
       "        [ 10,  36],\n",
       "        [ 10,  38],\n",
       "        [ 10,  45],\n",
       "        [ 10,  60],\n",
       "        [ 10,  61],\n",
       "        [ 10,  69],\n",
       "        [ 10,  78],\n",
       "        [ 10,  88],\n",
       "        [ 10, 117],\n",
       "        [ 11,  14],\n",
       "        [ 11,  65],\n",
       "        [ 11,  72],\n",
       "        [ 11,  82],\n",
       "        [ 11, 134],\n",
       "        [ 12,   1],\n",
       "        [ 12,  12],\n",
       "        [ 12,  57],\n",
       "        [ 12,  58],\n",
       "        [ 12,  81],\n",
       "        [ 12, 116],\n",
       "        [ 12, 123],\n",
       "        [ 13,  43],\n",
       "        [ 13,  94],\n",
       "        [ 13, 124],\n",
       "        [ 14,   4],\n",
       "        [ 14,  90],\n",
       "        [ 15,  40],\n",
       "        [ 15,  74],\n",
       "        [ 15, 108],\n",
       "        [ 15, 115],\n",
       "        [ 16,  16],\n",
       "        [ 16,  24],\n",
       "        [ 16,  39],\n",
       "        [ 16,  50],\n",
       "        [ 16,  62],\n",
       "        [ 16,  80],\n",
       "        [ 16,  86],\n",
       "        [ 16,  89],\n",
       "        [ 16, 114],\n",
       "        [ 16, 125],\n",
       "        [ 17, 118],\n",
       "        [ 18,  67],\n",
       "        [ 18, 133],\n",
       "        [ 19,  21],\n",
       "        [ 19,  51],\n",
       "        [ 19,  75],\n",
       "        [ 19,  77],\n",
       "        [ 19,  84],\n",
       "        [ 19, 121],\n",
       "        [ 19, 139],\n",
       "        [ 19, 142],\n",
       "        [ 20,  15],\n",
       "        [ 20,  48],\n",
       "        [ 20,  91],\n",
       "        [ 20, 135],\n",
       "        [ 21,  35],\n",
       "        [ 21,  70],\n",
       "        [ 21, 103],\n",
       "        [ 22,  19],\n",
       "        [ 22,  76],\n",
       "        [ 22, 140],\n",
       "        [ 23,  29],\n",
       "        [ 23,  64],\n",
       "        [ 23,  99],\n",
       "        [ 24,   5],\n",
       "        [ 24,  10],\n",
       "        [ 24,  22],\n",
       "        [ 24,  46],\n",
       "        [ 24,  49],\n",
       "        [ 24, 101],\n",
       "        [ 24, 102],\n",
       "        [ 24, 120],\n",
       "        [ 24, 131],\n",
       "        [ 25,  26],\n",
       "        [ 25,  34],\n",
       "        [ 25,  85],\n",
       "        [ 25, 113],\n",
       "        [ 25, 130],\n",
       "        [ 25, 136],\n",
       "        [ 26,   0],\n",
       "        [ 26,  11],\n",
       "        [ 26,  30],\n",
       "        [ 26,  87],\n",
       "        [ 27,  66],\n",
       "        [ 27, 128],\n",
       "        [ 27, 132]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nonzero(torch.tensor(model_dict[4].layers[2].weight)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f30399a7-963a-458c-9030-0d8d3fad4050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,  27],\n",
       "        [  0,  33],\n",
       "        [  0,  37],\n",
       "        [  0,  42],\n",
       "        [  0,  98],\n",
       "        [  0, 100],\n",
       "        [  0, 112],\n",
       "        [  0, 127],\n",
       "        [  1,   8],\n",
       "        [  1,  23],\n",
       "        [  1,  25],\n",
       "        [  1,  32],\n",
       "        [  1,  44],\n",
       "        [  1,  53],\n",
       "        [  1,  56],\n",
       "        [  1,  73],\n",
       "        [  1,  83],\n",
       "        [  1,  92],\n",
       "        [  1,  93],\n",
       "        [  1,  97],\n",
       "        [  1, 105],\n",
       "        [  1, 107],\n",
       "        [  1, 129],\n",
       "        [  1, 141],\n",
       "        [  2, 106],\n",
       "        [  3,   3],\n",
       "        [  3,  13],\n",
       "        [  3,  18],\n",
       "        [  3,  28],\n",
       "        [  3,  41],\n",
       "        [  3,  59],\n",
       "        [  3,  71],\n",
       "        [  3,  79],\n",
       "        [  3,  95],\n",
       "        [  3, 104],\n",
       "        [  3, 110],\n",
       "        [  3, 111],\n",
       "        [  3, 126],\n",
       "        [  3, 137],\n",
       "        [  3, 138],\n",
       "        [  4, 108],\n",
       "        [  5,  31],\n",
       "        [  5,  47],\n",
       "        [  5,  54],\n",
       "        [  5,  55],\n",
       "        [  5,  63],\n",
       "        [  5,  68],\n",
       "        [  5,  96],\n",
       "        [  6,   7],\n",
       "        [  6,  20],\n",
       "        [  6, 119],\n",
       "        [  7,   6],\n",
       "        [  7,   9],\n",
       "        [  8,   2],\n",
       "        [  8, 109],\n",
       "        [  9,  52],\n",
       "        [  9, 122],\n",
       "        [ 10,  17],\n",
       "        [ 10,  36],\n",
       "        [ 10,  38],\n",
       "        [ 10,  45],\n",
       "        [ 10,  60],\n",
       "        [ 10,  61],\n",
       "        [ 10,  69],\n",
       "        [ 10,  78],\n",
       "        [ 10,  88],\n",
       "        [ 10, 117],\n",
       "        [ 11,  14],\n",
       "        [ 11,  65],\n",
       "        [ 11,  72],\n",
       "        [ 11,  82],\n",
       "        [ 11, 134],\n",
       "        [ 12,   1],\n",
       "        [ 12,  12],\n",
       "        [ 12,  57],\n",
       "        [ 12,  58],\n",
       "        [ 12,  81],\n",
       "        [ 12, 116],\n",
       "        [ 12, 123],\n",
       "        [ 13,  43],\n",
       "        [ 13,  94],\n",
       "        [ 13, 124],\n",
       "        [ 14,   4],\n",
       "        [ 14,  90],\n",
       "        [ 15,  40],\n",
       "        [ 15,  74],\n",
       "        [ 15, 108],\n",
       "        [ 15, 115],\n",
       "        [ 16,  16],\n",
       "        [ 16,  24],\n",
       "        [ 16,  39],\n",
       "        [ 16,  50],\n",
       "        [ 16,  62],\n",
       "        [ 16,  80],\n",
       "        [ 16,  86],\n",
       "        [ 16,  89],\n",
       "        [ 16, 114],\n",
       "        [ 16, 125],\n",
       "        [ 17, 118],\n",
       "        [ 18,  67],\n",
       "        [ 18, 133],\n",
       "        [ 19,  21],\n",
       "        [ 19,  51],\n",
       "        [ 19,  75],\n",
       "        [ 19,  77],\n",
       "        [ 19,  84],\n",
       "        [ 19, 121],\n",
       "        [ 19, 139],\n",
       "        [ 19, 142],\n",
       "        [ 20,  15],\n",
       "        [ 20,  48],\n",
       "        [ 20,  91],\n",
       "        [ 20, 135],\n",
       "        [ 21,  35],\n",
       "        [ 21,  70],\n",
       "        [ 21, 103],\n",
       "        [ 22,  19],\n",
       "        [ 22,  76],\n",
       "        [ 22, 140],\n",
       "        [ 23,  29],\n",
       "        [ 23,  64],\n",
       "        [ 23,  99],\n",
       "        [ 24,   5],\n",
       "        [ 24,  10],\n",
       "        [ 24,  22],\n",
       "        [ 24,  46],\n",
       "        [ 24,  49],\n",
       "        [ 24, 101],\n",
       "        [ 24, 102],\n",
       "        [ 24, 120],\n",
       "        [ 24, 131],\n",
       "        [ 25,  26],\n",
       "        [ 25,  34],\n",
       "        [ 25,  85],\n",
       "        [ 25, 113],\n",
       "        [ 25, 130],\n",
       "        [ 25, 136],\n",
       "        [ 26,   0],\n",
       "        [ 26,  11],\n",
       "        [ 26,  30],\n",
       "        [ 26,  87],\n",
       "        [ 27,  66],\n",
       "        [ 27, 128],\n",
       "        [ 27, 132]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_zero_indices_flat = torch.nonzero(torch.tensor(masking[0])).squeeze()\n",
    "non_zero_indices_flat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f7f6b0-cdd8-42e9-b838-e069cf22d782",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    train_y_df[output_layer] = get_prediction(output_train[output_layer],\n",
    "                                                 pd.get_dummies(train_y['diagnosis']),\n",
    "                                                 train_x,\n",
    "                                                 datatype='diagnosis')\n",
    "\n",
    "    \n",
    "    for j in range(len(output_train)):\n",
    "        ctp_sort = layers_node[0]\n",
    "        ctp_sort.sort()\n",
    "        if j != output_layer - 1:\n",
    "            output_train[j + 1] = pd.DataFrame(data=output_train[j + 1],\n",
    "                                                   index=layers_node[len(layers_node) - 2 - j],\n",
    "                                                   columns=train_x.columns)\n",
    "        else:\n",
    "            output_train[j + 1] = pd.DataFrame(data=output_train[j + 1],\n",
    "                                                   index=ctp_sort,\n",
    "                                                   columns=train_x.columns)\n",
    "    activation_output[output_layer] = output_train\n",
    "    pred_y_df[output_layer] = get_prediction(output_test[output_layer],\n",
    "                                                 pd.get_dummies(train_y['diagnosis']),\n",
    "                                                 test_x,\n",
    "                                                 datatype='diagnosis')\n",
    "    \n",
    "pred_test_y = pd.DataFra000me(data=0, index=test_x.columns, columns=['diagnosis'])\n",
    "pred_test_y['diagnosis'] = pred_y_df.T.describe().T['top']\n",
    "\n",
    "pred_train_y = pd.DataFrame(data=0, index= train_x.columns, columns=['diagnosis'])\n",
    "pred_train_y['diagnosis'] = train_y_df.T.describe().T['top']'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4dfe8d-ac4a-4ab6-8b8b-d55e02531bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d536e5c-1ad5-4431-b2cb-10591afdf8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example tensors\n",
    "x = torch.randn(1024, 216, 1)  # Shape: [1024, 216, 1]\n",
    "mask = torch.randn(1024, 216, 159)  # Shape: [1024, 216, 159]\n",
    "\n",
    "# Perform element-wise multiplication\n",
    "result = mask * x  # Shape: [1024, 216, 159]\n",
    "\n",
    "# Print shapes\n",
    "print(\"x shape:\", x.shape)\n",
    "print(\"mask shape:\", mask.shape)\n",
    "print(\"result shape:\", result.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c888d49-ef5c-4b32-b041-450bcbef70ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.randint(0, 2, (100,))  \n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e644a6-4af5-4e35-b2ab-f61042ca0ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd6154f-b2a3-42b3-99ed-ef87b9b6bffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7ba1ef44-1238-4541-a5de-fe5ef1b1b79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer output: tensor([[ 1.8656, -0.9858]], grad_fn=<AddmmBackward0>)\n",
      "After tanh activation: tensor([[ 0.9532, -0.7556]], grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a simple linear layer\n",
    "layer = nn.Linear(3, 2)  # Linear layer with 3 input features and 2 output features\n",
    "\n",
    "# Input tensor\n",
    "x = torch.tensor([[1.0, 2.0, 3.0]])\n",
    "\n",
    "# Apply the layer\n",
    "layer_output = layer(x)\n",
    "\n",
    "# Apply the tanh activation function\n",
    "tanh_output = torch.tanh(layer_output)\n",
    "\n",
    "print(\"Layer output:\", layer_output)\n",
    "print(\"After tanh activation:\", tanh_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f983e9b7-bf48-4aaa-9ee3-e7df63b3b582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a49c10c-f91b-4e0f-badb-0087f1a00faf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaa4a26-e8f8-4f4f-b747-38806641a289",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(pred_train_y.diagnosis.values == rlabel.diagnosis.values)/len(pred_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d96327c-58f3-4c56-be5e-cc92dcb24ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(pred_test_y.diagnosis.values == qlabel.diagnosis.values)/len(pred_test_y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a95032-bab2-4264-a703-4a75f3f31089",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cfde36-ecf3-409a-92dc-c6f9f9a8b8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "qlabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a6648f2b-ab84-4d45-bf46-4925d0c4a96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer output: tensor([[0.0911, 0.1891]], grad_fn=<AddmmBackward0>)\n",
      "After tanh activation: tensor([[0.0908, 0.1869]], grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a simple linear layer\n",
    "layer = nn.Linear(3, 2)  # Linear layer with 3 input features and 2 output features\n",
    "\n",
    "# Input tensor\n",
    "x = torch.tensor([[1.0, 2.0, 3.0]])\n",
    "\n",
    "# Apply the layer\n",
    "layer_output = layer(x)\n",
    "\n",
    "# Apply the tanh activation function\n",
    "tanh_output = torch.tanh(layer_output)\n",
    "\n",
    "print(\"Layer output:\", layer_output)\n",
    "print(\"After tanh activation:\", tanh_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "31ab862b-a5f3-4136-a953-04d103a879d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0911, 0.1891]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "11a0a768-4830-4729-bb36-022cf602d277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original weights:\n",
      "Parameter containing:\n",
      "tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.]], requires_grad=True)\n",
      "Mask matrix:\n",
      "tensor([[1., 0., 1.],\n",
      "        [0., 1., 0.]])\n",
      "Sparse weights:\n",
      "Parameter containing:\n",
      "tensor([[5., 0., 5.],\n",
      "        [0., 5., 0.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Step 1: Define the Linear Layer\n",
    "layer = nn.Linear(3, 2)\n",
    "\n",
    "# Initialize layer weights for demonstration\n",
    "nn.init.constant_(layer.weight, 5.0)\n",
    "nn.init.constant_(layer.bias, 0.0)\n",
    "\n",
    "print(\"Original weights:\")\n",
    "print(layer.weight)\n",
    "\n",
    "# Step 2: Create a Mask Matrix\n",
    "# Example mask matrix: 2x3, matching the shape of layer.weight\n",
    "mask = torch.tensor([[1, 0, 1],\n",
    "                     [0, 1, 0]], dtype=torch.float32)\n",
    "\n",
    "print(\"Mask matrix:\")\n",
    "print(mask)\n",
    "\n",
    "# Step 3: Apply the Mask\n",
    "# Element-wise multiplication of the layer's weights and the mask\n",
    "layer.weight.data *= mask\n",
    "\n",
    "print(\"Sparse weights:\")\n",
    "print(layer.weight)\n",
    "\n",
    "# Now you can use the layer in your model as usual.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "242584e0-1c92-424f-b404-54c95bcf0b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 135)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masking[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2edd1e13-d702-4730-85e4-31034c56a291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.5483,  0.3901,  0.0170],\n",
       "        [ 0.1705,  0.3766, -0.3806]], requires_grad=True)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5a452b2c-79ed-4394-887d-a8f81b924bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "56008d09-aab5-48d6-8a93-59eeba7679c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "42ad6c68-f6e2-4baf-89c9-6b26c1e23b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7911999999999999"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-0.1207*1 + 0.5761*2 -0.0801*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c44666-4a10-4080-9480-c190b530db18",
   "metadata": {},
   "outputs": [],
   "source": [
    "-0.1207,  0.5761, -0.0801"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6755c25e-e8b2-46fa-8e7c-906c55dcc123",
   "metadata": {},
   "outputs": [],
   "source": [
    "(-0.3824*1) +  (0.5135*2) -(0.1200*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c35e56-ac27-490f-8df1-018342dad4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a simple linear layer\n",
    "layer = nn.Linear(3, 2, bias = False)  # Linear layer with 3 input features and 2 output features\n",
    "\n",
    "# Input tensor\n",
    "x = torch.tensor([[1.0, 2.0, 3.0]])\n",
    "\n",
    "# Apply the layer\n",
    "layer_output = layer(x)\n",
    "\n",
    "# Apply the tanh activation function\n",
    "tanh_output = torch.tanh(layer_output)\n",
    "\n",
    "print(\"Layer output:\", layer_output)\n",
    "print(\"After tanh activation:\", tanh_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dab1582-2d77-44df-bc5f-0748e389589c",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12683e48-30fd-436f-8d01-528dbf28ebd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7048a058-832b-4085-a757-850d88a5ce67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What nn.linear does\n",
    "\n",
    "torch.matmul(x,layer.weight.T)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778eff23-4ebf-431b-9e79-603534027e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecf3caa-e38f-497a-8fc4-cf1571eae053",
   "metadata": {},
   "outputs": [],
   "source": [
    "1*0.5236 -0.5384*2-0.4127*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fd7b3e-7697-461f-8f75-91d106fcc0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28610ff-915f-49ac-9a7a-7f7da183b64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create a batch of tensors\n",
    "num_samples = 1024\n",
    "num_features = 3\n",
    "x_batch = torch.randn(num_samples, num_features)  # Shape: [1024, 3]§\n",
    "\n",
    "# Print the shape and content of the batch\n",
    "print(\"x_batch shape:\", x_batch.shape)\n",
    "print(\"x_batch tensor:\", x_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6ec33d-7ddb-4542-a241-aae6740f054e",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = nn.Linear(3, 2, bias = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fffec3-5c6c-466c-aea1-850a05a6b112",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d9b01c-2076-4d53-99bf-d45135435b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_output = layer(x_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa1aa77-c2aa-4b3b-ba0c-af8a9956e7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d90573-d194-4acb-87e4-129eb6a8a976",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.3851 * -0.1641 + 0.2640 * 0.5494 + 0.1831 * -1.0035"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eebf343-ebd3-4ad1-97c7-93083a111477",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bdd3d1-c6c0-4bd8-af0b-886281a0db77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838cf8de-fa87-4d0f-abb0-9e7a73a969be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ba6fbbb-526f-4ceb-86e7-1b240f373593",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bbeffc-b225-44a5-b9a1-b15d02a4a876",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
