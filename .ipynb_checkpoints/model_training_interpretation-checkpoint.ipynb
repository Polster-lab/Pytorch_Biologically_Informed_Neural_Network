{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a405eab-a0b4-44d5-bcd6-ba814b870c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 08:36:10.173140: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-05 08:36:10.182591: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-05 08:36:10.194379: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-05 08:36:10.194399: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-05 08:36:10.203371: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-05 08:36:10.590049: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from gene_expression import *\n",
    "from pathway_hierarchy import *\n",
    "from utils import *\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb0cea0-3807-442e-8374-f4c7a4b49355",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': {'train': '/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/Preprocessed_data/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/train.csv', 'test': '/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/Preprocessed_data/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/test.csv', 'val': '/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/Preprocessed_data/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/val.csv', 'y_train': '/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/Preprocessed_data/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/y_train.csv', 'y_test': '/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/Preprocessed_data/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/y_test.csv', 'y_val': '/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/Preprocessed_data/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/y_val.csv'}, 'model_output': {'model_save_dir': '/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/model_save/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/'}, 'train': {'epochs': 50, 'learning_rate': 0.001, 'weight_decay': 0.0001, 'batch_size': 8192}, 'gene_expression': {'highly_expressed_threshold': 0.95, 'lowly_expressed_threshold': 0.95, 'normalization': True, 'marker': True, 'print_information': True}, 'pathways_network': {'species': 'human', 'n_hidden_layer': 3, 'pathway_relation': '../../usman/CellTICS/reactome/ReactomePathwaysRelation.txt', 'pathway_names': '../../usman/CellTICS/reactome/ReactomePathways.txt', 'ensemble_pathway_relation': '../../usman/CellTICS/reactome/Ensembl2Reactome_All_Levels.txt', 'datatype': 'diagnosis'}}\n",
      "Getting Marker Genes.......\n",
      "1125\n",
      "1125\n",
      "2250\n",
      "2250\n",
      "                    0                1\n",
      "0     ENSG00000101210  ENSG00000172270\n",
      "1     ENSG00000099622  ENSG00000141905\n",
      "2     ENSG00000105278  ENSG00000167658\n",
      "3     ENSG00000178951  ENSG00000089847\n",
      "4     ENSG00000141985  ENSG00000127663\n",
      "...               ...              ...\n",
      "1120  ENSG00000285395  ENSG00000103316\n",
      "1121  ENSG00000140740  ENSG00000284218\n",
      "1122  ENSG00000122254  ENSG00000103365\n",
      "1123  ENSG00000006116  ENSG00000182601\n",
      "1124  ENSG00000077235  ENSG00000171208\n",
      "\n",
      "[1125 rows x 2 columns]\n",
      "                    0                1\n",
      "0     ENSG00000101210  ENSG00000172270\n",
      "1     ENSG00000099622  ENSG00000141905\n",
      "2     ENSG00000105278  ENSG00000167658\n",
      "3     ENSG00000178951  ENSG00000089847\n",
      "4     ENSG00000141985  ENSG00000127663\n",
      "...               ...              ...\n",
      "1120  ENSG00000285395  ENSG00000103316\n",
      "1121  ENSG00000140740  ENSG00000284218\n",
      "1122  ENSG00000122254  ENSG00000103365\n",
      "1123  ENSG00000006116  ENSG00000182601\n",
      "1124  ENSG00000077235  ENSG00000171208\n",
      "\n",
      "[1125 rows x 2 columns]\n",
      "Getting Pathway Genes.........\n",
      "Getting Masking.........\n",
      "HSA\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import argparse\n",
    "from utils import *\n",
    "from gene_expression import *\n",
    "from pathway_hierarchy import *\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from custom_neural_network import *\n",
    "from custom_fc_network import *\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import copy\n",
    "import pickle\n",
    "import random\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "model_dct = dict()\n",
    "\n",
    "# Hook function\n",
    "def hook_fn(module, input, output, layer_name):\n",
    "    global model_dct\n",
    "    input_list = [i.detach().cpu().numpy().tolist() for i in input]\n",
    "    output_list = output.detach().cpu().numpy().tolist()\n",
    "    \n",
    "    # If the layer name is not in the dictionary, create a new list for it\n",
    "    if layer_name not in model_dct:\n",
    "        model_dct[layer_name] = []\n",
    "\n",
    "    # Append the activations to the corresponding layer list\n",
    "    model_dct[layer_name].append({\n",
    "        'input': input_list,\n",
    "        'output': output_list\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "# Define the file path for the CSV file\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, count_matrix, label):\n",
    "        # Read the CSV file\n",
    "        self.data = count_matrix\n",
    "        # Separate features and target\n",
    "        self.features = self.data.values\n",
    "        self.target = label.values\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get features and target for a given index\n",
    "        features = torch.tensor(self.features[idx], dtype=torch.float32)\n",
    "        target = torch.tensor(self.target[idx], dtype=torch.float32)\n",
    "        return features, target\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predicted_list = []\n",
    "    probability_list = []\n",
    "    labels_list = []\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    loss = 0\n",
    "    with torch.no_grad():  # No need to compute gradients during evaluation\n",
    "        for features, labels in dataloader:\n",
    "            outputs = model(features)\n",
    "            #print(outputs)\n",
    "            probability = torch.sigmoid(outputs.data)\n",
    "            predicted = torch.round(torch.sigmoid(outputs.data))\n",
    "            #print(outputs)\n",
    "            #print(predicted)\n",
    "            loss += criterion(outputs, labels)\n",
    "            #_, predicted = torch.sigmoid(outputs.data)\n",
    "            predicted_list.extend(predicted)\n",
    "            labels_list.extend(labels)\n",
    "            probability_list.extend(probability)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    #print(total)\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy, loss, predicted_list, labels_list, probability_list\n",
    "\n",
    "def save_model(model_nn,model_path, model_state_dict_path):\n",
    "    \n",
    "    model_nn.eval()\n",
    "    torch.save(model_nn, model_path)\n",
    "    torch.save(model_nn.state_dict(), model_state_dict_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def model_fc(train_dataloader , val_dataloader, test_dataloader, test_cell_id, layers_node, masking, output_layer,model_save_dir, date_string, learning_rate=0.001, num_epochs=50, weight_decay = 0):\n",
    "\n",
    "    model_nn = CustomfcNetwork(layers_node, output_layer, masking)\n",
    "    optimizer = optim.AdamW(model_nn.parameters(), lr=learning_rate,weight_decay = weight_decay )  # Using SGD with momentum\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=5, verbose=True)\n",
    "    patience = 20\n",
    "    best_val_accuracy = 0.0\n",
    "    epochs_no_improve = 0\n",
    "    early_stop = False\n",
    "    csv_file_path = f'{model_save_dir}{date_string}/fc_training_log_{output_layer}.csv'\n",
    "\n",
    "    try:\n",
    "        os.makedirs(f'{model_save_dir}{date_string}')\n",
    "    except:\n",
    "        print(('...'))\n",
    "\n",
    "    with open(csv_file_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Epoch', 'Train_Loss', 'Train_accuracy','Validation_Loss','Val_accuracy'])\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        if early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        epoch_cost = 0.\n",
    "        \n",
    "        total_loss = 0\n",
    "        for batch_features,batch_targets in train_dataloader:\n",
    "            outputs = model_nn(batch_features)\n",
    "            #print(outputs)\n",
    "            #print(batch_targets)\n",
    "            #print(outputs)\n",
    "            loss = criterion(outputs, batch_targets)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "        \n",
    "        train_accuracy, train_loss, predicted_list_train, labels_list_train, train_probability_list = evaluate(model_nn, train_dataloader)\n",
    "        val_accuracy, val_loss, predicted_list_val, labels_list_val, val_probability_list = evaluate(model_nn, val_dataloader)\n",
    "        #scheduler.step(val_accuracy)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss.item():.4f}, Train_accuracy: {train_accuracy}, Val Loss: {val_loss.item():.4f}, Val_accuracy: {val_accuracy}')\n",
    "        with open(csv_file_path, mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([epoch + 1, loss.item(), train_accuracy, val_loss.item(), val_accuracy])\n",
    "        \n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            epochs_no_improve = 0\n",
    "        # Save the best model\n",
    "            model_path = f'{model_save_dir}{date_string}/fc_best_model_{output_layer}.pth'\n",
    "            model_state_dict_path = f'{model_save_dir}{date_string}/fc_best_model_{output_layer}_state_dict.pth'\n",
    "            save_model(model_nn, model_path, model_state_dict_path)\n",
    "            best_model_nn = copy.deepcopy(model_nn)\n",
    "            #torch.save(model_nn, f'{model_save_dir}{date_string}/fc_best_model_{output_layer}.pth')\n",
    "            #torch.save(model_nn.state_dict(), f'{model_save_dir}{date_string}/fc_best_model_{output_layer}_state_dict.pth')\n",
    "            print('Model saved.')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "    \n",
    "        # Early stopping\n",
    "        '''if epochs_no_improve >= patience:\n",
    "            early_stop = True\n",
    "            print(\"Early stopping triggered\")'''\n",
    "        \n",
    "    \n",
    "    train_accuracy, train_loss, predicted_list_train, labels_list_train, train_probability_list = evaluate(best_model_nn, train_dataloader)\n",
    "    val_accuracy, val_loss, predicted_list_val, labels_list_val, val_probability_list = evaluate(best_model_nn, val_dataloader)\n",
    "    test_accuracy, test_loss, predicted_list_test, labels_list_test, test_probability_list = evaluate(best_model_nn, test_dataloader)\n",
    "    print('Test Accucary', test_accuracy)\n",
    "    output_train = (predicted_list_train, labels_list_train)\n",
    "    output_val = (predicted_list_val, labels_list_val)\n",
    "\n",
    "    labels_list_test = [m.item() for m in labels_list_test]\n",
    "    predicted_list_test = [m.item() for m in predicted_list_test]\n",
    "    test_probability_list = [m.item() for m in test_probability_list]\n",
    "\n",
    "\n",
    "    test_df = pd.DataFrame({'cell_id': test_cell_id, 'true_y': labels_list_test, 'pred_y': predicted_list_test, 'probabilty': test_probability_list})\n",
    "    csv_file_path = f'{model_save_dir}{date_string}/fc_test_log_{output_layer}.csv'\n",
    "    test_df.to_csv(csv_file_path)\n",
    "    #torch.save(model_nn, f'{model_save_dir}{date_string}/fc_last_epoch_model_{output_layer}.pth')\n",
    "    return output_train, output_val,best_model_nn\n",
    "\n",
    "\n",
    "\n",
    "def model(train_dataloader , val_dataloader, test_dataloader, test_cell_id, layers_node, masking, output_layer,model_save_dir, date_string, learning_rate=0.001, num_epochs=50, weight_decay = 0):\n",
    "\n",
    "    model_nn = CustomNetwork(layers_node, output_layer, masking)\n",
    "    optimizer = optim.AdamW(model_nn.parameters(), lr=learning_rate,weight_decay = weight_decay )  # Using SGD with momentum\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=5, verbose=True)\n",
    "    patience = 20\n",
    "    best_val_accuracy = 0.0\n",
    "    epochs_no_improve = 0\n",
    "    early_stop = False\n",
    "    csv_file_path = f'{model_save_dir}{date_string}/training_log_{output_layer}.csv'\n",
    "\n",
    "    try:\n",
    "        os.makedirs(f'{model_save_dir}{date_string}')\n",
    "    except:\n",
    "        print(('...'))\n",
    "\n",
    "    with open(csv_file_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Epoch', 'Train_Loss', 'Train_accuracy','Validation_Loss','Val_accuracy'])\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        if early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        epoch_cost = 0.\n",
    "        \n",
    "        total_loss = 0\n",
    "        for batch_features,batch_targets in train_dataloader:\n",
    "            \n",
    "            #print(outputs)\n",
    "            #print(batch_targets)\n",
    "            #print(outputs)\n",
    "            \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model_nn(batch_features)\n",
    "            loss = criterion(outputs, batch_targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "        \n",
    "        train_accuracy, train_loss, predicted_list_train, labels_list_train, train_probability_list = evaluate(model_nn, train_dataloader)\n",
    "        val_accuracy, val_loss, predicted_list_val, labels_list_val, val_probability_list = evaluate(model_nn, val_dataloader)\n",
    "        #scheduler.step(val_accuracy)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss.item():.4f}, Train_accuracy: {train_accuracy}, Val Loss: {val_loss.item():.4f}, Val_accuracy: {val_accuracy}')\n",
    "        with open(csv_file_path, mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([epoch + 1, loss.item(), train_accuracy, val_loss.item(), val_accuracy])\n",
    "        \n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            epochs_no_improve = 0\n",
    "        # Save the best model\n",
    "            model_path = f'{model_save_dir}{date_string}/best_model_{output_layer}.pth'\n",
    "            model_state_dict_path = f'{model_save_dir}{date_string}/best_model_{output_layer}_state_dict.pth'\n",
    "            save_model(model_nn, model_path, model_state_dict_path)\n",
    "            best_model_nn = copy.deepcopy(model_nn)\n",
    "            #torch.save(model_nn, f'{model_save_dir}{date_string}/best_model_{output_layer}.pth')\n",
    "            #torch.save(model_nn.state_dict(), f'{model_save_dir}{date_string}/best_model_{output_layer}_state_dict.pth')\n",
    "            print('Model saved.')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "    \n",
    "        # Early stopping\n",
    "        '''if epochs_no_improve >= patience:\n",
    "            early_stop = True\n",
    "            print(\"Early stopping triggered\")'''\n",
    "        \n",
    "    \n",
    "    train_accuracy, train_loss, predicted_list_train, labels_list_train, train_probability_list = evaluate(best_model_nn, train_dataloader)\n",
    "    val_accuracy, val_loss, predicted_list_val, labels_list_val, val_probability_list = evaluate(best_model_nn, val_dataloader)\n",
    "    test_accuracy, test_loss, predicted_list_test, labels_list_test, test_probability_list = evaluate(best_model_nn, test_dataloader)\n",
    "    print('Test Accucary', test_accuracy)\n",
    "    output_train = (predicted_list_train, labels_list_train)\n",
    "    output_val = (predicted_list_val, labels_list_val)\n",
    "\n",
    "    labels_list_test = [m.item() for m in labels_list_test]\n",
    "    predicted_list_test = [m.item() for m in predicted_list_test]\n",
    "    test_probability_list = [m.item() for m in test_probability_list]\n",
    "\n",
    "\n",
    "    test_df = pd.DataFrame({'cell_id': test_cell_id, 'true_y': labels_list_test, 'pred_y': predicted_list_test, 'probabilty': test_probability_list})\n",
    "    csv_file_path = f'{model_save_dir}{date_string}/test_log_{output_layer}.csv'\n",
    "    test_df.to_csv(csv_file_path)\n",
    "    #torch.save(model_nn, f'{model_save_dir}{date_string}/last_epoch_model_{output_layer}.pth')\n",
    "    return output_train, output_val,best_model_nn\n",
    "\n",
    "\n",
    "def load_config(config_file):\n",
    "    with open(config_file, 'r') as file:\n",
    "        return yaml.safe_load(file)\n",
    "\n",
    "\n",
    "def main_file():\n",
    "\n",
    "    '''parser = argparse.ArgumentParser(description='Sample application with config and argparse')\n",
    "    parser.add_argument('--config', type=str, default='config.yml', help='Path to the configuration file')\n",
    "    args = parser.parse_args()'''\n",
    "\n",
    "    config = load_config('config.yml')\n",
    "    print(config)\n",
    "    train = pd.read_csv(config['dataset']['train'],index_col=0)\n",
    "    test = pd.read_csv(config['dataset']['test'],index_col=0)\n",
    "    val = pd.read_csv(config['dataset']['val'],index_col=0)\n",
    "\n",
    "    y_train = pd.read_csv(config['dataset']['y_train'])\n",
    "    y_test = pd.read_csv(config['dataset']['y_test'])\n",
    "    y_val = pd.read_csv(config['dataset']['y_val'])\n",
    "  \n",
    "\n",
    "\n",
    "    r_data_tmp = train.T\n",
    "    q_data_tmp = test.T\n",
    "    v_data_tmp = val.T\n",
    "    r_label_tmp = y_train\n",
    "\n",
    "    print('Getting Marker Genes.......')\n",
    "    train_x, test_x, val_x, train_y = get_expression(r_data_tmp,\n",
    "                                                q_data_tmp,\n",
    "                                                v_data_tmp,\n",
    "                                                r_label_tmp,\n",
    "                                                thrh=config['gene_expression']['highly_expressed_threshold'],\n",
    "                                                thrl=config['gene_expression']['lowly_expressed_threshold'],\n",
    "                                                normalization=config['gene_expression']['normalization'],\n",
    "                                                marker=config['gene_expression']['marker'])\n",
    "    \n",
    "    print('Getting Pathway Genes.........')\n",
    "    pathway_genes = get_gene_pathways(config['pathways_network']['ensemble_pathway_relation'], species=config['pathways_network']['species'])\n",
    "\n",
    "\n",
    "    print('Getting Masking.........')\n",
    "    masking, masking_df, layers_node, train_x, test_x,val_x = get_masking(config['pathways_network']['pathway_names'],\n",
    "                                                        pathway_genes,\n",
    "                                                        config['pathways_network']['pathway_relation'],\n",
    "                                                        train_x,\n",
    "                                                        test_x,\n",
    "                                                        val_x,\n",
    "                                                        train_y,\n",
    "                                                        config['pathways_network']['datatype'],\n",
    "                                                        config['pathways_network']['species'],\n",
    "                                                        config['pathways_network']['n_hidden_layer'])\n",
    "\n",
    "    test_cell_id = list(test_x.T.index) \n",
    "    try:\n",
    "        masking = list(masking.values())\n",
    "        layers_node = list(layers_node.values())\n",
    "    except:\n",
    "        print('already_done')\n",
    "\n",
    "\n",
    "    train_dataset = TabularDataset(train_x.T,train_y)\n",
    "    val_dataset = TabularDataset(val_x.T,y_val)\n",
    "    test_dataset = TabularDataset(test_x.T,y_test)  \n",
    "    \n",
    "    \n",
    "\n",
    "    dataloader_params = {\n",
    "    'batch_size': config['train']['batch_size'],\n",
    "    'shuffle': False\n",
    "    }\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset,**dataloader_params)\n",
    "    test_dataloader = DataLoader(test_dataset, **dataloader_params)\n",
    "    val_dataloader = DataLoader(val_dataset,**dataloader_params)\n",
    "    # Example of iterating through the DataLoader\n",
    "\n",
    "\n",
    "    pred_y_df = pd.DataFrame(data=0, index=test_x.columns, columns=list(range(2, len(masking) + 2)))\n",
    "    train_y_df = pd.DataFrame(data=0, index=train_x.columns, columns=list(range(2, len(masking) + 2)))\n",
    "    model_dict_sparse = dict()\n",
    "    model_dict_fc = dict()\n",
    "    activation_output = {}\n",
    "    now = datetime.now()\n",
    "\n",
    "# Format the date as a string\n",
    "    date_string = datetime_string = now.strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "    try:\n",
    "        os.makedirs(f'{config['model_output']['model_save_dir']}{date_string}')\n",
    "    except:\n",
    "        print(('...'))\n",
    "\n",
    "   \n",
    "\n",
    "    print('Training.........')\n",
    "    for output_layer in range(2, len(masking) + 2):\n",
    "        if config['gene_expression']['print_information']:\n",
    "            print(\"Current sub-neural network has \" + str(output_layer - 1) + \" hidden layers.\")\n",
    "        output_train, output_val,model_dict_sparse[output_layer] = model(train_dataloader,\n",
    "                                            val_dataloader,test_dataloader, test_cell_id,\n",
    "                                            layers_node,\n",
    "                                            masking,\n",
    "                                            output_layer,\n",
    "                                            model_save_dir = config['model_output']['model_save_dir'],date_string = date_string,\n",
    "                                            learning_rate=config['train']['learning_rate'],num_epochs=config['train']['epochs'],weight_decay = config['train']['weight_decay']\n",
    "                                        )  \n",
    "\n",
    "    print('tranining_fully_connected_layers:')\n",
    "    for output_layer in range(2, len(masking) + 2):\n",
    "        if config['gene_expression']['print_information']:\n",
    "            print(\"Current sub-neural network has \" + str(output_layer - 1) + \" hidden layers.\")\n",
    "        output_train, output_val,model_dict_fc[output_layer] = model_fc(train_dataloader,\n",
    "                                            val_dataloader,test_dataloader, test_cell_id,\n",
    "                                            layers_node,\n",
    "                                            masking,\n",
    "                                            output_layer,\n",
    "                                            model_save_dir = config['model_output']['model_save_dir'],date_string = date_string,\n",
    "                                            learning_rate=config['train']['learning_rate'],num_epochs=config['train']['epochs'],weight_decay = config['train']['weight_decay']\n",
    "                                        )  \n",
    "        \n",
    "    new_parameter = {'date_string': date_string}\n",
    "    config.update(new_parameter)\n",
    "    save_path =   str(config['model_output']['model_save_dir'])+ date_string + '/config.yml'\n",
    "    with open(save_path, 'w') as file:\n",
    "        yaml.dump(config, file)\n",
    "\n",
    "        \n",
    "    for i in range(len(masking_df)):\n",
    "        masking_df[i].to_csv(str(config['model_output']['model_save_dir'])+ date_string+ '/' +f'masking_df_{i}.csv')\n",
    "    \n",
    "   \n",
    "        \n",
    "    return model_dict_sparse, val_dataloader, test_dataloader, train_dataloader, train_x, train_y, val_x, y_val, test_x, y_test, config\n",
    "\n",
    "   \n",
    "model_dict_sparse, val_dataloader, test_dataloader, train_dataloader,train_x, train_y, val_x, y_val, test_x, y_test, config = main_file()\n",
    "for j,i in model_dict_sparse.items():\n",
    "\n",
    "# Assuming 'model' is your neural network\n",
    "    torch.save(i.state_dict(), f'{config['model_output']['model_save_dir']}{config['date_string']}/model_{j}_state_dict_jupyter_notebook.pth')\n",
    "\n",
    "\n",
    "for j,i in model_dict_sparse.items():\n",
    "        \n",
    "        print(f'Hidden_Layers: {j}')\n",
    "        accuracy, loss, predicted_list, labels_list, probability_list = evaluate(i, test_dataloader)\n",
    "        print(f'Test Accuracy: {accuracy}')   \n",
    "        accuracy, loss, predicted_list, labels_list, probability_list = evaluate(i, train_dataloader)\n",
    "        print(f'Train Accuracy: {accuracy}')   \n",
    "        accuracy, loss, predicted_list, labels_list, probability_list = evaluate(i, val_dataloader)\n",
    "        print(f'Validation Accuracy: {accuracy}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "32a465e1-58ab-4f18-9943-54217f87d0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden_Layers: 2\n",
      "Test Accuracy: 73.04088586030664\n",
      "Train Accuracy: 73.53212637557685\n",
      "Validation Accuracy: 73.46112886048988\n",
      "Hidden_Layers: 3\n",
      "Test Accuracy: 68.54770017035776\n",
      "Train Accuracy: 68.92438764643238\n",
      "Validation Accuracy: 67.77422790202343\n",
      "Hidden_Layers: 4\n",
      "Test Accuracy: 66.12010221465077\n",
      "Train Accuracy: 66.63116790912318\n",
      "Validation Accuracy: 65.49520766773163\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataset = TabularDataset(train_x.T,train_y)\n",
    "val_dataset = TabularDataset(val_x.T,y_val)\n",
    "test_dataset = TabularDataset(test_x.T,y_test) \n",
    "\n",
    "dataloader_params = {\n",
    "    'batch_size': 1,\n",
    "    'shuffle': False\n",
    "    }\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,**dataloader_params)\n",
    "test_dataloader = DataLoader(test_dataset, **dataloader_params)\n",
    "val_dataloader = DataLoader(val_dataset,**dataloader_params)\n",
    "\n",
    "for j,i in model_dict_sparse.items():\n",
    "        \n",
    "        print(f'Hidden_Layers: {j}')\n",
    "        accuracy, loss, predicted_list, labels_list, probability_list = evaluate(i, test_dataloader)\n",
    "        print(f'Test Accuracy: {accuracy}')   \n",
    "        accuracy, loss, predicted_list, labels_list, probability_list = evaluate(i, train_dataloader)\n",
    "        print(f'Train Accuracy: {accuracy}')   \n",
    "        accuracy, loss, predicted_list, labels_list, probability_list = evaluate(i, val_dataloader)\n",
    "        print(f'Validation Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "07cf86ae-23f8-429d-838e-df8abde3c2ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': {'train': '/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/Preprocessed_data/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/train.csv',\n",
       "  'test': '/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/Preprocessed_data/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/test.csv',\n",
       "  'val': '/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/Preprocessed_data/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/val.csv',\n",
       "  'y_train': '/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/Preprocessed_data/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/y_train.csv',\n",
       "  'y_test': '/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/Preprocessed_data/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/y_test.csv',\n",
       "  'y_val': '/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/Preprocessed_data/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/y_val.csv'},\n",
       " 'model_output': {'model_save_dir': '/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/model_save/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/'},\n",
       " 'train': {'epochs': 50,\n",
       "  'learning_rate': 0.001,\n",
       "  'weight_decay': 0.0001,\n",
       "  'batch_size': 8192},\n",
       " 'gene_expression': {'highly_expressed_threshold': 0.95,\n",
       "  'lowly_expressed_threshold': 0.95,\n",
       "  'normalization': True,\n",
       "  'marker': True,\n",
       "  'print_information': True},\n",
       " 'pathways_network': {'species': 'human',\n",
       "  'n_hidden_layer': 3,\n",
       "  'pathway_relation': '../../usman/CellTICS/reactome/ReactomePathwaysRelation.txt',\n",
       "  'pathway_names': '../../usman/CellTICS/reactome/ReactomePathways.txt',\n",
       "  'ensemble_pathway_relation': '../../usman/CellTICS/reactome/Ensembl2Reactome_All_Levels.txt',\n",
       "  'datatype': 'diagnosis'},\n",
       " 'date_string': '2024_08_05_13_54_14'}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "01c89375-5f2d-4175-958c-8c59fc2b1667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/model_save/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['model_output']['model_save_dir'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "867f09af-cd2b-45d7-ac06-29e192830d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>cell_id</th>\n",
       "      <th>GTACAACAGGCGTTGA.21.11</th>\n",
       "      <th>GAGTTGTAGCAACAGC.42.2</th>\n",
       "      <th>ACACCAATCCCGATCT.23.6</th>\n",
       "      <th>GCACATAAGAATCTAG.10.7</th>\n",
       "      <th>AAGACCTAGCCTATGT.13.1</th>\n",
       "      <th>ACTTACTGTCAGAAGC.8.1</th>\n",
       "      <th>CACAGGCAGAATCTCC.38.1</th>\n",
       "      <th>CGAGCCAGTATTACCG.29.0</th>\n",
       "      <th>ACCCAAAGTGACCTGC.1.5</th>\n",
       "      <th>CGGACGTTCATGTGGT.15.1</th>\n",
       "      <th>...</th>\n",
       "      <th>AGGGCCTTCGAGAACG.12.14</th>\n",
       "      <th>GCGATCGCACCTCTAC.12.11</th>\n",
       "      <th>ATGTCTTGTTCACCGG.16.2</th>\n",
       "      <th>TGCAGTACAACCAATC.26.2</th>\n",
       "      <th>CGGACGTCACGGTAGA.19.0</th>\n",
       "      <th>CTAACCCTCCCGGTAG.31.6</th>\n",
       "      <th>TCGGGCATCCCAGTGG.12.14</th>\n",
       "      <th>ATCACGAGTATCACCA.6.2</th>\n",
       "      <th>AAGACTCGTTGGGTAG.10.7</th>\n",
       "      <th>GATGATCCAGCCATTA.30.6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ENSG00000142920</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.145814</td>\n",
       "      <td>1.360721</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.643206</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.504917</td>\n",
       "      <td>1.582466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.861235</td>\n",
       "      <td>1.664975</td>\n",
       "      <td>0.940507</td>\n",
       "      <td>0.649033</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000128298</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.381069</td>\n",
       "      <td>3.4106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.504917</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.861235</td>\n",
       "      <td>0.625523</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.828458</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.501618</td>\n",
       "      <td>0.656484</td>\n",
       "      <td>0.454942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000128739</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.145814</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.931244</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.174485</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.625523</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.601402</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.245558</td>\n",
       "      <td>1.105994</td>\n",
       "      <td>2.713896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000115738</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.381069</td>\n",
       "      <td>3.4106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.347186</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.625523</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.360711</td>\n",
       "      <td>2.018482</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.501618</td>\n",
       "      <td>1.105994</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000104325</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.931244</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.625523</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.873105</td>\n",
       "      <td>0.656484</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000157152</th>\n",
       "      <td>2.006299</td>\n",
       "      <td>2.820393</td>\n",
       "      <td>1.745110</td>\n",
       "      <td>2.468479</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.945731</td>\n",
       "      <td>1.582466</td>\n",
       "      <td>...</td>\n",
       "      <td>2.345670</td>\n",
       "      <td>2.684303</td>\n",
       "      <td>2.484399</td>\n",
       "      <td>2.611894</td>\n",
       "      <td>2.341633</td>\n",
       "      <td>1.990744</td>\n",
       "      <td>1.351873</td>\n",
       "      <td>2.679290</td>\n",
       "      <td>2.487972</td>\n",
       "      <td>1.846029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000112186</th>\n",
       "      <td>2.006299</td>\n",
       "      <td>2.820393</td>\n",
       "      <td>1.360721</td>\n",
       "      <td>1.492540</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.391544</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.254404</td>\n",
       "      <td>2.318932</td>\n",
       "      <td>...</td>\n",
       "      <td>0.861235</td>\n",
       "      <td>1.060421</td>\n",
       "      <td>0.940507</td>\n",
       "      <td>0.360711</td>\n",
       "      <td>1.012347</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.828808</td>\n",
       "      <td>1.805211</td>\n",
       "      <td>0.656484</td>\n",
       "      <td>2.234892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000076685</th>\n",
       "      <td>2.006299</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.745110</td>\n",
       "      <td>1.328568</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.420167</td>\n",
       "      <td>1.582466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.861235</td>\n",
       "      <td>1.394136</td>\n",
       "      <td>0.940507</td>\n",
       "      <td>2.140355</td>\n",
       "      <td>2.605483</td>\n",
       "      <td>1.990744</td>\n",
       "      <td>0.828808</td>\n",
       "      <td>1.967391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.312032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000134539</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.776280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.381069</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.643206</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.990744</td>\n",
       "      <td>1.351873</td>\n",
       "      <td>1.622463</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.454942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000127663</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.328568</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.878202</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.861235</td>\n",
       "      <td>1.060421</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.095070</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.734937</td>\n",
       "      <td>0.873105</td>\n",
       "      <td>1.105994</td>\n",
       "      <td>1.078734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>582 rows × 14085 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "cell_id          GTACAACAGGCGTTGA.21.11  GAGTTGTAGCAACAGC.42.2  \\\n",
       "ENSG00000142920                0.000000               1.145814   \n",
       "ENSG00000128298                0.000000               0.000000   \n",
       "ENSG00000128739                0.000000               1.145814   \n",
       "ENSG00000115738                0.000000               0.000000   \n",
       "ENSG00000104325                0.000000               0.000000   \n",
       "...                                 ...                    ...   \n",
       "ENSG00000157152                2.006299               2.820393   \n",
       "ENSG00000112186                2.006299               2.820393   \n",
       "ENSG00000076685                2.006299               0.000000   \n",
       "ENSG00000134539                0.000000               1.776280   \n",
       "ENSG00000127663                0.000000               0.000000   \n",
       "\n",
       "cell_id          ACACCAATCCCGATCT.23.6  GCACATAAGAATCTAG.10.7  \\\n",
       "ENSG00000142920               1.360721               0.000000   \n",
       "ENSG00000128298               0.000000               0.381069   \n",
       "ENSG00000128739               0.000000               0.931244   \n",
       "ENSG00000115738               0.000000               0.381069   \n",
       "ENSG00000104325               0.000000               0.931244   \n",
       "...                                ...                    ...   \n",
       "ENSG00000157152               1.745110               2.468479   \n",
       "ENSG00000112186               1.360721               1.492540   \n",
       "ENSG00000076685               1.745110               1.328568   \n",
       "ENSG00000134539               0.000000               0.381069   \n",
       "ENSG00000127663               0.000000               1.328568   \n",
       "\n",
       "cell_id          AAGACCTAGCCTATGT.13.1  ACTTACTGTCAGAAGC.8.1  \\\n",
       "ENSG00000142920                 0.0000              1.643206   \n",
       "ENSG00000128298                 3.4106              0.000000   \n",
       "ENSG00000128739                 0.0000              0.000000   \n",
       "ENSG00000115738                 3.4106              0.000000   \n",
       "ENSG00000104325                 0.0000              0.000000   \n",
       "...                                ...                   ...   \n",
       "ENSG00000157152                 0.0000              0.000000   \n",
       "ENSG00000112186                 0.0000              2.391544   \n",
       "ENSG00000076685                 0.0000              0.000000   \n",
       "ENSG00000134539                 0.0000              1.643206   \n",
       "ENSG00000127663                 0.0000              0.000000   \n",
       "\n",
       "cell_id          CACAGGCAGAATCTCC.38.1  CGAGCCAGTATTACCG.29.0  \\\n",
       "ENSG00000142920               0.000000                    0.0   \n",
       "ENSG00000128298               0.000000                    0.0   \n",
       "ENSG00000128739               0.000000                    0.0   \n",
       "ENSG00000115738               2.347186                    0.0   \n",
       "ENSG00000104325               0.000000                    0.0   \n",
       "...                                ...                    ...   \n",
       "ENSG00000157152               0.000000                    0.0   \n",
       "ENSG00000112186               0.000000                    0.0   \n",
       "ENSG00000076685               0.000000                    0.0   \n",
       "ENSG00000134539               0.000000                    0.0   \n",
       "ENSG00000127663               0.000000                    0.0   \n",
       "\n",
       "cell_id          ACCCAAAGTGACCTGC.1.5  CGGACGTTCATGTGGT.15.1  ...  \\\n",
       "ENSG00000142920              0.504917               1.582466  ...   \n",
       "ENSG00000128298              0.504917               0.000000  ...   \n",
       "ENSG00000128739              1.174485               0.000000  ...   \n",
       "ENSG00000115738              0.000000               0.000000  ...   \n",
       "ENSG00000104325              0.000000               0.000000  ...   \n",
       "...                               ...                    ...  ...   \n",
       "ENSG00000157152              2.945731               1.582466  ...   \n",
       "ENSG00000112186              2.254404               2.318932  ...   \n",
       "ENSG00000076685              1.420167               1.582466  ...   \n",
       "ENSG00000134539              0.000000               0.000000  ...   \n",
       "ENSG00000127663              0.878202               0.000000  ...   \n",
       "\n",
       "cell_id          AGGGCCTTCGAGAACG.12.14  GCGATCGCACCTCTAC.12.11  \\\n",
       "ENSG00000142920                0.861235                1.664975   \n",
       "ENSG00000128298                0.861235                0.625523   \n",
       "ENSG00000128739                0.000000                0.625523   \n",
       "ENSG00000115738                0.000000                0.625523   \n",
       "ENSG00000104325                0.000000                0.625523   \n",
       "...                                 ...                     ...   \n",
       "ENSG00000157152                2.345670                2.684303   \n",
       "ENSG00000112186                0.861235                1.060421   \n",
       "ENSG00000076685                0.861235                1.394136   \n",
       "ENSG00000134539                0.000000                0.000000   \n",
       "ENSG00000127663                0.861235                1.060421   \n",
       "\n",
       "cell_id          ATGTCTTGTTCACCGG.16.2  TGCAGTACAACCAATC.26.2  \\\n",
       "ENSG00000142920               0.940507               0.649033   \n",
       "ENSG00000128298               0.000000               0.000000   \n",
       "ENSG00000128739               0.000000               0.000000   \n",
       "ENSG00000115738               0.000000               0.360711   \n",
       "ENSG00000104325               0.000000               0.000000   \n",
       "...                                ...                    ...   \n",
       "ENSG00000157152               2.484399               2.611894   \n",
       "ENSG00000112186               0.940507               0.360711   \n",
       "ENSG00000076685               0.940507               2.140355   \n",
       "ENSG00000134539               0.000000               0.000000   \n",
       "ENSG00000127663               0.000000               1.095070   \n",
       "\n",
       "cell_id          CGGACGTCACGGTAGA.19.0  CTAACCCTCCCGGTAG.31.6  \\\n",
       "ENSG00000142920               0.000000               0.000000   \n",
       "ENSG00000128298               2.828458               0.000000   \n",
       "ENSG00000128739               1.601402               0.000000   \n",
       "ENSG00000115738               2.018482               0.000000   \n",
       "ENSG00000104325               0.000000               0.000000   \n",
       "...                                ...                    ...   \n",
       "ENSG00000157152               2.341633               1.990744   \n",
       "ENSG00000112186               1.012347               0.000000   \n",
       "ENSG00000076685               2.605483               1.990744   \n",
       "ENSG00000134539               0.000000               1.990744   \n",
       "ENSG00000127663               0.000000               0.000000   \n",
       "\n",
       "cell_id          TCGGGCATCCCAGTGG.12.14  ATCACGAGTATCACCA.6.2  \\\n",
       "ENSG00000142920                0.000000              0.000000   \n",
       "ENSG00000128298                0.000000              0.501618   \n",
       "ENSG00000128739                0.000000              2.245558   \n",
       "ENSG00000115738                0.000000              0.501618   \n",
       "ENSG00000104325                0.000000              0.873105   \n",
       "...                                 ...                   ...   \n",
       "ENSG00000157152                1.351873              2.679290   \n",
       "ENSG00000112186                0.828808              1.805211   \n",
       "ENSG00000076685                0.828808              1.967391   \n",
       "ENSG00000134539                1.351873              1.622463   \n",
       "ENSG00000127663                1.734937              0.873105   \n",
       "\n",
       "cell_id          AAGACTCGTTGGGTAG.10.7  GATGATCCAGCCATTA.30.6  \n",
       "ENSG00000142920               0.000000               0.000000  \n",
       "ENSG00000128298               0.656484               0.454942  \n",
       "ENSG00000128739               1.105994               2.713896  \n",
       "ENSG00000115738               1.105994               0.000000  \n",
       "ENSG00000104325               0.656484               0.000000  \n",
       "...                                ...                    ...  \n",
       "ENSG00000157152               2.487972               1.846029  \n",
       "ENSG00000112186               0.656484               2.234892  \n",
       "ENSG00000076685               0.000000               1.312032  \n",
       "ENSG00000134539               0.000000               0.454942  \n",
       "ENSG00000127663               1.105994               1.078734  \n",
       "\n",
       "[582 rows x 14085 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4dce6d6f-cadc-4111-bc5c-6618bec14a2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: CustomNetwork(\n",
       "   (layers): ModuleList(\n",
       "     (0): Linear(in_features=582, out_features=356, bias=False)\n",
       "     (1): Linear(in_features=356, out_features=1, bias=True)\n",
       "   )\n",
       " ),\n",
       " 3: CustomNetwork(\n",
       "   (layers): ModuleList(\n",
       "     (0): Linear(in_features=582, out_features=356, bias=False)\n",
       "     (1): Linear(in_features=356, out_features=134, bias=False)\n",
       "     (2): Linear(in_features=134, out_features=1, bias=True)\n",
       "   )\n",
       " ),\n",
       " 4: CustomNetwork(\n",
       "   (layers): ModuleList(\n",
       "     (0): Linear(in_features=582, out_features=356, bias=False)\n",
       "     (1): Linear(in_features=356, out_features=134, bias=False)\n",
       "     (2): Linear(in_features=134, out_features=29, bias=False)\n",
       "     (3): Linear(in_features=29, out_features=1, bias=True)\n",
       "   )\n",
       " )}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dict_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b8458044-062e-48b9-a957-11ed67272bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomNetwork(\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=582, out_features=356, bias=False)\n",
       "    (1): Linear(in_features=356, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dict_sparse[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5337d8b1-82eb-4825-be59-e962176e58a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73.04088586030664"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy, loss, predicted_list, labels_list, probability_list = evaluate(model_dict_sparse[2], test_dataloader)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e823f1ea-15ab-417a-a6ca-9976f4876d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]])\n",
      "torch.Size([1, 582])\n"
     ]
    }
   ],
   "source": [
    "for features, labels in test_dataloader:\n",
    "    print(labels)\n",
    "    print(features.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6022339a-cf84-4801-b60c-666f947f77c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': {'train': '/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/Preprocessed_data/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/train.csv',\n",
       "  'test': '/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/Preprocessed_data/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/test.csv',\n",
       "  'val': '/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/Preprocessed_data/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/val.csv',\n",
       "  'y_train': '/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/Preprocessed_data/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/y_train.csv',\n",
       "  'y_test': '/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/Preprocessed_data/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/y_test.csv',\n",
       "  'y_val': '/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/Preprocessed_data/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/y_val.csv'},\n",
       " 'model_output': {'model_save_dir': '/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/model_save/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/'},\n",
       " 'train': {'epochs': 50,\n",
       "  'learning_rate': 0.001,\n",
       "  'weight_decay': 0.0001,\n",
       "  'batch_size': 8192},\n",
       " 'gene_expression': {'highly_expressed_threshold': 0.95,\n",
       "  'lowly_expressed_threshold': 0.95,\n",
       "  'normalization': True,\n",
       "  'marker': True,\n",
       "  'print_information': True},\n",
       " 'pathways_network': {'species': 'human',\n",
       "  'n_hidden_layer': 3,\n",
       "  'pathway_relation': '../../usman/CellTICS/reactome/ReactomePathwaysRelation.txt',\n",
       "  'pathway_names': '../../usman/CellTICS/reactome/ReactomePathways.txt',\n",
       "  'ensemble_pathway_relation': '../../usman/CellTICS/reactome/Ensembl2Reactome_All_Levels.txt',\n",
       "  'datatype': 'diagnosis'},\n",
       " 'date_string': '2024_08_05_09_46_44'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8b688d50-c892-4781-8630-259ed9e5078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict_sparse[3]\n",
    "# Define a hook function to capture the activations\n",
    "def get_activation(name, sample_idx, number_of_layers, config):\n",
    "    def hook(model, input, output):\n",
    "        # Convert output to numpy array for easier handling, but this is optional\n",
    "        activations[name] = output.detach().numpy()\n",
    "        '''output_dir = config['model_output']['model_save_dir'] + config['date_string'] + '/model_interpretation'\n",
    "        try:\n",
    "            os.makedirs(output_dir)\n",
    "        except:\n",
    "            print('already_directory there')\n",
    "        csv_filename = os.path.join(output_dir, f\"{name}_sample_{sample_idx}_model_hidden_layers_{number_of_layers}.csv\")\n",
    "        np.savetxt(csv_filename, activations[name], delimiter=\",\")'''\n",
    "           \n",
    "    return hook\n",
    "\n",
    "for idx, layer in enumerate(model_dict_sparse[3].layers):\n",
    "    layer_name = f'layer_{idx}'\n",
    "    number_of_layers = len(model_dict_sparse[3].layers)\n",
    "    activation_hook = get_activation(layer_name, sample_idx, number_of_layers, config)\n",
    "\n",
    "    layer.register_forward_hook(activation_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5a223d76-ebaf-479f-9eb3-6c79676b365c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]])\n",
      "torch.Size([1, 582])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "tanh(): argument 'input' (position 1) must be Tensor, not function",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(labels)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(features\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 13\u001b[0m outputs \u001b[38;5;241m=\u001b[39m  \u001b[43mk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m#print(outputs)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m probability \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(outputs\u001b[38;5;241m.\u001b[39mdata)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/custom_neural_network.py:51\u001b[0m, in \u001b[0;36mCustomNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m     x \u001b[38;5;241m=\u001b[39m layer(x)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03m    x = x.unsqueeze(2)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03m    print('x shape before multi: ',x.shape)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    x = torch.tanh(x)\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtanh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m#print('x shape: ',x.shape)\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m#print(x.shape)\u001b[39;00m\n\u001b[1;32m     55\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m](x)\n",
      "\u001b[0;31mTypeError\u001b[0m: tanh(): argument 'input' (position 1) must be Tensor, not function"
     ]
    }
   ],
   "source": [
    "k.eval()  # Set the model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "predicted_list = []\n",
    "probability_list = []\n",
    "labels_list = []\n",
    "l = []\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "loss = 0\n",
    "with torch.no_grad():  # No need to compute gradients during evaluation\n",
    "    for sample_idx, (features, labels) in enumerate(test_dataloader):\n",
    "        activations = {}\n",
    "        print(labels)\n",
    "        print(features.shape)\n",
    "        outputs =  k(features)\n",
    "            #print(outputs)\n",
    "        probability = torch.sigmoid(outputs.data)\n",
    "        predicted = torch.round(torch.sigmoid(outputs.data))\n",
    "            #print(outputs)\n",
    "            #print(predicted)\n",
    "        #loss += criterion(outputs, labels)\n",
    "            #_, predicted = torch.sigmoid(outputs.data)\n",
    "        predicted_list.extend(predicted)\n",
    "        labels_list.extend(labels)\n",
    "        probability_list.extend(probability)\n",
    "        total += labels.size(0)\n",
    "        l.append(activations)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    #print(total)\n",
    "accuracy = 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7e365da-98dd-4268-ba12-58198d2588a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "752bb9d4-3a1a-49f4-b260-1a115d09f49b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layer_0': array([[ 2.96206146e-01,  1.27690226e-01,  3.66973221e-01,\n",
       "          7.09568322e-01, -2.00132981e-01,  4.48493399e-02,\n",
       "          1.67873546e-01, -1.66143663e-02, -4.97219235e-01,\n",
       "         -2.31709689e-01, -1.32067248e-01,  3.03427190e-01,\n",
       "          3.21313180e-02, -6.74475580e-02,  3.19500625e-01,\n",
       "          1.72643438e-01,  4.34936918e-02,  5.29077530e-01,\n",
       "         -1.06818840e-01,  2.16723651e-01,  1.84147567e-01,\n",
       "          6.32062137e-01,  1.91032559e-01, -1.75858647e-01,\n",
       "         -3.60671431e-01, -3.09281737e-01,  8.85776758e-01,\n",
       "          2.93016732e-01, -1.84145919e-03,  1.88473284e-01,\n",
       "         -1.34294018e-01, -6.66129589e-02, -4.00170200e-02,\n",
       "          3.17997992e-01,  1.91933215e-02, -2.64761318e-02,\n",
       "         -2.09678158e-01, -7.97831893e-01, -7.12285042e-01,\n",
       "          1.62922710e-01,  3.33372295e-01,  2.31371477e-01,\n",
       "         -6.45204723e-01, -3.02555680e-01, -6.11125231e-02,\n",
       "         -2.07096145e-01, -5.21495007e-02,  0.00000000e+00,\n",
       "          1.52642205e-01,  3.70814651e-01, -7.84361642e-03,\n",
       "         -4.72317077e-02, -6.85185492e-02,  2.97301769e-01,\n",
       "          1.92409575e-01,  6.18134141e-02,  2.25801736e-01,\n",
       "          0.00000000e+00,  7.35189497e-01,  2.28915364e-01,\n",
       "          7.16437876e-01,  1.88652992e-01,  1.74692720e-02,\n",
       "         -1.20815165e-01, -6.74264356e-02,  4.65764143e-02,\n",
       "         -3.78432333e-01,  3.73301283e-02,  4.53714058e-02,\n",
       "          2.12257698e-01, -1.94459766e-01,  3.04979891e-01,\n",
       "         -2.05549181e-01,  8.60682130e-02,  2.19858378e-01,\n",
       "         -6.36808872e-02,  5.54965973e-01, -1.60953507e-01,\n",
       "          1.01826519e-01, -3.10841531e-01, -1.02495298e-01,\n",
       "         -1.89395130e-01, -8.13148841e-02, -1.03520319e-01,\n",
       "          1.97376549e-01, -3.82798791e-01, -8.90492499e-02,\n",
       "          9.19755250e-02,  2.34917346e-02, -3.94720465e-01,\n",
       "         -2.18991190e-01,  0.00000000e+00, -1.27010942e-02,\n",
       "          7.00815767e-02, -7.95292377e-01, -4.11187336e-02,\n",
       "         -1.82872847e-01, -1.00326918e-01,  3.17433864e-01,\n",
       "          4.91489209e-02, -2.06045675e-04,  2.24063173e-01,\n",
       "          1.81849107e-01,  8.19651708e-02, -2.53623426e-02,\n",
       "          4.39168662e-02,  1.85238227e-01, -3.66985500e-02,\n",
       "         -9.70825553e-03, -1.93041712e-01, -2.42604643e-01,\n",
       "          5.45821786e-02, -1.25069380e-01, -2.14097872e-01,\n",
       "          1.00337595e-01, -1.93215460e-01, -2.09701747e-01,\n",
       "         -3.81996781e-02, -3.87359619e-01, -6.80578649e-02,\n",
       "         -3.72039646e-01,  7.80396089e-02, -2.21203625e-01,\n",
       "         -2.85818100e-01,  2.93869853e-01, -1.67020082e-01,\n",
       "          0.00000000e+00,  6.41078770e-01, -7.92685896e-02,\n",
       "          3.92002314e-02,  1.49576396e-01,  8.51214826e-02,\n",
       "          8.70190412e-02,  0.00000000e+00, -4.41367269e-01,\n",
       "          1.46479636e-01,  1.76753938e-01, -2.94000581e-02,\n",
       "          1.62156940e-01, -2.85999358e-01,  1.86779752e-01,\n",
       "          3.82629335e-02,  0.00000000e+00, -1.61942258e-01,\n",
       "         -1.08534552e-01,  5.09525277e-02,  0.00000000e+00,\n",
       "         -4.68010008e-01, -1.66103825e-01,  1.79421887e-01,\n",
       "         -1.25596553e-01, -3.53145361e-01, -1.24310791e-01,\n",
       "         -3.62097263e-01, -1.26945823e-01,  2.45080754e-01,\n",
       "          2.06784472e-01,  5.06388605e-01,  2.54938006e-03,\n",
       "          9.81529504e-02, -5.64830005e-02,  0.00000000e+00,\n",
       "          2.12770760e-01,  1.36947796e-01, -2.72778898e-01,\n",
       "          6.74179643e-02,  2.13450164e-01, -1.82143778e-01,\n",
       "          3.94374341e-01, -6.43240809e-02,  2.78127640e-02,\n",
       "          0.00000000e+00, -9.35048983e-02,  2.38691986e-01,\n",
       "          1.48830310e-01,  7.29233995e-02, -4.35622036e-01,\n",
       "          4.42132503e-01,  9.09783840e-02, -5.52278571e-02,\n",
       "          5.03921881e-04,  3.47562075e-01,  1.36416346e-01,\n",
       "         -8.44834447e-02, -3.41631919e-01, -1.27087384e-01,\n",
       "          1.12652376e-01, -2.08918542e-01, -4.72916901e-01,\n",
       "         -1.33498386e-03,  6.81240320e-01, -3.48048270e-01,\n",
       "          1.92420930e-01,  1.01256996e-01, -3.42071354e-01,\n",
       "          6.67854100e-02, -8.12186226e-02,  5.13672978e-02,\n",
       "          4.04082239e-02,  6.43495262e-01,  9.80660319e-04,\n",
       "          0.00000000e+00, -4.05967951e-01, -2.23194405e-01,\n",
       "          6.26691803e-02,  3.28545749e-01, -2.10200220e-01,\n",
       "          1.13919653e-01, -5.97439289e-01, -7.62405917e-02,\n",
       "          0.00000000e+00, -7.13914335e-02, -1.58563629e-01,\n",
       "         -4.31835264e-01,  1.16770789e-01, -3.79836977e-01,\n",
       "         -9.75196213e-02,  1.58443227e-01,  1.55620247e-01,\n",
       "         -6.99879378e-02,  1.09848797e-01,  2.33079195e-02,\n",
       "         -4.24755454e-01,  0.00000000e+00, -1.30122304e-01,\n",
       "          0.00000000e+00, -2.27231383e-02, -1.71349838e-01,\n",
       "          4.74754833e-02, -3.58252712e-02, -2.48343647e-02,\n",
       "          2.89515197e-01,  3.12495083e-01,  0.00000000e+00,\n",
       "         -4.43181992e-01,  9.52093378e-02,  7.15527713e-01,\n",
       "          4.48904157e-01,  1.56068221e-01, -9.79935378e-02,\n",
       "         -1.37199253e-01, -6.49092048e-02,  1.16097912e-01,\n",
       "          2.98038006e-01,  3.39727700e-02, -1.45170800e-02,\n",
       "         -1.90241739e-01, -4.72435772e-01,  3.04028302e-01,\n",
       "          9.40061137e-02,  4.26344797e-02,  2.29093745e-01,\n",
       "         -2.98042178e-01,  2.03283861e-01, -1.65066019e-01,\n",
       "         -3.39468330e-01, -4.95893061e-01, -6.11710139e-02,\n",
       "          7.65155911e-01, -5.61880358e-02, -2.63497025e-01,\n",
       "          1.97866052e-01, -7.93479905e-02,  1.61098465e-01,\n",
       "         -1.41655102e-01,  1.60721451e-01,  0.00000000e+00,\n",
       "          1.00665145e-01,  3.69959384e-01,  1.90668523e-01,\n",
       "          2.72630192e-02, -7.47279972e-02, -2.23524302e-01,\n",
       "          2.28054985e-01,  3.63502771e-01,  5.14232814e-01,\n",
       "          5.96753359e-01,  1.11274689e-01,  3.38497818e-01,\n",
       "          0.00000000e+00,  0.00000000e+00,  2.25857049e-01,\n",
       "         -4.21067119e-01, -1.66264519e-01, -1.38432652e-01,\n",
       "          7.69409984e-02,  1.78994313e-01, -2.70343255e-02,\n",
       "          4.72393781e-02,  4.69708480e-02,  7.06185214e-03,\n",
       "          2.25994796e-01, -4.07492578e-01, -1.16366431e-01,\n",
       "         -3.97965968e-01,  1.38426289e-01,  3.74042615e-03,\n",
       "          1.91571787e-01,  1.77821428e-01, -8.95267129e-02,\n",
       "          3.78776155e-02, -1.40308857e-01,  5.09303153e-01,\n",
       "         -1.32730693e-01, -4.68604863e-02, -7.83297792e-03,\n",
       "          2.01461196e-01,  2.31327862e-01, -3.72915924e-01,\n",
       "         -1.18528828e-01, -2.99888849e-02, -2.29058117e-01,\n",
       "          1.03400335e-01,  0.00000000e+00,  3.83846790e-01,\n",
       "         -3.94572616e-01,  2.60059655e-01,  2.25212306e-01,\n",
       "         -7.62772486e-02, -8.99374485e-02, -2.44068891e-01,\n",
       "          1.03953779e-01, -2.10829437e-01,  5.12389988e-02,\n",
       "          5.95675230e-01, -2.43100643e-01,  1.47206128e-01,\n",
       "          1.70735136e-01,  1.07495621e-01, -1.69089381e-02,\n",
       "         -1.76275253e-01,  4.41564471e-02,  8.04968178e-02,\n",
       "         -6.34540990e-02, -2.30825499e-01,  4.11197364e-01,\n",
       "          3.29159237e-02, -1.01404563e-02, -1.60336733e-01,\n",
       "         -5.02661355e-02, -6.17425591e-02, -3.74361962e-01,\n",
       "          3.41476649e-01,  3.76556128e-01,  1.40976429e-01,\n",
       "          2.80286461e-01,  7.19060004e-03,  1.00521818e-01,\n",
       "         -4.62362945e-01,  5.75318858e-02, -9.66162533e-02,\n",
       "         -1.31226415e-02,  7.15610325e-01,  2.70697325e-02,\n",
       "          1.30647629e-01, -5.14991619e-02]], dtype=float32),\n",
       " 'layer_1': array([[-4.17350717e-02, -1.38227046e-02, -6.89482018e-02,\n",
       "          6.52164500e-03, -1.78219639e-02,  5.59299067e-03,\n",
       "         -1.16091808e-02,  5.74546214e-03, -1.97860524e-02,\n",
       "          6.20345678e-03, -4.61748896e-05,  1.56063810e-02,\n",
       "          4.62580621e-02, -4.36461009e-02, -1.69018023e-02,\n",
       "          3.45075019e-02, -3.73895355e-02,  5.09759821e-02,\n",
       "          1.75671041e-01, -9.66888759e-03, -3.13848117e-03,\n",
       "          3.90089527e-02,  2.69476771e-02,  1.07598472e-02,\n",
       "          1.78664625e-02,  2.65812278e-02,  3.49316411e-02,\n",
       "         -1.45123461e-02,  4.52420727e-06,  6.12798855e-02,\n",
       "         -1.00348368e-01, -2.29277946e-02, -4.14801277e-02,\n",
       "         -2.79595879e-05, -3.34137445e-03,  6.14844039e-02,\n",
       "         -3.42230196e-03,  2.61817724e-02,  2.75540333e-02,\n",
       "         -3.87711935e-02,  1.23213893e-02,  2.66446173e-01,\n",
       "          1.12053730e-01, -1.22413456e-01, -3.27071063e-02,\n",
       "         -4.63733487e-02, -6.07503997e-03, -4.59648110e-02,\n",
       "         -1.03173312e-04,  5.30815162e-02, -7.65431896e-02,\n",
       "         -7.50479400e-02,  5.54235503e-02,  2.84769014e-02,\n",
       "          0.00000000e+00,  2.86645889e-02, -7.96613991e-02,\n",
       "          3.10514104e-02,  1.52033344e-02, -3.51419020e-03,\n",
       "         -4.85012643e-02,  1.28018670e-02, -1.37900382e-01,\n",
       "          1.75488815e-02,  5.37526049e-02,  9.85697955e-02,\n",
       "         -4.20949273e-02, -4.12339829e-02,  1.40636368e-02,\n",
       "          1.62072778e-01, -9.81092751e-02,  3.40641066e-02,\n",
       "         -4.81100604e-02, -3.61183956e-02, -4.69815545e-02,\n",
       "          1.57805942e-02, -1.75603285e-01,  5.97312935e-02,\n",
       "          1.74404740e-01,  1.07205259e-02, -2.25572698e-02,\n",
       "         -4.55457829e-02, -1.98017210e-02,  1.55795366e-01,\n",
       "         -8.98606256e-02,  4.32669697e-03,  3.30826007e-02,\n",
       "         -2.01033615e-02,  2.68282220e-02,  3.01848203e-02,\n",
       "          7.80249536e-02, -6.81684166e-03,  1.68786496e-02,\n",
       "          3.84278595e-04,  0.00000000e+00,  5.73579445e-02,\n",
       "          7.81667531e-02, -1.08283935e-02, -4.55473624e-02,\n",
       "         -5.32849517e-05, -1.36511857e-02, -9.88160372e-02,\n",
       "         -1.47785917e-02,  0.00000000e+00, -2.66111009e-02,\n",
       "          3.58934216e-02,  5.98989252e-04,  1.52897770e-02,\n",
       "         -2.41756216e-02, -1.60460807e-02,  2.23280102e-01,\n",
       "         -4.44655344e-02, -2.84864847e-02, -6.00642003e-02,\n",
       "          1.00785308e-03,  0.00000000e+00,  6.27333671e-02,\n",
       "         -4.20746431e-02,  1.00754909e-01, -6.55777827e-02,\n",
       "          9.76162776e-02,  3.71073112e-02,  4.09010462e-02,\n",
       "          4.23038229e-02,  1.32320463e-04, -5.02535366e-02,\n",
       "         -4.01482638e-03,  8.12428221e-02,  1.03624240e-01,\n",
       "         -2.41779014e-02,  2.81769298e-02, -2.62606470e-03,\n",
       "          7.04981908e-02, -7.64573216e-02]], dtype=float32),\n",
       " 'layer_2': array([[0.03309521]], dtype=float32)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6814bb8-c98f-4875-9020-0f13ba0aef3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 134)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations['layer_1'].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
