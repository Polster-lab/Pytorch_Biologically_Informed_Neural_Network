{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8af6cf94-47ed-48b1-8925-7b4ffcbca149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import argparse\n",
    "from utils import *\n",
    "from gene_expression import *\n",
    "from pathway_hierarchy import *\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from custom_neural_network import *\n",
    "from custom_fc_network import *\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import copy\n",
    "import pickle\n",
    "import random\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c4ff0b8-ba70-47e0-b5c3-71a563c23b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/model_save/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/2024_08_04_09_50_56/config.yml'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_path = '/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/model_save/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/2024_08_04_09_50_56/config.yml'\n",
    "config_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e9d32477-ba62-47b5-a820-83b50a21d72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_file):\n",
    "    with open(config_file, 'r') as file:\n",
    "        return yaml.safe_load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab3f8835-521d-4797-a9c4-0c9ebc944c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': {'test': '/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/Preprocessed_data/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/test.csv',\n",
       "  'train': '/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/Preprocessed_data/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/train.csv',\n",
       "  'val': '/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/Preprocessed_data/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/val.csv',\n",
       "  'y_test': '/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/Preprocessed_data/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/y_test.csv',\n",
       "  'y_train': '/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/Preprocessed_data/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/y_train.csv',\n",
       "  'y_val': '/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/Preprocessed_data/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/y_val.csv'},\n",
       " 'date_string': '2024_08_04_09_50_56',\n",
       " 'gene_expression': {'highly_expressed_threshold': 0.95,\n",
       "  'lowly_expressed_threshold': 0.95,\n",
       "  'marker': True,\n",
       "  'normalization': True,\n",
       "  'print_information': True},\n",
       " 'model_output': {'model_save_dir': '/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/model_save/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/'},\n",
       " 'pathways_network': {'datatype': 'diagnosis',\n",
       "  'ensemble_pathway_relation': '../../usman/CellTICS/reactome/Ensembl2Reactome_All_Levels.txt',\n",
       "  'n_hidden_layer': 3,\n",
       "  'pathway_names': '../../usman/CellTICS/reactome/ReactomePathways.txt',\n",
       "  'pathway_relation': '../../usman/CellTICS/reactome/ReactomePathwaysRelation.txt',\n",
       "  'species': 'human'},\n",
       " 'train': {'batch_size': 2048,\n",
       "  'epochs': 50,\n",
       "  'learning_rate': 0.001,\n",
       "  'weight_decay': 0.0001}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = load_config(config_path)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "35c5cc0a-e0e2-47e1-a048-c32f6f57ad09",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['pathways_network']['ensemble_pathway_relation'] = '../../../usman/CellTICS/reactome/Ensembl2Reactome_All_Levels.txt'\n",
    "config['pathways_network']['pathway_names'] = '../../../usman/CellTICS/reactome/ReactomePathways.txt'\n",
    "config['pathways_network']['pathway_relation'] = '../../../usman/CellTICS/reactome/ReactomePathwaysRelation.txt'\n",
    "config['train']['epochs'] = 1\n",
    "#config['model_output']['model_save_dir'] = '..'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b072719-08e3-4f90-aa08-feaf0f3f447c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': {'test': '/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/Preprocessed_data/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/test.csv',\n",
       "  'train': '/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/Preprocessed_data/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/train.csv',\n",
       "  'val': '/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/Preprocessed_data/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/val.csv',\n",
       "  'y_test': '/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/Preprocessed_data/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/y_test.csv',\n",
       "  'y_train': '/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/Preprocessed_data/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/y_train.csv',\n",
       "  'y_val': '/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/Preprocessed_data/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/y_val.csv'},\n",
       " 'date_string': '2024_08_04_09_50_56',\n",
       " 'gene_expression': {'highly_expressed_threshold': 0.95,\n",
       "  'lowly_expressed_threshold': 0.95,\n",
       "  'marker': True,\n",
       "  'normalization': True,\n",
       "  'print_information': True},\n",
       " 'model_output': {'model_save_dir': '/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/model_save/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/'},\n",
       " 'pathways_network': {'datatype': 'diagnosis',\n",
       "  'ensemble_pathway_relation': '../../../usman/CellTICS/reactome/Ensembl2Reactome_All_Levels.txt',\n",
       "  'n_hidden_layer': 3,\n",
       "  'pathway_names': '../../../usman/CellTICS/reactome/ReactomePathways.txt',\n",
       "  'pathway_relation': '../../../usman/CellTICS/reactome/ReactomePathwaysRelation.txt',\n",
       "  'species': 'human'},\n",
       " 'train': {'batch_size': 2048,\n",
       "  'epochs': 1,\n",
       "  'learning_rate': 0.001,\n",
       "  'weight_decay': 0.0001}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "93a3689c-55f0-49e3-a550-36d806cbf597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import argparse\n",
    "from utils import *\n",
    "from gene_expression import *\n",
    "from pathway_hierarchy import *\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from custom_neural_network import *\n",
    "from custom_fc_network import *\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import copy\n",
    "import pickle\n",
    "import random\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "model_dct = dict()\n",
    "\n",
    "# Hook function\n",
    "def hook_fn(module, input, output, layer_name):\n",
    "    global model_dct\n",
    "    input_list = [i.detach().cpu().numpy().tolist() for i in input]\n",
    "    output_list = output.detach().cpu().numpy().tolist()\n",
    "    \n",
    "    # If the layer name is not in the dictionary, create a new list for it\n",
    "    if layer_name not in model_dct:\n",
    "        model_dct[layer_name] = []\n",
    "\n",
    "    # Append the activations to the corresponding layer list\n",
    "    model_dct[layer_name].append({\n",
    "        'input': input_list,\n",
    "        'output': output_list\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "# Define the file path for the CSV file\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, count_matrix, label):\n",
    "        # Read the CSV file\n",
    "        self.data = count_matrix\n",
    "        # Separate features and target\n",
    "        self.features = self.data.values\n",
    "        self.target = label.values\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get features and target for a given index\n",
    "        features = torch.tensor(self.features[idx], dtype=torch.float32)\n",
    "        target = torch.tensor(self.target[idx], dtype=torch.float32)\n",
    "        return features, target\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predicted_list = []\n",
    "    probability_list = []\n",
    "    labels_list = []\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    loss = 0\n",
    "    with torch.no_grad():  # No need to compute gradients during evaluation\n",
    "        for features, labels in dataloader:\n",
    "            outputs = model(features)\n",
    "            #print(outputs)\n",
    "            probability = torch.sigmoid(outputs.data)\n",
    "            predicted = torch.round(torch.sigmoid(outputs.data))\n",
    "            #print(outputs)\n",
    "            #print(predicted)\n",
    "            loss += criterion(outputs, labels)\n",
    "            #_, predicted = torch.sigmoid(outputs.data)\n",
    "            predicted_list.extend(predicted)\n",
    "            labels_list.extend(labels)\n",
    "            probability_list.extend(probability)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    #print(total)\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy, loss, predicted_list, labels_list, probability_list\n",
    "\n",
    "def save_model(model_nn,model_path, model_state_dict_path):\n",
    "    \n",
    "    model_nn.eval()\n",
    "    torch.save(model_nn, model_path)\n",
    "    torch.save(model_nn.state_dict(), model_state_dict_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def model_fc(train_dataloader , val_dataloader, test_dataloader, test_cell_id, layers_node, masking, output_layer,model_save_dir, date_string, learning_rate=0.001, num_epochs=50, weight_decay = 0):\n",
    "\n",
    "    model_nn = CustomfcNetwork(layers_node, output_layer, masking)\n",
    "    optimizer = optim.AdamW(model_nn.parameters(), lr=learning_rate,weight_decay = weight_decay )  # Using SGD with momentum\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=5, verbose=True)\n",
    "    patience = 20\n",
    "    best_val_accuracy = 0.0\n",
    "    epochs_no_improve = 0\n",
    "    early_stop = False\n",
    "    csv_file_path = f'{model_save_dir}{date_string}/fc_training_log_{output_layer}.csv'\n",
    "\n",
    "    try:\n",
    "        os.makedirs(f'{model_save_dir}{date_string}')\n",
    "    except:\n",
    "        print(('...'))\n",
    "\n",
    "    with open(csv_file_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Epoch', 'Train_Loss', 'Train_accuracy','Validation_Loss','Val_accuracy'])\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        if early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        epoch_cost = 0.\n",
    "        \n",
    "        total_loss = 0\n",
    "        for batch_features,batch_targets in train_dataloader:\n",
    "            outputs = model_nn(batch_features)\n",
    "            #print(outputs)\n",
    "            #print(batch_targets)\n",
    "            #print(outputs)\n",
    "            loss = criterion(outputs, batch_targets)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "        \n",
    "        train_accuracy, train_loss, predicted_list_train, labels_list_train, train_probability_list = evaluate(model_nn, train_dataloader)\n",
    "        val_accuracy, val_loss, predicted_list_val, labels_list_val, val_probability_list = evaluate(model_nn, val_dataloader)\n",
    "        #scheduler.step(val_accuracy)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss.item():.4f}, Train_accuracy: {train_accuracy}, Val Loss: {val_loss.item():.4f}, Val_accuracy: {val_accuracy}')\n",
    "        with open(csv_file_path, mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([epoch + 1, loss.item(), train_accuracy, val_loss.item(), val_accuracy])\n",
    "        \n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            epochs_no_improve = 0\n",
    "        # Save the best model\n",
    "            model_path = f'{model_save_dir}{date_string}/fc_best_model_{output_layer}.pth'\n",
    "            model_state_dict_path = f'{model_save_dir}{date_string}/fc_best_model_{output_layer}_state_dict.pth'\n",
    "            save_model(model_nn, model_path, model_state_dict_path)\n",
    "            best_model_nn = copy.deepcopy(model_nn)\n",
    "            #torch.save(model_nn, f'{model_save_dir}{date_string}/fc_best_model_{output_layer}.pth')\n",
    "            #torch.save(model_nn.state_dict(), f'{model_save_dir}{date_string}/fc_best_model_{output_layer}_state_dict.pth')\n",
    "            print('Model saved.')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "    \n",
    "        # Early stopping\n",
    "        '''if epochs_no_improve >= patience:\n",
    "            early_stop = True\n",
    "            print(\"Early stopping triggered\")'''\n",
    "        \n",
    "    \n",
    "    train_accuracy, train_loss, predicted_list_train, labels_list_train, train_probability_list = evaluate(best_model_nn, train_dataloader)\n",
    "    val_accuracy, val_loss, predicted_list_val, labels_list_val, val_probability_list = evaluate(best_model_nn, val_dataloader)\n",
    "    test_accuracy, test_loss, predicted_list_test, labels_list_test, test_probability_list = evaluate(best_model_nn, test_dataloader)\n",
    "    print('Test Accucary', test_accuracy)\n",
    "    output_train = (predicted_list_train, labels_list_train)\n",
    "    output_val = (predicted_list_val, labels_list_val)\n",
    "\n",
    "    labels_list_test = [m.item() for m in labels_list_test]\n",
    "    predicted_list_test = [m.item() for m in predicted_list_test]\n",
    "    test_probability_list = [m.item() for m in test_probability_list]\n",
    "\n",
    "\n",
    "    \n",
    "    #torch.save(model_nn, f'{model_save_dir}{date_string}/fc_last_epoch_model_{output_layer}.pth')\n",
    "    return output_train, output_val,best_model_nn\n",
    "\n",
    "\n",
    "\n",
    "def model(train_dataloader , val_dataloader, test_dataloader, test_cell_id, layers_node, masking, output_layer,model_save_dir, date_string, learning_rate=0.001, num_epochs=50, weight_decay = 0):\n",
    "\n",
    "    model_nn = CustomNetwork(layers_node, output_layer, masking)\n",
    "    optimizer = optim.AdamW(model_nn.parameters(), lr=learning_rate,weight_decay = weight_decay )  # Using SGD with momentum\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=5, verbose=True)\n",
    "    patience = 20\n",
    "    best_val_accuracy = 0.0\n",
    "    epochs_no_improve = 0\n",
    "    early_stop = False\n",
    "\n",
    "\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        if early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        epoch_cost = 0.\n",
    "        \n",
    "        total_loss = 0\n",
    "        for batch_features,batch_targets in train_dataloader:\n",
    "            \n",
    "            #print(outputs)\n",
    "            #print(batch_targets)\n",
    "            #print(outputs)\n",
    "            \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model_nn(batch_features)\n",
    "            loss = criterion(outputs, batch_targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "        \n",
    "        train_accuracy, train_loss, predicted_list_train, labels_list_train, train_probability_list = evaluate(model_nn, train_dataloader)\n",
    "        val_accuracy, val_loss, predicted_list_val, labels_list_val, val_probability_list = evaluate(model_nn, val_dataloader)\n",
    "        #scheduler.step(val_accuracy)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss.item():.4f}, Train_accuracy: {train_accuracy}, Val Loss: {val_loss.item():.4f}, Val_accuracy: {val_accuracy}')\n",
    "       \n",
    "        \n",
    "    best_model_nn = model_nn\n",
    "    train_accuracy, train_loss, predicted_list_train, labels_list_train, train_probability_list = evaluate(best_model_nn, train_dataloader)\n",
    "    val_accuracy, val_loss, predicted_list_val, labels_list_val, val_probability_list = evaluate(best_model_nn, val_dataloader)\n",
    "    test_accuracy, test_loss, predicted_list_test, labels_list_test, test_probability_list = evaluate(best_model_nn, test_dataloader)\n",
    "    print('Test Accucary', test_accuracy)\n",
    "    output_train = (predicted_list_train, labels_list_train)\n",
    "    output_val = (predicted_list_val, labels_list_val)\n",
    "\n",
    "    labels_list_test = [m.item() for m in labels_list_test]\n",
    "    predicted_list_test = [m.item() for m in predicted_list_test]\n",
    "    test_probability_list = [m.item() for m in test_probability_list]\n",
    "\n",
    "\n",
    "    #torch.save(model_nn, f'{model_save_dir}{date_string}/last_epoch_model_{output_layer}.pth')\n",
    "    return output_train, output_val,best_model_nn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main_file(config):\n",
    "\n",
    "    '''parser = argparse.ArgumentParser(description='Sample application with config and argparse')\n",
    "    parser.add_argument('--config', type=str, default='config.yml', help='Path to the configuration file')\n",
    "    args = parser.parse_args()'''\n",
    "\n",
    "    \n",
    "    train = pd.read_csv(config['dataset']['train'],index_col=0)\n",
    "    test = pd.read_csv(config['dataset']['test'],index_col=0)\n",
    "    val = pd.read_csv(config['dataset']['val'],index_col=0)\n",
    "\n",
    "    y_train = pd.read_csv(config['dataset']['y_train'])\n",
    "    y_test = pd.read_csv(config['dataset']['y_test'])\n",
    "    y_val = pd.read_csv(config['dataset']['y_val'])\n",
    "  \n",
    "\n",
    "\n",
    "    r_data_tmp = train.T\n",
    "    q_data_tmp = test.T\n",
    "    v_data_tmp = val.T\n",
    "    r_label_tmp = y_train\n",
    "\n",
    "    print('Getting Marker Genes.......')\n",
    "    train_x, test_x, val_x, train_y = get_expression(r_data_tmp,\n",
    "                                                q_data_tmp,\n",
    "                                                v_data_tmp,\n",
    "                                                r_label_tmp,\n",
    "                                                thrh=config['gene_expression']['highly_expressed_threshold'],\n",
    "                                                thrl=config['gene_expression']['lowly_expressed_threshold'],\n",
    "                                                normalization=config['gene_expression']['normalization'],\n",
    "                                                marker=config['gene_expression']['marker'])\n",
    "    \n",
    "    print('Getting Pathway Genes.........')\n",
    "    pathway_genes = get_gene_pathways(config['pathways_network']['ensemble_pathway_relation'], species=config['pathways_network']['species'])\n",
    "\n",
    "\n",
    "    print('Getting Masking.........')\n",
    "    masking, masking_df, layers_node, train_x, test_x,val_x = get_masking(config['pathways_network']['pathway_names'],\n",
    "                                                        pathway_genes,\n",
    "                                                        config['pathways_network']['pathway_relation'],\n",
    "                                                        train_x,\n",
    "                                                        test_x,\n",
    "                                                        val_x,\n",
    "                                                        train_y,\n",
    "                                                        config['pathways_network']['datatype'],\n",
    "                                                        config['pathways_network']['species'],\n",
    "                                                        config['pathways_network']['n_hidden_layer'])\n",
    "\n",
    "    test_cell_id = list(test_x.T.index) \n",
    "    try:\n",
    "        masking = list(masking.values())\n",
    "        layers_node = list(layers_node.values())\n",
    "    except:\n",
    "        print('already_done')\n",
    "\n",
    "\n",
    "    train_dataset = TabularDataset(train_x.T,train_y)\n",
    "    val_dataset = TabularDataset(val_x.T,y_val)\n",
    "    test_dataset = TabularDataset(test_x.T,y_test)  \n",
    "    \n",
    "    \n",
    "\n",
    "    dataloader_params = {\n",
    "    'batch_size': config['train']['batch_size'],\n",
    "    'shuffle': False\n",
    "    }\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset,**dataloader_params)\n",
    "    test_dataloader = DataLoader(test_dataset, **dataloader_params)\n",
    "    val_dataloader = DataLoader(val_dataset,**dataloader_params)\n",
    "    # Example of iterating through the DataLoader\n",
    "\n",
    "\n",
    "    pred_y_df = pd.DataFrame(data=0, index=test_x.columns, columns=list(range(2, len(masking) + 2)))\n",
    "    train_y_df = pd.DataFrame(data=0, index=train_x.columns, columns=list(range(2, len(masking) + 2)))\n",
    "    model_dict_sparse = dict()\n",
    "    model_dict_fc = dict()\n",
    "    activation_output = {}\n",
    "    now = datetime.now()\n",
    "\n",
    "# Format the date as a string\n",
    "    date_string = now.strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "   \n",
    "   \n",
    "\n",
    "    print('Training.........')\n",
    "    for output_layer in range(2, len(masking) + 2):\n",
    "        if config['gene_expression']['print_information']:\n",
    "            print(\"Current sub-neural network has \" + str(output_layer - 1) + \" hidden layers.\")\n",
    "        output_train, output_val,model_dict_sparse[output_layer] = model(train_dataloader,\n",
    "                                            val_dataloader,test_dataloader, test_cell_id,\n",
    "                                            layers_node,\n",
    "                                            masking,\n",
    "                                            output_layer,\n",
    "                                            model_save_dir = config['model_output']['model_save_dir'],date_string = date_string,\n",
    "                                            learning_rate=config['train']['learning_rate'],num_epochs=config['train']['epochs'],weight_decay = config['train']['weight_decay']\n",
    "                                        )  \n",
    "\n",
    "    print('tranining_fully_connected_layers:')\n",
    "    for output_layer in range(2, len(masking) + 2):\n",
    "        if config['gene_expression']['print_information']:\n",
    "            print(\"Current sub-neural network has \" + str(output_layer - 1) + \" hidden layers.\")\n",
    "        output_train, output_val,model_dict_fc[output_layer] = model_fc(train_dataloader,\n",
    "                                            val_dataloader,test_dataloader, test_cell_id,\n",
    "                                            layers_node,\n",
    "                                            masking,\n",
    "                                            output_layer,\n",
    "                                            model_save_dir = config['model_output']['model_save_dir'],date_string = date_string,\n",
    "                                            learning_rate=config['train']['learning_rate'],num_epochs=config['train']['epochs'],weight_decay = config['train']['weight_decay']\n",
    "                                        )  \n",
    "        \n",
    "       \n",
    "    \n",
    "    dataloader_params = {\n",
    "    'batch_size': 1,\n",
    "    'shuffle': False\n",
    "    }\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset,**dataloader_params)\n",
    "    test_dataloader = DataLoader(test_dataset, **dataloader_params)\n",
    "    val_dataloader = DataLoader(val_dataset,**dataloader_params)\n",
    "    \n",
    "    for j,i in model_dict_sparse.items():\n",
    "        \n",
    "        for name, layer in enumerate(i.children()):\n",
    "            layer_name = 'fc'+str(name+1)\n",
    "            layer.register_forward_hook(lambda module, input, output, name=layer_name: hook_fn(module, input, output, name))\n",
    "\n",
    "\n",
    "\n",
    "        accuracy, loss, predicted_list, labels_list, probability_list = evaluate(i, test_dataloader)\n",
    "        print(j)\n",
    "        print(f'Test Accuracy: {accuracy}')   \n",
    "        accuracy, loss, predicted_list, labels_list, probability_list = evaluate(i, train_dataloader)\n",
    "        print(f'Train Accuracy: {accuracy}')   \n",
    "        accuracy, loss, predicted_list, labels_list, probability_list = evaluate(i, val_dataloader)\n",
    "        print(f'Validation Accuracy: {accuracy}') \n",
    "        \n",
    "    return model_dict_sparse, val_dataloader, test_dataloader, train_dataloader, config\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7be8ac7a-0079-4700-8787-ce8469f7804a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Marker Genes.......\n",
      "1125\n",
      "1125\n",
      "2250\n",
      "2250\n",
      "                    0                1\n",
      "0     ENSG00000101210  ENSG00000172270\n",
      "1     ENSG00000099622  ENSG00000141905\n",
      "2     ENSG00000105278  ENSG00000167658\n",
      "3     ENSG00000178951  ENSG00000089847\n",
      "4     ENSG00000141985  ENSG00000127663\n",
      "...               ...              ...\n",
      "1120  ENSG00000285395  ENSG00000103316\n",
      "1121  ENSG00000140740  ENSG00000284218\n",
      "1122  ENSG00000122254  ENSG00000103365\n",
      "1123  ENSG00000006116  ENSG00000182601\n",
      "1124  ENSG00000077235  ENSG00000171208\n",
      "\n",
      "[1125 rows x 2 columns]\n",
      "                    0                1\n",
      "0     ENSG00000101210  ENSG00000172270\n",
      "1     ENSG00000099622  ENSG00000141905\n",
      "2     ENSG00000105278  ENSG00000167658\n",
      "3     ENSG00000178951  ENSG00000089847\n",
      "4     ENSG00000141985  ENSG00000127663\n",
      "...               ...              ...\n",
      "1120  ENSG00000285395  ENSG00000103316\n",
      "1121  ENSG00000140740  ENSG00000284218\n",
      "1122  ENSG00000122254  ENSG00000103365\n",
      "1123  ENSG00000006116  ENSG00000182601\n",
      "1124  ENSG00000077235  ENSG00000171208\n",
      "\n",
      "[1125 rows x 2 columns]\n",
      "Getting Pathway Genes.........\n",
      "Getting Masking.........\n",
      "HSA\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_dict_sparse, val_dataloader, test_dataloader, train_dataloader, config \u001b[38;5;241m=\u001b[39m \u001b[43mmain_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[31], line 288\u001b[0m, in \u001b[0;36mmain_file\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    284\u001b[0m pathway_genes \u001b[38;5;241m=\u001b[39m get_gene_pathways(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpathways_network\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mensemble_pathway_relation\u001b[39m\u001b[38;5;124m'\u001b[39m], species\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpathways_network\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspecies\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGetting Masking.........\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 288\u001b[0m masking, masking_df, layers_node, train_x, test_x,val_x \u001b[38;5;241m=\u001b[39m \u001b[43mget_masking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpathways_network\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpathway_names\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mpathway_genes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpathways_network\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpathway_relation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mtest_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpathways_network\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdatatype\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpathways_network\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspecies\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpathways_network\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mn_hidden_layer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m test_cell_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(test_x\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mindex) \n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/model_interpretation/pathway_hierarchy.py:148\u001b[0m, in \u001b[0;36mget_masking\u001b[0;34m(pathway_names, pathway_genes, relations_file_name, train_x, test_x, val_x, train_y, datatype, species, n_hidden)\u001b[0m\n\u001b[1;32m    146\u001b[0m reactome_net \u001b[38;5;241m=\u001b[39m ReactomeNetwork(pathway_names, pathway_genes, relations_file_name, species)\n\u001b[1;32m    147\u001b[0m genes_df \u001b[38;5;241m=\u001b[39m reactome_net\u001b[38;5;241m.\u001b[39mreactome\u001b[38;5;241m.\u001b[39mpathway_genes\n\u001b[0;32m--> 148\u001b[0m genes_df \u001b[38;5;241m=\u001b[39m \u001b[43mgene_mapping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenes_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m original_network \u001b[38;5;241m=\u001b[39m reactome_net\u001b[38;5;241m.\u001b[39mnetx\n\u001b[1;32m    151\u001b[0m original_terminal_nodes \u001b[38;5;241m=\u001b[39m [n \u001b[38;5;28;01mfor\u001b[39;00m n, d \u001b[38;5;129;01min\u001b[39;00m original_network\u001b[38;5;241m.\u001b[39mout_degree() \u001b[38;5;28;01mif\u001b[39;00m d \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/model_interpretation/pathway_hierarchy.py:131\u001b[0m, in \u001b[0;36mgene_mapping\u001b[0;34m(gene, df)\u001b[0m\n\u001b[1;32m    129\u001b[0m genedict[df\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m=\u001b[39m genelist\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m--> 131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m    132\u001b[0m         genelist\u001b[38;5;241m.\u001b[39mappend(df\u001b[38;5;241m.\u001b[39miloc[i, \u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/indexing.py:1180\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1178\u001b[0m check_dict_or_set_indexers(key)\n\u001b[1;32m   1179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(key) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m:\n\u001b[0;32m-> 1180\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1181\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(com\u001b[38;5;241m.\u001b[39mapply_if_callable(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m key)\n\u001b[1;32m   1182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_dict_sparse, val_dataloader, test_dataloader, train_dataloader, config = main_file(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e60c5cdb-baeb-4d43-a6f9-c89b1d0f544f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Test Accuracy: 51.59710391822828\n",
      "Train Accuracy: 51.1537096201633\n",
      "Validation Accuracy: 51.30990415335463\n",
      "3\n",
      "Test Accuracy: 50.27683134582624\n",
      "Train Accuracy: 49.82605608803692\n",
      "Validation Accuracy: 50.24494142705005\n",
      "4\n",
      "Test Accuracy: 50.27683134582624\n",
      "Train Accuracy: 49.82605608803692\n",
      "Validation Accuracy: 50.24494142705005\n"
     ]
    }
   ],
   "source": [
    "for j,i in model_dict_sparse.items():\n",
    "        \n",
    "        '''for name, layer in enumerate(i.children()):\n",
    "            layer_name = 'fc'+str(name+1)\n",
    "            layer.register_forward_hook(lambda module, input, output, name=layer_name: hook_fn(module, input, output, name))'''\n",
    "\n",
    "\n",
    "\n",
    "        accuracy, loss, predicted_list, labels_list, probability_list = evaluate(i, test_dataloader)\n",
    "        print(j)\n",
    "        print(f'Test Accuracy: {accuracy}')   \n",
    "        accuracy, loss, predicted_list, labels_list, probability_list = evaluate(i, train_dataloader)\n",
    "        print(f'Train Accuracy: {accuracy}')   \n",
    "        accuracy, loss, predicted_list, labels_list, probability_list = evaluate(i, val_dataloader)\n",
    "        print(f'Validation Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a03dd2c-cb71-4bb2-a547-7846b41750be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': {'test': '/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/Preprocessed_data/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/test.csv',\n",
       "  'train': '/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/Preprocessed_data/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/train.csv',\n",
       "  'val': '/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/Preprocessed_data/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/val.csv',\n",
       "  'y_test': '/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/Preprocessed_data/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/y_test.csv',\n",
       "  'y_train': '/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/Preprocessed_data/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/y_train.csv',\n",
       "  'y_val': '/12tb_dsk1/danish/Pytorch_Biologically_Informed_Neural_Network/Preprocessed_data/excitory_neurons/Exc_L2-3_CBLN2_LINC02306/y_val.csv'},\n",
       " 'date_string': '2024_08_04_09_50_56',\n",
       " 'gene_expression': {'highly_expressed_threshold': 0.95,\n",
       "  'lowly_expressed_threshold': 0.95,\n",
       "  'marker': True,\n",
       "  'normalization': True,\n",
       "  'print_information': True},\n",
       " 'model_output': {'model_save_dir': '..'},\n",
       " 'pathways_network': {'datatype': 'diagnosis',\n",
       "  'ensemble_pathway_relation': '../../../usman/CellTICS/reactome/Ensembl2Reactome_All_Levels.txt',\n",
       "  'n_hidden_layer': 3,\n",
       "  'pathway_names': '../../../usman/CellTICS/reactome/ReactomePathways.txt',\n",
       "  'pathway_relation': '../../../usman/CellTICS/reactome/ReactomePathwaysRelation.txt',\n",
       "  'species': 'human'},\n",
       " 'train': {'batch_size': 2048,\n",
       "  'epochs': 1,\n",
       "  'learning_rate': 0.001,\n",
       "  'weight_decay': 0.0001}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "af2a2e92-07f5-42a0-95d8-5e956c8c58fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.1748,  0.0152, -0.1350, -0.0740,  0.0213,  0.0491, -0.2728, -0.1561,\n",
      "         -0.0766,  0.1402, -0.0118, -0.2619, -0.2314, -0.1046,  0.1428,  0.0146,\n",
      "         -0.0429, -0.1270, -0.1081,  0.2178, -0.0518,  0.0503,  0.0419, -0.1045,\n",
      "          0.0132, -0.0098,  0.2846,  0.1598,  0.0234, -0.2266, -0.0348,  0.2263,\n",
      "         -0.0268, -0.2532, -0.1278, -0.1432,  0.0282, -0.1271, -0.1251, -0.1040,\n",
      "          0.0186, -0.0985, -0.0123,  0.1058, -0.0501, -0.1063, -0.2014,  0.0481,\n",
      "          0.0871,  0.0126, -0.0878, -0.2914,  0.2065, -0.2382, -0.1758, -0.2415,\n",
      "          0.0243, -0.1082, -0.0465, -0.1171, -0.0993, -0.2302,  0.0044,  0.0499,\n",
      "         -0.0444, -0.1438,  0.0361,  0.0345, -0.0840, -0.0204, -0.0959, -0.0915,\n",
      "         -0.2200,  0.0376,  0.1359,  0.0454, -0.1834, -0.1784, -0.0729, -0.1123,\n",
      "         -0.1033, -0.2710,  0.0018,  0.0389,  0.2087, -0.0518,  0.2324,  0.1801,\n",
      "          0.0087, -0.2656, -0.0378, -0.0917,  0.1963,  0.2176,  0.0019, -0.0354,\n",
      "         -0.2074,  0.1242,  0.2372, -0.1792,  0.0199, -0.0054, -0.1142, -0.0009,\n",
      "          0.2084,  0.0080, -0.1304,  0.0873, -0.1908, -0.1276,  0.0153, -0.2786,\n",
      "         -0.0027, -0.1964, -0.0329,  0.2025, -0.2346,  0.0192,  0.0295,  0.1242,\n",
      "          0.0056,  0.0287, -0.2188, -0.0349, -0.1606, -0.1717, -0.2742,  0.0094,\n",
      "          0.1944,  0.0056, -0.0007, -0.2179, -0.0051,  0.2418,  0.1229, -0.3409,\n",
      "          0.0332,  0.1276, -0.0260, -0.2296,  0.1867, -0.2038,  0.0500, -0.0434,\n",
      "         -0.1355,  0.1700,  0.1311,  0.3032, -0.3197,  0.0408, -0.2510, -0.2287,\n",
      "         -0.0559,  0.0084,  0.1670,  0.2191, -0.2287,  0.0284, -0.0215, -0.0365,\n",
      "          0.2257, -0.1431, -0.1894,  0.0584,  0.1910,  0.0363, -0.0651,  0.1167,\n",
      "         -0.0374, -0.2821, -0.2579, -0.0207,  0.1369,  0.0519,  0.0056,  0.1319,\n",
      "         -0.1488,  0.0215, -0.2342,  0.2476,  0.3295, -0.1194,  0.1939, -0.1068,\n",
      "          0.0653,  0.0171,  0.3362,  0.0377,  0.1876,  0.0172, -0.0023, -0.1047,\n",
      "         -0.0371, -0.0278, -0.0304,  0.2924,  0.1085, -0.1804,  0.0257, -0.2791,\n",
      "         -0.2481, -0.1575,  0.2392, -0.2428,  0.2879,  0.0722, -0.0345, -0.0007,\n",
      "         -0.0300, -0.0316,  0.0294, -0.0081, -0.0536, -0.0071, -0.1583,  0.0373,\n",
      "          0.1625, -0.1322,  0.0482, -0.0448, -0.2520, -0.0532,  0.2098, -0.1143,\n",
      "          0.1836,  0.0626, -0.1679, -0.2160,  0.2537, -0.1967, -0.2072,  0.1263,\n",
      "         -0.0250, -0.2458,  0.0016, -0.2056, -0.1771, -0.1213, -0.0935,  0.1465,\n",
      "         -0.0623, -0.2542,  0.1061,  0.0174,  0.0371, -0.1970, -0.0212, -0.2397,\n",
      "         -0.0237, -0.0564,  0.1079,  0.2072, -0.0278, -0.0861, -0.0958,  0.2555,\n",
      "         -0.0098,  0.1055,  0.1498, -0.2036, -0.2147,  0.1306, -0.1258, -0.0870,\n",
      "         -0.1646, -0.0943, -0.0308, -0.0186,  0.0092, -0.3265,  0.0052, -0.1994,\n",
      "         -0.2000,  0.0241, -0.2360,  0.0731,  0.0862, -0.0992,  0.0518,  0.0968,\n",
      "          0.1376,  0.0745,  0.1618, -0.0916, -0.1646,  0.0159,  0.1515, -0.2993,\n",
      "         -0.0463, -0.0796,  0.0102, -0.1417,  0.0032, -0.1766,  0.0294,  0.1066,\n",
      "         -0.0069, -0.1172, -0.0158, -0.0802, -0.0004,  0.0622, -0.0200, -0.1026,\n",
      "         -0.0020, -0.1479,  0.2164, -0.2421,  0.0549,  0.3114, -0.0408,  0.0253,\n",
      "         -0.0639,  0.1469,  0.0538,  0.0149, -0.1561, -0.0057, -0.1877,  0.0335,\n",
      "         -0.1438,  0.0774, -0.2593, -0.0393, -0.2298, -0.3346, -0.1049,  0.1803,\n",
      "         -0.2152,  0.0551,  0.0112,  0.0094,  0.1948,  0.1054, -0.1717,  0.0883,\n",
      "         -0.1106,  0.0182,  0.1492, -0.1563, -0.2842, -0.0308, -0.2385, -0.1262,\n",
      "         -0.0763,  0.0377, -0.1855, -0.0359, -0.3508,  0.1491,  0.0210, -0.0462,\n",
      "          0.2173,  0.0144, -0.0693, -0.0073]], requires_grad=True)\n",
      "2\n",
      "Test Accuracy: 49.48892674616695\n",
      "Train Accuracy: 49.78345757898474\n",
      "Validation Accuracy: 49.75505857294995\n"
     ]
    }
   ],
   "source": [
    "for j,i in model_dict_sparse.items():\n",
    "        \n",
    "        '''for name, layer in enumerate(i.children()):\n",
    "            layer_name = 'fc'+str(name+1)\n",
    "            layer.register_forward_hook(lambda module, input, output, name=layer_name: hook_fn(module, input, output, name))'''\n",
    "        k= torch.load(f'{config['model_output']['model_save_dir']}/2024_08_04_14_54_25/model_{j}_state_dict_jupyter_notebook.pth')\n",
    "        #print(k)\n",
    "        i.load_state_dict(k)\n",
    "        print(i.layers[1].weight)\n",
    "        accuracy, loss, predicted_list, labels_list, probability_list = evaluate(i, test_dataloader)\n",
    "        print(j)\n",
    "        print(f'Test Accuracy: {accuracy}')   \n",
    "        accuracy, loss, predicted_list, labels_list, probability_list = evaluate(i, train_dataloader)\n",
    "        print(f'Train Accuracy: {accuracy}')   \n",
    "        accuracy, loss, predicted_list, labels_list, probability_list = evaluate(i, val_dataloader)\n",
    "        print(f'Validation Accuracy: {accuracy}')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3be4b686-e43d-4439-911c-9ed7354757c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024_08_04_09_50_56'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['date_string']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1438244c-cf31-42b9-8c1b-600b20ef1571",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
